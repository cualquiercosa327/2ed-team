\part[Inferencia estadística univariada]{\Huge {\textcolor{purpura}{\tit {Inferencia estadística univariada}}}}

\chapter[Conceptos preliminares]{\Huge {\textcolor{purpura}{\tit {Conceptos preliminares}}}}
En esta parte del libro, se hace un breve repaso de las principales distribuciones de variables aleatorias. Para cada una de ellas, presentamos las principales características, tales como los diferentes momentos y relaciones entre distribuciones. Queremos hacer énfasis sobre las diferentes aplicaciones que pueden tener estas distribución, además de caracterizar los datos que provienen de estas distribuciones. Para mayores detalles acerca de la teoría básica de probabilidad, consulte \citeasnoun{Liliana}.

\section{Variables aleatorias y distribuciones de probabilidad}
La teoría estadística estudia fenómenos cuyos comportamientos no pueden ser predeterminados. La vida práctica está llena de estos fenómenos, algunos de gran impacto socio-económico tales como la tasa de desempleo, precio del dólar, la inflación entre otras; otros asociados más a la vida cotidiana tales como el resultado de un juego de azar, de un partido de fútbol, el clima de mañana, etcétera. Con las herramientas estadísticas apropiadas, se pueden conocer más a fondo estos fenómenos y así poder describirlos y/o predecirlos.

Estos fenómenos pueden ser descritos como un experimento aleatorio, esto es, un experimento cuyo resultado no se conoce de antemano. El conjunto que contiene todos los posibles resultados de un experimento aleatorio se denomina el espacio muestral, y en este libro será denotada por $\Omega$. Así que para el experimento de observar la tasa de desempleo para el siguiente mes, el espacio muestral será $\Omega=[0,1]$; mientras que mientras que para el resultado de un partido de fútbol, el espacio muestral puede ser $\Omega=\{\text{Gana el equipo A}, \text{Pierde el equipo A}\ \text{o}\ \text{Empatan los dos equipos}\}$, si en el experimento solo se observa si el equipo A gana o pierde, más no la diferencia de goles.

Dado un experimento aleatorio con el espacio muestral $\Omega$, una variable aleatoria $X$ es una función definida sobre $\Omega$ que asigna a cada elemento de $\Omega$ un número real. Por ejemplo, en el ejemplo del partido de fútbol, podemos definir la variable $X$ que vale $1$ si el equipo A no pierde el partido y $-1$ si lo hace, de esta forma, $X$ es una función que asigna el valor 1 a los resultados \emph{Gana el equipo A} y \emph{Empatan los dos equipos}, y asigna el valor -1 al resultado \emph{Pierde el equipo A}. En algunas situaciones, una variable aleatoria puede ser, simplemente, la función idéntica, como en el caso de observar la tasa de desempleo en el siguiente mes, la variable \emph{tasa de desempleo en el siguiente mes} tomará valor en $[0,1]$ y corresponde simplemente al resultado del experimento, esto es, una función idéntica.

Una forma de clasificar a las variables aleatorias es según los valores que toman y se tiene dos tipos de variables aleatorias: las variables discretas son aquellas que toman valores en un conjunto finito o enumerable,\footnote{Un conjunto $A$ es enumerable cuando existe una función inyectiva que tiene como dominio $A$ y recorrido el conjunto de los números naturales.} aunque en la teoría estadística, la mayoría de las variables discretas toman valores finitos o en el conjunto de los números naturales, más no en conjuntos enumerables más extraños como los racionales. Por otro lado, tenemos las variables continuas que son aquellas que toman valores en un intervalo, entendiendo que el conjunto de los números reales $\mathbb{R}$ es un intervalo de la forma $(-\infty,\infty)$. En los ejemplos dados anteriormente, las variables \emph{tasa de desempleo} y \emph{precio de dólar} son continuas, mientras que el \emph{resultado del partido de fútbol}, \emph{clima de mañana} se consideran discretas.

Dada una variable aleatoria $X$, estamos interesados en calcular probabilidades acerca de los valores que toma, por ejemplo, la probabilidad de que la tasa de empleo del siguiente mes sea inferior al 10\% o la probabilidad de que el equipo A no pierde un partido. Y estas probabilidades se resumen en la la función de distribución $F(x)$ o equivalentemente en la función de densidad, $f(x)$. Y para algunas funciones de densidad de alguna forma especial, se les dan algunos nombres específicos a la distribución de $X$, en los siguientes capítulos se repasan algunas de las distribuciones discretas y continuas. Como se mencionó antes, los distintos nombres de las distribuciones se dan cuando la función de densidad toma una forma especial. De esta forma, las definiciones de los siguientes capítulos se basan en la forma funcional de las funciones de densidad.

Adicionalmente, presentamos algunas instrucciones en el paquete \verb"R" para generar números aleatorios de las distribuciones que presentaremos. Esto es importante, puesto que nos da una idea general sobre cómo es el comportamiento de un conjunto de valores provenientes de una distribución específica, y esto nos ayuda a identificar distribuciones en contextos específicos. Por otro lado, la generación de números aleatorios también será útil cuando abordamos el tema de la inferencia estadística.

Antes de repasar las distribuciones de probabilidad, se define los conceptos de parámetro de distribución y espacio paramétrico. Un parámetro de distribución es aquel valor fijo que define la forma funcional de una distribución de probabilidad, es decir, cuando el parámetro cambia de valor, la función de distribución y la función de densidad cambian. Las distribuciones de probabilidad pueden tener más de un parámetro. Cuando una distribución tiene solo un parámetro, éste se denota usualmente por $\theta$; cuando se presentan más de un parámetro, la notación se cambia a $\btheta$, representando el vector de parámetros. El espacio paramétrico, $\bTheta$, es el conjunto que contiene todos los posibles valores del parámetro o el vector de parámetros. Para distribuciones con un solo parámetro, $\bTheta$ será un subconjunto de $\mathbb{R}$, mientras que para distribuciones con $k$ parámetros, $\bTheta$ será un subconjunto de $\mathbb{R}^k$.

\subsection{Distribuciones discretas}
En esta parte, presentamos algunas de las distribuciones discretas más conocidas. En primer lugar, se tiene la distribución uniforme discreta que es muy común en los juegos de azar.
\subsubsection{Distribución uniforme discreta}

\begin{Defi}
Una variable aleatoria $X$ tiene distribución uniforme discreta sobre el conjunto $\{1,2,\cdots,N\}$ si su función de densidad está dada por:
\begin{equation}
f_X(x)=P(X=x)=\frac{1}{N}I_{\{1,2,\cdots,N\}}(x)
\end{equation}
\end{Defi}

En la siguiente Figura, podemos visualizar la función de densidad de una distribución uniforme discreta sobre $\{1,\cdots,5\}$.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{Uniforme.eps}
\caption{Función de densidad de una distribución uniforme discreta sobre $\{1,\cdots,5\}$.}
\end{figure}

Esta distribución describe situaciones donde el experimento aleatorio puede tener un finito de resultados, y la probabilidad de ocurrencia la misma para cada posible resultado. Entre los ejemplos de la distribución uniforme discreta en la vida práctica están lanzamiento de una moneda corriente, lanzamiento de un dado corriente, extracción de una urna que contiene bolas enumeradas de 1 a $N$. También en las rifas, donde en una bolsa que contiene, digamos, 145 nombres de los empleados de una empresas, al seleccionar un nombre de la bolsa para ser ganador de un computador portátil, la probabilidad de que Juan Gómez sea el ganador es $1/145$, y es claro que entre menos empleados hayan en la empresa, más probable es que Juan Gómez sea el ganador. Ahora, suponga que de las 145 empleados, hay 60 mujeres y 85 hombres (donde las 60 mujeres se denotan por M1, M2, $\cdots$, M60), entonces la probabilidad de que el ganador sea mujer puede ser pensado como la probabilidad de que el ganador sea M1, o sea M2, o $\cdots$, o M60. Recurriendo a propiedades de la probabilidad, se tiene que la probabilidad requerida será $1/145+1/145+\cdots+1/145$, 60 veces, esto es, $60/145$. Más adelante, se verá que la anterior situación también puede ser descrita por una variable con distribución hipergeométrica.

En la vida práctica, para identificar variables con distribución uniforme discreta, en muchos casos basta con conocer el contexto del problema, es decir, con conocer condiciones que garantiza que los valores ocurren con la misma probabilidad, como por ejemplo, en el lanzamiento de una moneda, saber que la moneda no está cargada garantiza que los resultados siguen esta distribución. Sin embargo, puede suceder que se desconoce si esta condición se tiene o no, y lo disponible es simplemente un conjunto de valores. Suponga que se tienen los valores, 3, 1, 3, 4, 2, 4, 2, 2, 1, 3 que denotan resultados de 10 selecciones de una bolsa con bolas enumeradas de 1 hasta 5. Dada la característica de la distribución uniforme discreta, afirmar que el resultado de selección tiene distribución uniforme discreta es equivalente a afirmar que el proceso de selección es completamente al azar, sin ninguna preferencia de números. Ahora, si los datos provinieran de una distribución uniforme sobre $\{1,\cdots,N\}$, entonces la probabilidad de ocurrencia de cualquier $n=1,\cdots,N$ debe ser igual a $1/N$. Haciendo la analogía entre la probabilidad de ocurrencia con la frecuencia relativa, podemos intuir que sí se presenta una variable uniforme discreta si las frecuencias relativas para 1, $\cdots$, 5 son todos cercanos a $1/5=0.2$. Por lo tanto, del histograma de los datos, se observa que la frecuencia relativa del valor 5 está muy alejado del valor 0.2, de donde se sospecha la afirmación de que la selección fue realizada completamente al azar, sin ninguna preferencia de números.

\begin{figure}[!htb]
\centering
\includegraphics[bb=0 0 1104 1047, scale=0.19]{Uniforme_Hist.jpg}
\caption{Histograma de los datos 3, 1, 3, 4, 2, 4, 2, 2, 1, 3.}
\end{figure}

Para generar valores de una distribución uniforme discreta, se puede usar el comando \verb"sample" con la opción \verb"replace=TRUE", el siguiente código simula dos conjuntos de valores a partir de una distribución uniforme discreta sobre $\{1,2,3\}$, con tamaño 500 y 1000, respectivamente, y grafica los dos histogramas. Donde podemos observar que las frecuencias de los valores parecen ser constantes, especialmente cuando tamaño es grande, no hay algún valor con una frecuencia muy grande o muy pequeña con respecto a otros valores.
\begin{verbatim}
> set.seed(123)
> n<-c(500,1000)
> theta<-3
> par(mfrow=c(1,2))
> for(i in 1:length(n)){
+ a<-n[i]
+ hist(sample(theta,n[i],replace=TRUE),main="",xlab=a,ylab="Frecuencia")
+ }
\end{verbatim}


\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{random_unif.eps}
\caption{Histograma de valores simulados de una distribución uniforme discreta sobre $\{1,2,3\}$con tamaño de muestra 500 y 1000.}
\end{figure}

Algunas propiedades básicas de una distribución uniforme se muestra a continuación.

\begin{Res}
Si $X$ es una variable aleatoria con distribución uniforme discreta sobre el conjunto $\{1,2,\cdots,N\}$, entonces
    \begin{enumerate}
        \item $E(X)=\frac{N+1}{2}$.
        \item $Var(X)=\frac{N^2-1}{12}$.
        \item $m_X(t)=\sum_{i=1}^N\frac{e^{ti}}{N}$.
    \end{enumerate}
\end{Res}

\subsubsection{Distribución Bernoulli}
La distribución Bernoulli debe su nombre al matemático suizo Jacob Bernoulli (1654-1705). Esta distribución es asociada con experimentos aleatorios que tienen solo dos posibles resultados, los cuales se etiquetan como \emph{éxito} y \emph{fracaso}, donde la probabilidad de obtener \emph{éxito} es $p$, con $0<p<1$. De esta forma una variables aleatorias que toma valor 1 cuando se observa el \emph{éxito} y 0 en el caso de \emph{fracaso} tiene distribución Bernoulli con parámetro $p$. En la vida práctica, se presenta muchos ensayos del tipo Bernoulli, por ejemplo, el éxito o fracaso de un correo electrónico ofreciendo algún servicio o producto; en la teoría del muestreo, la pertenencia de un elemento de la población en la muestra también tiene distribución Bernoulli.

\begin{figure}[!htb]
\centering
\includegraphics[bb=0 0 300 455, scale=0.3]{Bernoulli.jpg}
\caption{Jacob Bernoulli (1654-1705)}
\end{figure}

En términos de la función de densidad tenemos la siguiente definición de la distribución Bernoulli .

\begin{Defi}
Una variable aleatoria $X$ tiene distribución Bernoulli con parámetro $p\in (0,1)$ si su función de densidad está dada por:
\begin{equation}
f_X(x)=p^x(1-p)^{1-x}I_{\{0,1\}}(x),
\end{equation}
y se nota como $X\sim Ber(p)$.
\end{Defi}

Nótese que si $X\sim Ber(p)$, entonces $P(X=1)=p$, y $P(X=0)=1-p$. Y tenemos las siguientes propiedades para la distribución Bernoulli.
\begin{Res}
Si $X$ es una variable aleatoria con distribución Bernoulli con parámetro $p$, entonces
    \begin{enumerate}
        \item $E(X)=p$.
        \item $Var(X)=p(1-p)$.
        \item $m_X(t)=pe^t+1-p$.
    \end{enumerate}
\end{Res}

\begin{proof}
Las anteriores tres expresiones se pueden obtener fácilmente usando la definición de la esperanza para una variable discreta. En particular, $m_X(t)=E(e^{tX})=e^{t}P(X=1)+e^{0}P(X=0)=pe^t+1-p$.
\end{proof}

En muchos casos, no se observa un solo ensayo del tipo Bernoulli, sino una series de ensayos. Por ejemplo, una empresa que hace ventas virtuales, no manda el correo de promocionamiento a una sola persona, sino a muchos, y la empresa está interesada en cantidades como, si se manda el mismo correo a 30 personas distintas, cuántas ventas exitosas obtendrá, es decir, estamos interesados en la variable definida como \emph{el número de éxitos en $n$ ensayos del tipo Bernoulli}, este tipo de variables se describen con la distribución binomial que se estudiará a continuación.

\subsubsection{Distribución binomial}
\begin{Defi}
Una variable aleatoria $X$ tiene distribución binomial con los parámetros $n\in \mathbb{N}$ y $p\in (0,1)$ si su función de densidad está dada por:
\begin{equation}
f_X(x)=P(X=x)=\binom{n}{x}p^x(1-p)^{n-x}I_{\{0,1,\cdots,n\}}(x),
\end{equation}
y se nota como $X\sim Bin(n,p)$.
\end{Defi}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{densidad_binomial.eps}
\caption{Función de densidad de una distribución $Bin(10,0.35)$.}
\end{figure}

De acuerdo a lo discutido al final de la sección anterior, una aplicación de la distribución binomial es cuando tenemos un número $n$ de repeticiones independientes de un mismo experimento del tipo Bernoulli, donde la probabilidad del éxito en cada ensayo se denota por $p$, entonces la variable \emph{número de éxitos obtenidos en las $n$ repeticiones} tiene distribución $Bin(n,p)$. Dada la anterior interpretación, podemos ver fácilmente que los valores que toma $X$ son enteros entre 0 y $n$, siendo el valor de 0 denotando la situación donde en todos los ensayos, se obtuvo como resultado \emph{fracaso}, y el valor de $n$ cuando todos los ensayos tuvieron como resultado \emph{éxito}. En la Figura 1.5, se muestra la función de densidad de una distribución $Bin(10,0.35)$, donde se observa que a diferencia de la distribución uniforme discreta, la distribución binomial tiene un valor que tiene mayor probabilidad que otros, digamos $x_0$, y a medida que se aleja de $x_0$, la probabilidad disminuye, aunque no de la forma simétrica.

Esta distribución tiene dos parámetros: $n$ y $p$, sin embargo, en muchas aplicaciones en la vida práctica el número de repeticiones $n$ es conocido, y la distribución dependerá sólo del valor $p$ que sería el parámetro de la distribución con espacio paramétrico $\bTheta=(0,1)$. Sin embargo, también hay situaciones donde

Algunas propiedades de la distribución binomial se enuncia a continuación.
\begin{Res}
Si $X$ es una variable aleatoria con distribución binomial con parámetros $n$ y $p$, entonces
    \begin{enumerate}
        \item $E(X)=np$.
        \item $Var(X)=np(1-p)$.
        \item $m_X(t)=(pe^t+1-p)^n$.
    \end{enumerate}
\end{Res}
\begin{proof}
La demostración de estas propiedades es bien conocida en la literatura de probabilidad. Y el procedimiento es encontrar la función generadora de momentos usando el teorema binomial, y usando ésta para encontrar la esperanza y la varianza.
\end{proof}

\textbf{Observación:} El lector puede chequear fácilmente que la distribución Bernoulli es un caso particular de la distribución binomial cuando $n=1$. Y el Resultado 1.1.2 también se puede obtener del anterior resultado con $n=1$. También podemos ver que en una distribución binomial, el valor más probable está cercano de la esperanza de la distribución, por ejemplo, en la distribución $Bin(10,0.35)$, la esperanza está dada por 3.5, mientras de la Figura 1.5 se observa que el valor más probable es 3. De allí, podemos sacar conclusiones muy sencillas sin mayores cálculos. Por ejemplo, la empresa de ventas virtuales sabe por experiencia que la probabilidad de obtener una venta exitosa con un correo enviado es del 0.04, entonces si envía 200 correos, lo más probable es que obtenga aproximadamente $0.04*200=8$ ventas.

La generación de observaciones provenientes de una distribución binomial puede realizarse mediante el comando \verb"rbinom", por ejemplo, si queremos simular 100 valores provenientes de una distribución $Bin(10,0.35)$, podemos usar el código \verb"rbinom(1000,10,0.35)", y el histograma de un conjunto de datos simulados con esta instrucción está dada en la siguiente figura, donde se observa un comportamiento muy similar a la función de densidad teórica dada en la Figura 1.5.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{hist_binomial.eps}
\caption{Histograma de un conjunto de datos provenientes de $Bin(10,0.35)$.}
\end{figure}


Ahora usando la función generadora de momentos del Resultado 1.1.3, podemos establecer el siguiente resultado que ilustra la relación entre las distribuciones Bernoulli y binomial.

\begin{Res}
Sea $X_1$, $\cdots$, $X_n$ variables aleatorias independientes e idénticamente distribuidas con distribución Bernoulli con parámetro $p$,
entonces la variable $\sum_{i=1}^nX_i$ tiene distribución $Bin(n,p)$.
\end{Res}

\begin{proof}
La demostración radica en el hecho de que la función generadora de momentos caracteriza la distribución probabilística, entonces basta demostrar que la función generadora de momentos de $\sum_{i=1}^nX_i$ es la de una distribución $Bin(n,p)$. Tenemos lo siguiente:
\begin{align*}
m_{\sum X_i}(t)=E(e^{\sum tX_i})&=E(\prod_{i=1}^ne^{tX_i})\\
               &=\prod_{i=1}^nE(e^{tX_i})\ \ \ \ (\text{por independencia})\\
               &=\prod_{i=1}^n(pe^t+1-p)\ \ \ \ (\text{definición de $m_{X_i}(t)$})\\
               &=(pe^t+1-p)^n.
\end{align*}
Y el resultado queda demostrado.
\end{proof}

\subsubsection{Distribución hipergeométrica}
\begin{Defi}
Una variable aleatoria $X$ tiene distribución hipergeométrica con parámetros $n$, $R$ y $N$ si su función de densidad está dada por:
\begin{equation}
f_X(x)=P(X=x)=\frac{\binom{R}{x}\binom{N-R}{n-x}}{\binom{N}{n}},
\end{equation}
para $x$ entero entre $\max(n-N+R,0)$ y $\min(R,n)$, y se nota como $X\sim Hg(n,R,N)$.
\end{Defi}

Una variable con distribución hipergeométrica se da cuando se desea extraer $n$ unidades de un conjunto de $N$ objetos en total, que pueden dividir en dos grupos, el primero de $R$ unidades y el segundo de $N-R$. Entonces la variable definida como el número de unidades del primer grupo extraídas tiene distribución hipergeométrica con parámetros $n$, $R$ y $N$. Suponga que se desea extraer 6 estudiantes de 15 en total, donde 10 son hombres y 5 son mujeres, la variable $X$ definida como\emph{ el número de estudiantes hombres seleccionados} tiene distribución $Hg(6,10,15)$. Nótese que $X$ solo toma valores entre 1 y 6, puesto que al seleccionar 6 estudiantes, a lo más 5 mujeres pueden quedar en la muestra, es decir, por lo menos estará en la muestra. En la siguiente Figura, se muestra la función de densidad para esta distribución.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{hipergeometrica.eps}
\caption{Función de densidad de una distribución $Hg(6,10,15)$.}
\end{figure}

Algunas propiedades de la distribución hipergeométrica se enuncia a continuación.

\begin{Res}
Si $X$ es una variable aleatoria con distribución hipergeométrica con parámetros $n$, $R$ y $N$, entonces
    \begin{enumerate}
        \item $E(X)=\frac{nR}{N}$.
        \item $Var(X)=\frac{nR(N-R)(N-n)}{N^2(N-1)}$.
    \end{enumerate}
\end{Res}
El anterior resultado no incluye la función generadora de momentos, pues éste no ha resultado ser útil en la teoría relacionada con la distribución hipergeométrica.

La distribución hipergeométrica también es útil en la teoría de muestreo, tal como lo ilustrado por \citeasnoun{Tille}: considera una región agricultural que consiste en $N=2010$ fincas de donde se desea extraer una muestra aleatoria simple sin reemplazos\footnote{Cada elemento de la población tiene la misma probabilidad de pertenecer a la muestra y puede ser seleccionado a lo más una vez.} de tamaño $n=100$. Suponga que adicionalmente se dispone la información sobre el área total cultivada de cada finca, de donde se sabe que en la población total existen 1580 fincas de menos de 160 hectáreas (subgrupo 1) y 430 de más de 160 hectáreas (subgrupo 2). Aunque el tamaño muestral $n$ está fijo, el número de fincas del subgrupo 1 seleccionadas denotada por $n_1$ es aleatoria y sigue una distribución $Hg(100,1580,2010)$; también el número de fincas del subgrupo 2 seleccionadas $n_2$ sigue una distribución hipergeométrica $Hg(100,430,2010)$. Y usando el resultado anterior, podemos obtener que se $E(n_1)=100\times1580/2010=78.6$, esto es, se espera seleccionar 78 o 79 fincas del subgrupo 1.

Otro uso de la distribución hipergeométrica es el problema de captura-recaptura, donde se necesita estimar el tamaño de una población de interés, para eso, se identifican o de alguna forma marcar un número $R$ menor que $N$ de individuos, luego se deja que estos $R$ individuo se mezclen bien con el resto de la población. Después de esto, se selecciona $n$ individuos de la mezcla homogénea, y se cuenta el número, $x$, de individuos marcados que quedaron seleccionados. Dado que la población estaba homogénea al momento de la selección, podemos pensar que las proporciones de objetos marcados en la muestra y en la población deben ser similares, esto es,
\begin{equation}\label{captura}
\frac{x}{n}\approx\frac{R}{N},
\end{equation}
de donde se tiene que $N\approx nR/x$. Otra aplicación de la distribución hipergeométrica es cuando se desea estimar el tamaño de un subgrupo de una población conocida, el procedimiento es el mismo de la captura recaptura, y se tiene la relación de (\ref{captura}), de donde se tiene que $R\approx xN/n$. Suponga que en una ciudad existen 2396 empresas que pueden clasificar en empresas grandes, medianas o pequeñas según el número de empleados. Si en una muestra aleatoria simple sin reemplazos de tamaño 500 se encuentran 38 empresas grandes, podemos estimar el número total de empresas grandes en la población total como $38*2396/500\approx182$ empresas grandes. Más detalles sobre la estimación en las anteriores situaciones se describen en el siguiente capítulo.

\begin{figure}[!htb]
\centering
\includegraphics[bb=0 0 1400 899, scale=0.14]{Captura_Recaptura.jpg}
\caption{Ilustración del problema de captura recaptura.}
\end{figure}

Nótese que un experimento del tipo hipergeométrica está muy relacionado con un experimento Bernoulli, puesto que en la $i$-ésima extracción para $i=1,\cdots,n$, podemos definir $X_i$ como 1 si el resultado es uno de los $R$ objetos y 0 si no, de esta forma tenemos $n$ variables con distribución Bernoulli, y la variable $Hg(n,R,N)$ viene siendo la variable $X=X_1+\cdots+X_n$. Aunque las variables $X_1$, $\cdots$, $X_n$ son del tipo Bernoulli, no podemos afirmar que $X$ tiene distribución binomial, puesto que dado el mecanismo de selección, estas $n$ variables no son independientes. Sin embargo, bajo algunas condiciones, sí podemos afirmar que una distribución hipergeométrica puede ser aproximada como una distribución binomial, tal como lo afirma el siguiente resultado.

\begin{Res}
Dada una variable aleatoria $X$ con distribución $Hg(n,R,N)$, si se tiene que $R/N\rightarrow p$, con $0<p<1$ cuando $R,N\rightarrow\infty$, entonces, la función de densidad de $X$ tiende a la función de densidad de una distribución $Bin(n,p)$.
\end{Res}

Para estudiar la convergencia enunciada en el anterior resultado, se calculó la función de densidad de cuatro distribuciones hipergeométrica con diferentes valores de $R$ y $N$, y las respectivas distribución $Bin(n,R/N)$. El código de \verb"R" es como sigue y la figura arrojada se muestra en la Figura 1.9, y podemos observar de la gráfica resultante que la aproximación por medio de la distribución binomial puede ser muy adecuado para valores grandes de $R$ y $N$.

\begin{verbatim}
> Hg<-function(x,n,R,N){
+ x<-c(max(0,n-N+R):min(R,n))
+ res<-rep(NA,length(x))
+ for(i in 1:length(x)){
+ res[i]<-choose(R,x[i])*choose(N-R,n-x[i])/choose(N,n)}
+ return(res)
+ }
>
> par(mfrow=c(2,2))
>
> plot(Hg(x,30,20,50),type="h",main="Hg(30,20,50)",xlab="x",ylab="f(x)")
> lines(dbinom(x,n,20/50),col=2)
>
> plot(Hg(x,30,50,80),type="h",main="Hg(30,50,80)",xlab="x",ylab="f(x)")
> lines(dbinom(x,n,50/80),col=2)
>
> plot(Hg(x,30,100,200),type="h",main="Hg(30,100,200)",xlab="x",ylab="f(x)")
> lines(dbinom(x,n,100/200),col=2)
>
> plot(Hg(x,30,300,600),type="h",main="Hg(30,300,600)",xlab="x",ylab="f(x)")
> lines(dbinom(x,n,300/600),col=2)
\end{verbatim}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{hiper_binomial.eps}
\caption{Ilustración de la aproximación de la distribución hipergeométrica mediante la distribución binomial. (Línea roja indica la correspondiente distribución binomial)}
\end{figure}

Volviendo al problema de la selección de una muestra con $n=100$ de fincas que se dividen en dos grupos, podemos aproximar la variable aleatoria $n_1$ con una distribución $Bin(100,1580/2010)$, y de esta forma calcular probabilidades acerca de los posibles valores de $n_1$.

\subsubsection{Distribución Poisson}
La distribución Poisson debe su nombre al francés Siméon-Denis Poisson (1781-1840) quien descubrió esta distribución en el año 1838, cuando la usó para describir el número de ocurrencias de algún evento durante un intervalo de tiempo de longitud dada. Como de costumbre, damos la definición de esta distribución en términos de la función de densidad.

\begin{figure}[!htb]
\centering
\includegraphics[bb=0 0 511 599, scale=0.3]{Poisson.jpg}
\caption{Siméon-Denis Poisson (1781-1840)}
\end{figure}

\begin{Defi}
Una variable aleatoria $X$ tiene distribución Poisson con parámetros $\lambda>0$ si su función de densidad está dada por:
\begin{equation}
f_X(x)=P(X=x)=\frac{e^{-\lambda}\lambda^x}{x!}I_{\{0,1,\cdots\}}(x)
\end{equation}
y se nota como $X\sim P(\lambda)$.
\end{Defi}

La función de densidad de una distribución Poisson presenta un pico en valores cercanos al parámetro $\lambda$. En la Figura 1.11, se ilustra la densidad de una distribución $P(5.5)$, donde se observa que el valor con mayor probabilidad corresponde al valor 5, y a mediad que el valor de $x$ se aleja de 5, las probabilidades disminuyen.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{densidad_poisson.eps}
\caption{Función de densidad de una distribución $P(5)$.}
\end{figure}

Nótese que una variable con distribución Poisson puede tomar cualquier valor entero no negativo, y por esta razón, es usada frecuentemente para describir datos de conteo. Cuando en la práctica se presentan un conjunto de valores que son conteos, debe tener en cuenta la diferencia entre las distribuciones binomial, hipergeométrica y Poisson. En primer lugar, los valores que toma una variable con distribución Poisson no debe tener un límite superior, es decir, puede ser cualquier entero positivo. Por lo tanto, contextos donde la variable en cuestión no puede ser más grande que algún valor no debe ser considerada como una variable Poisson. En segundo lugar, tanto la distribución binomial como la hipergeométrica, la variable puede ser vista como un número de éxito obtenido en una sucesión de ensayos, mientras que la distribución Poisson carece de esta interpretación. De esta forma, una variable que describe, por ejemplo, número de accidentes automovilísticos en una determinada localidad, número de transacciones en una entidad durante diez minutos, o en general, número de eventos ocurridos en un punto geográfico y/o en un determinado rango del tiempo puede ser vista como una variable con distribución Poison.

Por otro lado, aunque una variable con distribución Poisson solo toma valores enteros, el parámetro de la distribución puede ser cualquier número real positivo, esto es, el espacio paramétrico de la distribución es $\bTheta=(0,\infty)$.

Ahora, cuando no se puede conoce la procedencia de un conjunto de los datos, podemos utilizar el histograma de éstos para identificar la distribución de donde provienen, puesto que el histograma debe ser similar a la densidad teórica de la distribución. Considera el siguiente ejercicio: en \verb"R", la generación de números aleatorios puede ser llevado a cabo usando el comando \verb"rpois". En la Figura 1.12, se muestra el histograma de 300 datos provenientes de la distribución $P(5.5)$ generados usando la instrucción \verb"rpois(300,5.5)". Podemos observar que el histograma tiene un comportamiento muy similar a la función de densidad teórica de la distribución, presentando mayor frecuencia en el valor 5 y comportamiento decreciente para valores alejados de 5.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{hist_poisson.eps}
\caption{Histograma de un conjunto de datos provenientes de $P(5.5)$.}
\end{figure}

Algunas propiedades de la distribución Poisson se enuncian a continuación.

\begin{Res}
Si $X$ es una variable aleatoria con distribución Poisson con parámetro $\lambda$, entonces
    \begin{enumerate}
        \item $E(X)=\lambda$.
        \item $Var(X)=\lambda$.
        \item $m_X(t)=\exp\{\lambda(e^t-1)\}$.
    \end{enumerate}
\end{Res}

\begin{proof}
La demostración del anterior resultado consiste en encontrar la función generadora de momentos, $m_X(t)$, usando directamente su definición y el hecho de que $e^u=\sum_{i=0}^{\infty}\frac{u^i}{i!}$. Una vez encontrada $m_X(t)$, se encuentra la esperanza y la varianza de manera habitual.
\end{proof}

El anterior resultado a parte de proveernos propiedades de la distribución Poisson, también nos brinda una herramienta a la hora de identificar la distribución de datos que no se conoce la procedencia, puesto que de acuerdo al resultado anterior, si un conjunto de datos proviene de la distribución Poisson, entonces el promedio debe ser cercano a la varianza, más aún, el promedio y la varianza debe ser cercano al parámetro de la distribución. En la práctica, una distribución que puede ser confundida con la distribución Poisson es la distribución binomial, puesto que ambos toman valores enteros positivos, pero en la distribución Binomial, la varianza teórica está dada por $np(1-p)$ la cual es siempre menor que la esperanza $np$, situación que no ocurre si se tratara de una distribución Poisson.

Para corroborar la anterior afirmación en la práctica, se simuló muestras de tamaño 10, 30, 50, 100, 300, 500 y 1000 que provienen de la distribución $P(5)$ y $Bin(20,0.25)$, y en cada muestra se calculó el promedio y la varianza, el código en \verb"R" es como sigue, y tiene como resultado la Figura 1.13, donde podemos observar que en las muestras con distribución Poisson, la varianza muestral se asemeja al promedio muestral, mientras que en las muestras con distribución binomial, la varianza siempre estuvo por debajo del promedio con una diferencia considerable, corroborando las propiedades teóricas. De lo anterior, podemos concluir que en un conjunto de datos, si la varianza muy similar al promedio, hay más evidencia a favor de la distribución Poisson que la binomial; mientras que si la varianza es considerablemente menor que el promedio, se puede decir que la distribución binomial ajusta mejor a los datos.


\begin{verbatim}
> set.seed(1234)
>
> n<-c(10,30,50,100,300,500,1000)
> mp<-matrix(NA)
> vp<-matrix(NA)
>
> for(i in 1:length(n)){
+ a<-rpois(n[i],5)
+ mp[i]<-mean(a)
+ vp[i]<-var(a)
+ }
>
> #####################
> mb<-matrix(NA)
> vb<-matrix(NA)
>
> for(i in 1:length(n)){
+ b<-rbinom(n[i],20,5/20)
+ mb[i]<-mean(b)
+ vb[i]<-var(b)
+ }
>
> par(mfrow=c(2,1))
>
> plot(mp,type="b",ylim=c(0,7),ylab="",xaxt="n",xlab="n",main="Poisson(5)")
> lines(vp,type="b", pch=4)
> axis(1,1:length(n),n)
> legend("bottomright",c("Promedio","Varianza"), pch=c(1,4),bty="n")
>
>
> plot(mb,type="b",ylim=c(0,7),ylab="",xaxt="n",xlab="n",main="Binomial(20,0.25)")
> lines(vb,type="b", pch=4)
> axis(1,1:length(n),n)
> legend("bottomright",c("Promedio","Varianza"), pch=c(1,4),bty="n")
\end{verbatim}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.7]{poisson_binomial.eps}
\caption{Promedio y varianza de muestras provenientes de distribuciones $P(5.5)$ y $Bin(20,0.25)$.}
\end{figure}

Como mencionaba anteriormente, en la práctica si no se conoce la procedencia de los datos, sino solo los valores, unos datos provenientes de la distribución Poisson podrían confundirse con la distribución binomial. El siguiente resultado nos plantea una relación entre estas dos distribuciones bajo algunas circunstancias especiales.

\begin{Res}
Considera $n$ eventos del tipo Bernoulli, donde $p$ denota la probabilidad del éxito de cada uno de los $n$ eventos. Si el valor de $p$ es pequeño y $np\rightarrow\lambda>0$ cuando $n\rightarrow\infty$, entonces la variable $X$ con distribución $Bin(n,p)$ se distribuye aproximadamente con distribución $P(\lambda)$.
\end{Res}

En el anterior resultado, existe dos condiciones para garantizar la convergencia, la probabilidad de éxito en cada ensayo $p$ debe ser pequeño y el número de ensayos $n$ debe ser grande. Para hacer una idea sobre qué tan importante son estas dos condiciones, se elaboró la Figura 1.14 donde ilustra funciones de densidad de distribución binomial con diferentes valores de $n$ y de $p$, y también la correspondiente distribución Poisson. Se observa que efectivamente a medida que aumenta el valor de $p$, la aproximación se torna cada vez más mala, sin importar el tamaño muestral $n$. Por otro lado, se observa que la condición de que $p$ sea pequeña es más importante que la condición de que $n$ sea grande, puesto que para la distribución $Bin(10,0.2)$, aunque $n$ sea pequeña, la aproximación sigue siendo buena.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.8]{binomial_poisson.eps}
\caption{Ilustración de la aproximación de la distribución Poisson mediante la distribución binomial cuando $p$ es pequeño. (Línea roja indica la correspondiente distribución Poisson)}
\end{figure}

Ahora, aunque en la Figura 1.14, se observó que cuando $p$ es pequeña, la aproximación por la distribución Poisson resulta no adecuada, podemos transformar a una variable $X\sim Bin(n,p)$ con $p$ grande para que sigue siendo la válida la aproximación. En este caso, es fácil ver que la variable $Y=n-X\sim Bin(n,1-p)$, si $p$ es grande, $1-p$ es pequeño, entonces para calcular $f_X(x)=P(X=x)$, tenemos que ésta es igual a $P(Y=n-x)$, y utilizando la distribución Poisson para aproximar la distribución de $Y$ tenemos que
\begin{equation*}
P(X=x)=P(Y=n-x)\approx \frac{e^{-\lambda}\lambda^{n-x}}{(n-x)!},
\end{equation*}
\begin{figure}[!htb]
\centering
\includegraphics[scale=0.8]{binomial_poisson1.eps}
\caption{Ilustración de la aproximación de la distribución Poisson mediante la distribución binomial cuando $p$ es grande. (Línea roja indica la correspondiente distribución Poisson)}
\end{figure}

con $\lambda=n(1-p)$. En la Figura 1.15, se ilustra la bondad de la anterior aproximación para densidades de la distribución binomial con distintos valores de $p$ y $n$, se observa que la aproximación es bastante buena para valores grandes de $p$, mientras que cuando $p$ toma un valor cercano al 0.5, la anterior aproximación es muy similar a la presentada en el Resultado 1.1.8.


Una propiedad interesante de la distribución Poisson es el hecho de que la suma de variables independientes con distribución Poisson sigue teniendo la distribución Poisson. Lo anterior lo afirma el siguiente resultado.

\begin{Res}
Sea $X_1$, $\cdots$, $X_n$ variables aleatorias independientes con distribución $P(\lambda_i)$ para $i=1,\cdots,n$, entonces la variable $\sum_{i=1}^nX_i$ tiene distribución $P(\sum_{i=1}^n\lambda_i)$.
\end{Res}
\begin{proof}
Utilizando la función generadora de momentos, análogo a la demostración del Resultado 1.1.5.
\end{proof}

Suponga que una central telefónica de atención de clientes cuenta con 5 operadores en cada turno, la variable de interés es el número de llamadas que atienden el central durante 5 minutos. Un estudio acerca del rendimiento de los 5 operadores de turno revelan que el número de llamadas que atiende en 5 minutos sigue una distribución Poisson con parámetros 2, 3, 2, 4 y 3, respectivamente. Si además los operadores trabajan de forma independiente, entonces el anterior resultado garantiza que el número total de llamadas atendidas en 5 minutos puede ser modelado como una distribución $P(14)$, y podemos calcular probabilidades acerca de esta variable de la manera habitual.

\subsection{Distribuciones continuas}
En esta parte del libro consideraremos las distribuciones continuas, esto es, algunas distribuciones comunes para variables aleatorias continuas.
\subsubsection{Distribución Uniforme Continua}
Una de las distribuciones continuas más simples es la distribución uniforme continua sobre un intervalo $[a,b]$, la cual se caracteriza en que para un subintervalo de $[a,b]$ de longitud fija, una variable con esta distribución tiene la misma probabilidad de ubicarse en cualquier de estos subintervalos. Por lo consiguiente, esta distribución es apropiada para situaciones donde para un experimento no hay resultados que son más probables que otros, un aspecto similar a la distribución uniforme discreta. De esta forma, si suponemos que el primer bus puede demora a lo más 15 minutos al portal de transporte, y puede llegar en cualquier momento en ese rango, en este caso, la variable definida como el tiempo de llegada del bus tiene una distribución uniforme $[0,15]$, claramente el límite inferior 0 está dado por el contexto del problema y la naturaleza de la variable.

La definición de esta distribución en términos de la función de densidad está dada a continuación.

\begin{Defi}
Una variable aleatoria $X$ tiene distribución uniforme continua sobre el intervalo $[a,b]$ con $a<b$ si su función de densidad está dada por:
\begin{equation}
f_X(x)=\frac{1}{b-a}I_{[a,b]}(x),
\end{equation}
y se denotará por $X\sim U[a,b]$.
\end{Defi}

Análogo a lo discutido en la parte de la distribución uniforme discreta, cuando los datos provienen de la distribución $U[a,b]$, entonces el histograma debe ser plana, similar a la función de densidad teórica. Para ver eso, simulamos dos muestras provenientes de la distribución $U[1,3]$ del tamaño 500 y 1000 usando la instrucción \verb"runif", y graficamos los correspondientes histogramas. El código usado es

\begin{verbatim}
set.seed(123)
n<-c(500,1000)
par(mfrow=c(1,2))
for(i in 1:length(n)){
a<-n[i]
hist(runif(a,1,3),main="",xlab=a,ylab="Frecuencia")
}
\end{verbatim}


\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{random_unif_continua.eps}
\caption{Histograma de valores simulados de una distribución $U[1,3]$ con tamaño de muestra 500 y 1000.}
\end{figure}

Y la resultante gráfica está dada en la Figura 1.16, donde se observa que efectivamente no hay algún patrón reconocible en estos histogramas, sino que cada clase tiene aproximadamente la misma frecuencia, características propias de una distribución uniforme continua.

La generación de números aleatorios de una distribución $U[0,1]$ es particularmente importante, puesto que son de utilidad para simular otras distribuciones más complicadas que la distribución uniforme. El procedimiento viene dado por el siguiente resultado tomado de \citeasnoun{Rober-Case}.
\begin{Res}
Si $U\sim U(0,1)$ y $F(x)$ es una función de distribución, entonces la función de distribución de la  variable $F^{-}(U)$ está dada por $F$, donde $F^{-}$ denota la función inversa generalizada de $F$ dada por $F^{-}(u)=\inf\{x:\ F(x)\geq u\}$.
\end{Res}

El anterior resultado nos indica que si queremos simular $n$ valores a partir de una cierta distribución $F$, se debe, en primer lugar hallar la función de distribución inversa generalizada $F^{-}$\footnote{Cuando la inversa de $F$ existe, ésta coincide con la inversa generalizada.}, y en segundo lugar, simular $n$ observaciones de la distribución $U(0,1)$ que denotamos por $u_1$, $\cdots$, $u_n$, y finalmente el anterior resultado garantiza que los valores $F^{-}(u_1)$, $\cdots$, $F^{-}(u_n)$ provienen de la distribución $F$.

Por ejemplo, si queremos simular observaciones de la función de densidad $f(x)=e^{-x}I_{0,\infty}(x)$, esto es, la función de densidad de una distribución $Exp(1)$ que se describirá con mayor detalle más adelante. La función de distribución está dada por $F(x)=1-e^{-x}$, de donde la inversa de esta función está dada por $F^{-}(x)=-\ln(1-x)$, así que podemos simular observaciones usando esta función inversa. Ahora en el software \verb"R", se disponen las instrucciones para simular observaciones de la mayoría de las distribuciones de probabilidades, y podemos utilizarlos directamente sin tener que recurrir al anterior resultado manualmente. El siguiente comando simula 100 observaciones de la distribución $Exp(1)$ con el Resultado 1.1.10 y usando la instrucción \verb"rexp" incorporado en \verb"R". En la Figura 1.17 se observa las histogramas de los valores obtenidos de las dos formas de simulación, donde se puede observar la similitud en las estructuras de los datos, y por facilidad, usaremos en este libro las instrucciones de \verb"R".

\begin{verbatim}
> set.seed(1234)
> n<-100
> u<-runif(100)
> e<--log(1-u)
> e1<-rexp(100,1)
> par(mfrow=c(1,2))
> hist(e,ylab="Frecuencia",main="(a)")
> hist(e1,ylab="Frecuencia",main="(b)")
\end{verbatim}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{exp_unif.eps}
\caption{Histogramas de 100 observaciones simulados de una distribución $Exp(1)$, (a) con el Resultado 1.1.10, (b) con la instrucción rexp.}
\end{figure}

Las siguientes propiedades de una distribución uniforme continua se pueden comprobar fácilmente.
\begin{Res}
Si $X$ es una variable aleatoria con distribución uniforme continua sobre $[a,b]$, entonces
    \begin{enumerate}
        \item $E(X)=\frac{a+b}{2}$.
        \item $Var(X)=\frac{(b-a)^2}{12}$.
        \item $m_X(t)=\frac{e^{bt}-e^{at}}{(b-a)t}$.
    \end{enumerate}
\end{Res}

\subsubsection{Distribución Gamma}
La distribución Gamma es una distribución muy importante, puesto que muchas distribuciones de uso común, como la distribución exponencial y la distribución chi-cuadrado, son casos particulares de esta distribución. La definición de esta distribución en término de la función de densidad de probabilidad está dada por

\begin{Defi}
Una variable aleatoria $X$ tiene distribución Gamma con parámetro de forma $k>0$ y parámetro de escala $\theta>0$ si su función de densidad está dada por:
\begin{equation}
f_X(x)=\frac{x^{k-1}e^{-x/\theta}}{\theta^k\Gamma(k)}I_{(0,\infty)}(x),
\end{equation}
y en este libro, se usará la notación $X\sim Gamma(k,\theta)$.
\end{Defi}

La distribución Gamma tiene dos parámetros: $k$ que se denomina el parámetro de forma y $\theta$ el de escala, en este caso, el vector de parámetros es $\btheta=(k,\theta)'$ donde el espacio paramétrico está dado por $\bTheta=(0,\infty)\times(0,\infty)$. Pero cuando uno de los dos parámetros es fijo, por ejemplo, si $\theta$ es fijo, entonces la distribución tendría un sólo parámetro: $k$.

Nótese que en primer lugar, una variable con distribución solo puede tomar valores positivos, y en segundo lugar, la función de densidad no es simétrica donde el coeficiente de asimetría positivo. En la Figura 1.3, se muestran algunas funciones de densidad de la distribución Gamma donde se observa claramente la característica no simétrica.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{distribucion_gamma.eps}
\caption{Funciones de densidad de la distribución Gamma, negro: $k=3$, $\theta=0.5$, rojo: $k=2$, $\theta=1$, verde: $k=1$, $\theta=1$.}
\end{figure}

Datos que cuentan con la estructura de la distribución Gamma son los datos del ingreso salarial, puesto que la mayoría de la población tiene ingreso inferior a, por ejemplo, los 500 mil pesos colombianos, y a medida que aumenta el salario, menor número de individuos puede obtener este ingreso. Observe la Figura 1.19 donde se dispone el histograma de datos que denotan el ingreso, nótese que la clases dominante se encuentra alrededor de los 500 mil pesos, y a medida que incrementa el ingreso, menos datos se ubican en ese rango, y además se presenta una cola larga hacia la derecha, las cuales son características propias de una distribución Gamma.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{hist_gamma.eps}
\caption{Histograma de un conjunto de 10 mil datos con características de una distribución Gamma.}
\end{figure}

Algunas propiedades de la distribución Gamma se enuncian a continuación.
\begin{Res}
Si $X$ es una variable aleatoria con distribución Gamma con parámetro de forma $k$ y parámetro de escala $\theta$, entonces
    \begin{enumerate}
        \item $E(X)=k\theta$.
        \item $Var(X)=k\theta^2$.
        \item $m_X(t)=\left(\frac{1}{1-\theta t}\right)^k$ para $t<1/\theta$, y no existe para otros valores de $t$.
    \end{enumerate}
\end{Res}

Del anterior resultado, podemos ver que el parámetro de escala $\theta$ se puede escribir como función de la esperanza y la varianza de la distribución como $\theta=Var(X)/E(X)$, y por consiguiente, se tiene que el parámetro de forma se puede escribir como $k=E(X)/\theta=(E(X))^2/Var(X)$. De esta forma, para un conjunto de datos, una vez haya identificado que siguen una distribución Gamma, podemos calcular el promedio y la varianza de los datos y usarlos para tener un acercamiento acerca los dos parámetros de la distribución como $\theta'=s^2/\bar{x}$ y $k'=\bar{x}^2/s^2$.\footnote{Este concepto se conoce como la estimación de los parámetros que se discutirá en el siguiente capítulo.} El siguiente programa en \verb"R" simula muestras de diferentes tamaños de muestra provenientes de una distribución $Gamma(3,2)$, y en cada una de estas muestras calculan $\theta'$ y $k'$.

\begin{verbatim}
> set.seed(1234)
> n<-c(10,30,50,100,300,500,1000)
> tg<-matrix(NA)
> kg<-matrix(NA)
>
> for(i in 1:length(n)){
+ d<-rgamma(n[i],shape=3,scale=2)
+ tg[i]<-var(d)/mean(d)
+ kg[i]<-mean(d)/tg[i]
+ }
>
> par(mfrow=c(2,1))
> plot(tg,type="b",ylab="",xaxt="n",xlab="n",main="Parámetro de escala")
> axis(1,1:length(n),n)
> abline(h=2)
>
> plot(kg,type="b",ylab="",xaxt="n",xlab="n",main="Parámetro de forma")
> axis(1,1:length(n),n)
> abline(h=3)
\end{verbatim}

Como resultado del anterior programa, se tiene la Figura 1.20, donde las dos líneas horizontales representan los valores verdaderos de $\theta$ y $k$, podemos observar que los valores de $\theta'$ y $k'$ se encuentran aproximadamente alrededor de los valores verdaderos, pero a medida que la muestra crece, no se observa mejora alguna.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{est_Gamma.eps}
\caption{Estimación de $k$ y $\theta$ en muestras provenientes de una distribución Gamma de diferentes tamaños.}
\end{figure}

La distribución Gamma tiene una buena propiedad que establece que la suma de variables con distribución Gamma puede seguir teniendo la distribución Gamma bajo algunos supuestos. Esta propiedad será útil para algunos desarrollos en los siguiente capítulos, y lo enunciamos en el siguiente resultado.
\begin{Res}
Sea $X_1$, $\cdots$, $X_n$ variables aleatorias independientes con distribución Gamma con parámetro de forma $k_i$ y parámetro de escala $\theta$ para $i=1,\cdots,n$, entonces la variable $\sum_{i=1}^nX_i$ tiene distribución Gamma con parámetro de forma $\sum_{i=1}^nk_i$ y parámetro de escala $\theta$.
\end{Res}
\begin{proof}
Análogo a la demostración del Resultado 1.1.4.
\end{proof}

\subsubsection{Distribución exponencial}
La distribución exponencial es un caso particular de la distribución Gamma cuando el parámetro de forma $k$ toma el valor 1,  y por consiguiente se puede obtener fácilmente la función de densidad dada a continuación.
\begin{Defi}
Una variable aleatoria $X$ tiene distribución exponencial con parámetro de escala $\theta>0$ si su función de densidad está dada por:
\begin{equation}
f_X(x)=\frac{1}{\theta}e^{-x/\theta}I_{(0,\infty)}(x),
\end{equation}
y en este libro, se usará la notación $X\sim Exp(\theta)$.
\end{Defi}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{densidad_exponencial.eps}
\caption{Función de densidad de distribuciones $Exp(2)$, $Exp(3)$ y $Exp(5)$.}
\end{figure}

Una variable exponencial toma valores en el intervalo $(0,\infty)$, y puede ser utilizado para describir el tiempo necesario para la ocurrencia de algún evento o la vida útil de un componente eléctrico. En la Figura 1.21 se muestra la función de densidad de la distribución exponencial con diferentes valores de $\theta$. En primer lugar, observe que la función de densidad es siempre decreciente, y por consiguiente, para una variable $X\sim Exp(\theta)$, se tiene que $P(t_1<X<t_1+\delta)<P(t_2<X<t_2+\delta)$ si $t_1>t_2$. Para ver la interpretación de eso, suponga que $X$ denota la vida útil (en años) de una referencia de lavadora, entonces es natural afirma que es más probable que la lavadora funcione entre 2 y 3 años que entre 6 y 7 años, esto es, $P(6<X<7)<P(2<X<3)$, que es una característica reflejada en la función de densidad de una distribución exponencial.

Más aún, suponga que dos tipos de lavadoras, A y B, tienen la vida útil (en años) que pueden ser descritos por la distribución $Exp(2)$ y $Exp(5)$, respectivamente. Entonces por el Resultado 1.1.12 donde provee propiedades de una distribución Gamma, podemos afirmar que la vida útil promedio de A y B son 2 y 5 respectivamente, es decir, las lavadoras del tipo B pueden funcionar por más años que los del tipo A. Así que intuitivamente podemos afirmar que la probabilidad de que una lavadora funcione más de 6 años debe ser mayor en las del tipo B que en el tipo A, y recordando que esta probabilidad corresponde al area bajo la función de densidad en el intervalo $(6,\infty)$, podemos observar que la anterior afirmación sí se refleja en la Figura 1.21. Dadas las anteriores observaciones, podemos ver por qué la distribución exponencial es usada para describir este tipo de variables.

Ahora, aunque muchos veces se han dicho en la literatura estadística que una variable aleatoria que denota el tiempo puede ser descrita por la distribución exponencial dado que ésta siempre toma valores positivos, característica propia de la variable tiempo. Pero la función de densidad de la distribución exponencial es siempre decreciente, y puede no ser apto para algunas situaciones. Por ejemplo, considera una central telefónica que atiende quejas de consumidores, si se desea estudiar la variable $X$ definida como el tiempo de duración de una llamada hecha por un consumidor, no es natural pensar que la probabilidad de que la llamada dura menos de un minuto sea mayor a la probabilidad de que dura menos de 5 minutos, esto es, puede no suceder que $P(X<1)>P(X<5)$, en otras palabras, no necesariamente, entre más corta sea la llamada, mayor probabilidad tiene asociada. Y en este caso, la distribución exponencial no resulta adecuado, sino posiblemente una distribución Gamma.

Ahora, como la distribución exponencial es un caso particular de la distribución Gamma, se puede obtener fácilmente sus propiedades usando el Resultado 1.1.12.

\begin{Res}
Si $X$ es una variable aleatoria con distribución exponencial con parámetro $\theta$, entonces
    \begin{enumerate}
        \item $E(X)=\theta$.
        \item $Var(X)=\theta^2$.
        \item $m_X(t)=\frac{1}{1-\theta t}$ para $t<1/\theta$, y no existe para otros valores de $t$.
    \end{enumerate}
\end{Res}

Nótese que la varianza de la distribución es la esperanza al cuadrado, de esta forma, si un conjunto de datos continuos positivo tiene la varianza aproximadamente igual al promedio al cuadrado, podríamos afirmar que los datos provienen de una distribución exponencial.


El siguiente resultado es un caso particular del Resultado 1.1.13 y será de utilidad en los siguientes capítulos.
\begin{Res}
Sea $X_1$, $\cdots$, $X_n$ variables aleatorias independientes e idénticamente distribuidas con distribución exponencial con parámetro de escala $\theta$, entonces la variable $\sum_{i=1}^nX_i$ tiene distribución Gamma con parámetro de forma $n$ y parámetro de escala $\theta$.
\end{Res}
\begin{proof}
Se deja como ejercicio.
\end{proof}

\subsubsection{Distribución normal}
La distribución normal también es llamada la distribución gaussiana, rindiendo homenaje al matemático alemán Carl Friedrich Gauss (1777-1855). Esta distribución es, sin duda, una de las distribuciones más importantes, y de uso más frecuente en la teoría estadística, puesto que una gran parte de la teoría estadística fue desarrollada inicialmente para variables con esta distribución; por el otro lado, gracias al teorema del límite central, muchas distribuciones ajenas a la normal, incluyendo las variables discretas, pueden ser aproximadas por ésta cuando el tamaño muestral es grande. Para detalles sobre la historia de la distribución normal, consulte a \citeasnoun{normal}.

\begin{figure}[!htb]
\centering
\includegraphics[bb=0 0 468 600, scale=0.3]{Gauss.jpg}
\caption{Carl Friedrich Gauss (1777-1855)}
\end{figure}

\begin{Defi}
Una variable aleatoria $X$ tiene distribución normal con parámetros $\mu$ y $\sigma^2$ si su función de densidad está dada por:
\begin{equation}
f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}I_\mathbb{R}(x),
\end{equation}
donde $\sigma>0$ y se nota como $X\sim N(\mu,\sigma^2)$.
\end{Defi}
La distribución normal tiene dos parámetros, representado como $\btheta=(\mu,\sigma^2)$ y $\bTheta=\mathbb{R}\times(0,\infty)$. Cada uno de los dos parámetros de la distribución normal determina un aspecto específico de la distribución. En primer lugar, se puede ver fácilmente que para cualquier valor $x$, se tiene que $f(\mu+x)=f(\mu-x)$, de donde se tiene que la función de densidad es simétrica con respecto a $\mu$. En segundo lugar, la función de densidad toma el valor máximo en el punto $x=\mu$, y el máximo es igual a $(2\pi\sigma^2)^{-1/2}$, de donde se tiene que entre más pequeño sea el valor de $\sigma^2$, el área bajo la función de densidad está más concentrado alrededor del valor $\mu$. En la Figura 1.23, se muestran algunas funciones de densidad para diferentes valores de $\mu$ y $\sigma^2$, donde se puede confirmar lo comentado anteriormente.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{densidad_normal.eps}
\caption{Función de densidad de una distribución normal con diferentes parámetros.}
\end{figure}

Ahora, haciendo una analogía entre el histograma de un conjunto de datos y una función de densidad, podemos sospechar que en primer lugar, si los datos provienen de una distribución normal, entonces el valor $\mu$, debe ubicarse alrededor del punto centro de los datos; y en segundo lugar, el valor $\sigma^2$ está asociada con la variación de los datos, ya que entre más pequeña sea ésta, más concentrado estarán los datos. El siguiente resultado confirma esta sospecha.

\begin{Res}
Si $X$ es una variable aleatoria con distribución normal con parámetros $\mu$ y $\sigma^2$, entonces
    \begin{enumerate}
        \item $E(X)=\mu$.
        \item $Var(X)=\sigma^2$.
        \item $m_X(t)=\exp\{\mu t+\frac{1}{2}\sigma^2t^2\}$.
    \end{enumerate}
\end{Res}

En la vida práctica, para ver que un conjunto de datos tenga distribución normal, una herramienta básica es examinar si el histograma de los datos tiene la forma llamada campana de Gauss, esto es, características similares a la función de densidad presenta en la Figura 1.23. La mayor frecuencia debe estar asociada con la clase media, y a medida que las clases se alejan de la clase media, la frecuencia debe disminuir simétricamente. En la Figura 1.24 se muestra el histograma de varios conjuntos de datos simulados usando la función \verb"rnorm" de \verb"R". Obsérvese que la característica de la distribución normal es muy preeminente en muestras grandes, mientras que en muestras pequeñas, la detección de la normalidad mediante el histograma puede ser inadecuado.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{hist_normal.eps}
\caption{Histograma de grupos de datos provenientes de la distribución normal con diferentes tamaños muestrales.}
\end{figure}

Una propiedad muy particular de la distribución normal es que la distribución se conserva para transformaciones lineales, y se enuncia a continuación.

\begin{Res}
Si $X\sim N(\mu,\sigma^2)$, y $\alpha$, $\beta$ son constantes, entonces la variable $\alpha X+\beta$ tiene distribución $N(\alpha\mu+\beta,\alpha^2\sigma^2)$.
\end{Res}
\begin{proof} Se usará el hecho de que la función generadora de momentos caracteriza la distribución probabilística. Se tiene que:
\begin{align*}
m_{\alpha X+\beta}(t)&=E(e^{t(\alpha X+\beta)})\\
                     &=E(e^{\alpha tX})e^{\beta t}\\
                     &=m_X(\alpha t)e^{\beta t}\\
                     &=e^{\mu\alpha t+\sigma^2\alpha^2t/2}e^{\beta t}\\
                     &=e^{(\alpha\mu+\beta)t+\sigma^2\alpha^2t/2}
\end{align*}
la cual es la función generadora de momentos de una distribución $N(\alpha\mu+\beta,\alpha^2\sigma^2)$, y el resultado queda demostrado.
\end{proof}

Como consecuencia inmediata del anterior resultado, se define la estandarización que es fundamental en la teoría relacionado con las distribuciones normales.

\begin{Defi}
Si $X\sim N(\mu,\sigma^2)$ con $\mu=0$ y $\sigma=1$, entonces se dice que $X$ tiene distribución normal estándar y usualmente se denota por $Z$.
\end{Defi}

Utilizando el Resultado 1.1.17, podemos comprobar que cual variable $X\sim N(\mu,\sigma^2)$ puede ser transformado a una variable $Z$ mediante una transformación lineal. Suponga que esta transformación se denota por $\alpha X+\beta$, entonces encontrar los valores de $\alpha$ y $\beta$ para los cuales la variable transformada tenga distribución normal estándar equivale a solucionar las siguientes igualdades para $\alpha$ y $\beta$

\begin{equation*}
\begin{cases}
\alpha\mu+\beta=0\\
\alpha^2\sigma^2=1
\end{cases}
\end{equation*}

de donde se tiene dos soluciones $\alpha_1=1/\sigma$, $\beta_1=-\mu/\sigma$ y $\alpha_2=-1/\sigma$, $\beta_2=\mu/\sigma$. Y de esta forma, hemos encontrado dos variables $Z_1=\frac{X-\mu}{\sigma}$ y $Z_2=\frac{\mu-X}{\sigma}$ con la distribución normal estándar. Sin embargo, se acostumbre a utilizar la transformación dada por la primera solución, y esta transformación se conoce como la estandarización, y la variable $Z=Z_1=\frac{X-\mu}{\sigma}$ se conoce como la variable $X$ estandarizada.

Análogo a las distribuciones Poisson, Gamma, la distribución normal también se conserva para suma de variables normales independientes, tal como lo muestra el siguiente resultado.
\begin{Res}
Sea $X_1$, $\cdots$, $X_n$ variables aleatorias independientes, donde $X_i\sim N(\mu_i,\sigma^2_i)$ con $i=1,\cdots,n$, entonces la variable $\sum_{i=1}^nX_i$ tiene distribución $N(\sum_{i=1}^n\mu_i,\sum_{i=1}^n\sigma_i^2)$.
\end{Res}
\begin{proof}
Se deja como ejercicio.
\end{proof}

Combinando los resultados 1.1.17 y 1.1.18, se puede establecer que el promedio de variables independientes con la misma distribución normal sigue teniendo distribución normal, y lo enunciamos a continuación.

\begin{Res}
Sea $X_1$, $\cdots$, $X_n$ variables aleatorias independientes e idénticamente distribuidas con distribución $N(\mu,\sigma^2)$, entonces la variable $\bar{X}=\sum_{i=1}^nX_i/n$ tiene distribución $N(\mu,\sigma^2/n)$.
\end{Res}

\begin{proof}
Se deja como ejercicio.
\end{proof}

Una de las razones por las que la distribución normal es de las más importantes en la teoría estadística radica en el hecho de que en un conjunto de variables independientes e idénticamente distribuidas no necesariamente con distribución normal, si el número de variables es grande, entonces la distribución del promedio de estas variables puede ser aproximada por la de una distribución normal. Lo anterior se conoce como el famoso Teorema del Límite Central y se enuncia a continuación.

\begin{Res}
Sea $X_1$, $X_2$, $\cdots$, una sucesión de variables aleatorias independientes e idénticamente distribuidas, suponga que las funciones generadora de momentos $m_{X_i}(t)$ existen existe en una vecindad de 0, y la esperanza común se denota por $\mu$ y varianza común se denota por $\sigma^2>0$, y se define $\bar{X}_{n}$ como $\sum_{i=1}^nX_i/n$, y la función de distribución de la variable $\sqrt{n}(\bar{X}_n-\mu)/\sigma$ se denota por $F_n(x)$, entonces se tiene que
\begin{equation*}
\lim_{n\rightarrow\infty}F_n(x)=\int_{-\infty}^x\dfrac{1}{\sqrt{2\pi}}e^{-y^2/2}dy,
\end{equation*}
es decir, la distribución límite de $\sqrt{n}(\bar{X}_n-\mu)/\sigma$ corresponde a la distribución normal estándar.
\end{Res}

\begin{proof}
Se probará que la función generadora de momentos de la variable $\sqrt{n}(\bar{X}_n-\mu)/\sigma$ converge a la función $e^{t^2/2}$ que corresponde a la función generadora de momentos de una distribución $N(0,1)$. Para eso primero se define $Z_i$ como la variable $X_i$ estandarizada, entonces $Z_i\sim N(0,1)$ para todo $i=1,2,\cdots$. Y podemos comprobar fácilmente que $\sqrt{n}(\bar{X}_n-\mu)/\sigma=\sum_{i=1}^nZ_i/\sqrt{n}$, entonces tenemos
\begin{align*}
m_{\sqrt{n}(\bar{X}_n-\mu)/\sigma}(t)&=m_{\sum_{i=1}^nZ_i/\sqrt{n}}(t)\\
&=\prod_{i=1}^nm_{Z_i}\left(\frac{t}{\sqrt{n}}\right)\ \ \ \ \ \ \text{por la independencia}\\
&=\left(m_Z\left(\frac{t}{\sqrt{n}}\right)\right)^n,
\end{align*}
donde $Z$ denota una variable con distribución normal estándar. Ahora, usamos la expansión de Taylor para la función $m_Z\left(\frac{t}{\sqrt{n}}\right)$ alrededor del punto 0. Para eso recordamos que si $g(x)$ es una función derivable de cualquier orden, entonces se puede expandir $g(x)$ alrededor de un punto $a$ como
\begin{equation}\label{Taylor_g}
g(x)=\sum_{i=0}^\infty\frac{g^{(i)}(a)(x-a)^i}{i!},
\end{equation}
donde $g^{(i)}(a)$ es la $i$-ésima derivada de $g(x)$ evaluada en $x=a$ y $g^{(0)}(a)=g(a)$.

De esta forma, tenemos que
\begin{align*}
m_Z\left(\frac{t}{\sqrt{n}}\right)&=\sum_{i=0}^\infty \frac{m_Z^{(i)}(0)\left(\frac{t}{\sqrt{n}}\right)^i}{i!}\\
&=m_Z(0)+m_Z'(0)\left(\frac{t}{\sqrt{n}}\right)+\frac{1}{2}m_Z''(0)\left(\frac{t}{\sqrt{n}}\right)^2+\sum_{i=3}^\infty \frac{m_Z^{(i)}(0)\left(\frac{t}{\sqrt{n}}\right)^i}{i!}\\
&=1+\frac{1}{2}\left(\frac{t}{\sqrt{n}}\right)^2+R\left(\frac{t}{\sqrt{n}}\right),
\end{align*}
donde
\begin{equation*}
R\left(\frac{t}{\sqrt{n}}\right)=\sum_{i=3}^\infty \frac{m_Z^{(i)}(0)\left(\frac{t}{\sqrt{n}}\right)^i}{i!}.
\end{equation*}
Y por consiguiente, tenemos que
\begin{align}\label{Taylor_lim}
\lim_{n\rightarrow\infty}m_{\sqrt{n}(\bar{X}_n-\mu)/\sigma}(t)&=\lim_{n\rightarrow\infty}\left(m_Z\left(\frac{t}{\sqrt{n}}\right)\right)^n\notag\\
&=\lim_{n\rightarrow\infty}\left(1+\frac{1}{2}\left(\frac{t}{\sqrt{n}}\right)^2+R\left(\frac{t}{\sqrt{n}}\right)\right)^n\notag\\
&=\lim_{n\rightarrow\infty}\left[1+\frac{1}{n}\left(\frac{t^2}{2}+nR\left(\frac{t}{\sqrt{n}}\right)\right)\right]^n.
\end{align}

Por otro lado, el teorema de Taylor afirma que para la función $g$ en (\ref{Taylor_g}), se tiene que
\begin{equation*}
\lim_{x\rightarrow a}\frac{\sum_{i=r+1}^{\infty}\frac{g^{(i)}(a)(x-a)^i}{i!}}{(x-a)^r}=0.
\end{equation*}
Aplicando lo anterior a $m_Z\left(\frac{t}{\sqrt{n}}\right)$ con $r=2$, tenemos que
\begin{equation*}
\lim_{\frac{t}{\sqrt{n}}\rightarrow0}\frac{R\left(\frac{t}{\sqrt{n}}\right)}{\left(\frac{t}{\sqrt{n}}\right)^2}=0,
\end{equation*}
la cual es equivalente a
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{R\left(\frac{t}{\sqrt{n}}\right)}{\left(\frac{1}{\sqrt{n}}\right)^2}=\lim_{n\rightarrow\infty}nR\left(\frac{t}{\sqrt{n}}\right)=0,
\end{equation*}
para todo $t$. Y por consiguiente
\begin{equation}\label{Taylor_exp}
\lim_{n\rightarrow\infty}\left(\frac{t^2}{2}+nR\left(\frac{t}{\sqrt{n}}\right)\right)=\frac{t^2}{2}.
\end{equation}
Finalmente combinado (\ref{Taylor_lim}) y (\ref{Taylor_exp}), y usando el hecho de que si una sucesiones de números $a_n\rightarrow a$, entonces $\lim_{n\rightarrow\infty}\left(1+\frac{a_n}{n}\right)^n=e^a$, se tiene que
\begin{equation*}
\lim_{n\rightarrow\infty}\left[1+\frac{1}{n}\left(\frac{t^2}{2}+nR\left(\frac{t}{\sqrt{n}}\right)\right)\right]^n=e^{t^2/2},
\end{equation*}
y en conclusión
\begin{equation*}
\lim_{n\rightarrow\infty}m_{\sqrt{n}(\bar{X}_n-\mu)/\sigma}(t)=e^{t^2/2}.
\end{equation*}
En conclusión, la distribución límite de $\sqrt{n}(\bar{X}_n-\mu)/\sigma$ corresponde a la distribución normal estándar.
\end{proof}

En el anterior teorema exige la existencia de la función generadora de momentos de las variables $X_1$, $X_2$, $\cdots$, en general se puede demostrar la validez del resultado aún sin este supuesto, y la demostración se realiza por medio de funciones características de manera análoga.

El teorema del límite central es una herramienta muy poderosa en el sentido de que se aplica para la mayoría de las distribuciones de probabilidad, sin embargo, el teorema no nos brinda una medida de qué tan buena es la aproximación, y se debe examinar para cada distribución. En las figuras 1.25 y 1.26, se muestra las distribuciones muestrales del promedio en muestras simuladas distribuciones $P(3)$ y $Gamma(3,2)$ con tamaños de muestral 5, 10, 30, 50, 100 y 500 respectivamente y podemos observar que efectivamente la distribución $\bar{X}$ se aproxima a una distribución normal, especialmente para muestras grandes.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{TLC_1.eps}
\caption{Distribución muestral del promedio en muestras con distribución $P(3)$ de diferentes tamaños.}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{TLC_2.eps}
\caption{Distribución muestral del promedio en muestras con distribución $Gamma(3,2)$ de diferentes tamaños.}
\end{figure}


\subsubsection{Distribución chi-cuadrado}
Otra distribución como caso particular de la distribución Gamma es la distribución chi-cuadrado con $n$ grados de libertad es un caso particular de la distribución Gamma cuando el parámetro de forma $k$ toma el valor $n/2$ para algún $n$ entero, y el parámetro de escala $\theta$ toma el valor 2. La función de densidad de la distribución chi-cuadrado se muestra en la siguiente definición.

\begin{Defi}
Una variable aleatoria $X$ tiene distribución chi-cuadrado con $n$ grados de libertad, con $n$ entero positivo, si su función de densidad está dada por:
\begin{equation}
f_X(x)=\frac{x^{(n/2)-1}e^{-x/2}}{2^{n/2}\Gamma(n/2)}I_{(0,\infty)}(x),
\end{equation}
y se nota como $X\sim\chi^2_n$.
\end{Defi}

Aunque la distribución chi-cuadrado es un caso particular de la distribución Gamma, pero ésta está íntimamente relacionada con la distribución normal, tal como se muestra en la siguiente definición equivalente a la anterior, y resulta ser muy útil en el momento de demostrar que una variable tiene la distribución chi-cuadrado.

\begin{Defi}
Si $Z_1$, $\cdots$, $Z_n$ son variables aleatorias independientes e idénticamente distribuidas con distribución normal estándar, entonces la variable $\sum_{i=1}^nZ_i^2$ tiene distribución chi-cuadrado con $n$ grados de libertad.
\end{Defi}

Dado que la distribución $\chi^2$ es un caso particular de la distribución Gamma, la función de densidad también tiene facetas similares tales como la no simetría y la cola larga. En la Figura 1.27 se muestra la función de densidad de distribución $\chi^2_n$ para diferentes valores de $n$.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{densidad_chi.eps}
\caption{Función de densidad de distribuciones chi-cuadrado con diferentes grados de libertad.}
\end{figure}

Usando el Resultado 1.1.12 para la distribución Gamma, se tiene fácilmente las siguientes propiedades de una variable con distribución chi-cuadrado.
\begin{Res}
Si $X$ es una variable aleatoria con distribución chi-cuadro con $n$ grados de libertad, entonces
    \begin{enumerate}
        \item $E(X)=n$.
        \item $Var(X)=2n$.
        \item $m_X(t)=\left(\frac{1}{1-2t}\right)^{n/2}$ para $t<1/2$, y no existe para otros valores de $t$.
    \end{enumerate}
\end{Res}

A continuación se presenta un resultado que nos será muy útil en los capítulos futuros.

\begin{Res}
Sea $X_1$, $\cdots$, $X_m$ variables aleatorias independientes con distribución $\chi^2_{n_i}$ para $i=1,\cdots,m$, entonces la variable $X=\sum_{i=1}^mX_i$ tiene distribución chi-cuadrado con $\sum_{i=1}^mn_i$ grados de libertad.
\end{Res}
\begin{proof}
Se hará uso de la función generadora de momentos, tenemos que
\begin{align*}
m_{X}(t)&=E(e^{t\sum_{i=1}^mX_i})\\
&=\prod_{i=1}^nE(e^{tX_i})\\
&=\prod_{i=1}^nm_{X_i}(t)\\
&=\prod_{i=1}^n\left(\frac{1}{1-2t}\right)^{n_i/2}\\
&=\left(\frac{1}{1-2t}\right)^{\sum n_i/2},
\end{align*}
la cual corresponde a la función generadora de momentos de una distribución $\chi^2$ con grado de libertad $\sum_{i=1}^mn_i$, y el resultado queda demostrado.
\end{proof}

El anterior resultado establece que la suma de variables independientes con distribución $\chi^2$ sigue teniendo la distribución $\chi^2$. ¿Se puede afirmar que la resta de dos variables independientes $\chi^2$ sigue manteniendo la distribución $\chi^2$? Suponga que $X\sim\chi^2_{n_1}$ y $Y\sim\chi^2_{n_2}$ son independientes, y sea $Z=X-Y$, entonces
\begin{align*}
m_Z(t)&=E(e^{t(X-Y)})\\
&=E(e^{tX})/E(e^{tY})\ \ \ \ \ \ \ \text{Por ser $X$ y $Y$ independientes}\\
&=(1-2t)^{-n_1/2}/(1-2t)^{-n_2/2}\\
&=(1-2t)^{-(n_1-n_2)/2},
\end{align*}
la cual corresponde a la función generadora de momentos de una distribución $\chi^2$ con grado de libertad $n_1-n_2$ siempre y cuando $n_1>n_2$. Por lo anterior, podemos afirmar que la resta de dos variables independientes $\chi^2$ sigue teniendo la distribución $\chi^2$ siempre y cuando la resta de los dos grados de libertad sea positivo.

\subsubsection{Distribución $t$-student}
Otra distribución de vital importancia en la teoría estadística es la denominada distribución $t$-student. El descubrimiento de esta distribución fue publicado por  el estadístico inglés William Sealy Gosset (1876-1937) en el año 1908 cuando trabajaba en la famosa empresa cervecera Guinness. La publicación lo hizo de forma anónimo bajo el nombre de Student, pues Guinness le prohibía la publicación por ser el descubrimiento parte de resultados de investigación realizado por la empresa. La definición de esta distribución se da a continuación.
\begin{figure}[!htb]
\centering
\includegraphics[bb=0 0 427 600, scale=0.3]{Gosset.jpg}
\caption{William Sealy Gosset (1876-1937)}
\end{figure}

\begin{Defi}
Una variable aleatoria $X$ tiene distribución t-student con $n$ grados de libertad si su función de densidad está dada por:
\begin{equation}
f_X(x)=\frac{\Gamma(\frac{n+1}{2})}{\sqrt{\pi n}\ \Gamma(\frac{n}{2})}\left(1+\frac{x^2}{n}\right)^{-(n+1)/2}I_\mathbb{R}(x),
\end{equation}
donde $n>0$ y se nota como $X\sim t_n$.
\end{Defi}
Otra definición que se encuentra frecuentemente en la literatura estadística es la siguiente que es más útil que la definición anterior para demostrar que una variable tiene distribución $t$.

\begin{Defi}
Sea $Z$ una variable aleatoria con distribución normal estándar y $Y$ una variable aleatoria con distribución chi-cuadrado con $n$ grados de libertad, si $Z$ y $Y$ son independientes, entonces la variable $\frac{Z}{\sqrt{Y/n}}$ tiene distribución t-student con $n$ grados de libertad.
\end{Defi}

La función de densidad de la distribución t-student es muy parecida a la de distribución normal estándar, tiene la forma de campana de Gauss y simétrica con respecto al valor 0, además entre más grande sea el grado de libertad, más se parece a la distribución normal estándar. En la Figura 1.29 se muestra la función de densidad de la distribución normal estándar y la de distribución $t$ con diferentes grados de libertad donde podemos observar la similitud entre estas distribuciones.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{densidad_t.eps}
\caption{Funciones de densidad de la distribución $N(0,1)$ y $t_2$ con diferentes grados de libertad.}
\end{figure}

Algunas propiedades de la distribución t-student se muestran en el siguiente resultado.
\begin{Res}
Si $X$ es una variable aleatoria con distribución t-student con $n$ grados de libertad, entonces
    \begin{enumerate}
        \item $E(X)=0$ para $n>1$.
        \item $Var(X)=\frac{n}{n-2}$ para $n>2$.
    \end{enumerate}
\end{Res}

En primer lugar, nótese que cuando $n$ es grande, la varianza de $X$ se aproxima al valor 1, la varianza de una distribución normal estándar. Por otro lado, cabe resaltar que la distribución $t$-student no tiene función generadora de momentos.

\subsubsection{Distribución F}
Otra distribución muy útil es la distribución $F$ que también se conoce como la distribución F de Fisher o distribución de Fisher-Snedecor, haciendo referencia al gran estadístico Ronald Aylmer Fisher (1890-1962) y el fundador del primer departamento de estadística en los Estados Unidos, George Waddel Snedecor (1881-1974).

\begin{figure}[!htb]
\centering
\includegraphics[bb=0 0 268 326, scale=0.4]{Fisher.jpg}
\caption{Ronald Aylmer Fisher (1890-1962)}
\end{figure}

La función de densidad de la distribución F se da en la siguiente definición.
\begin{Defi}
Una variable aleatoria $X$ tiene distribución F con $m$ grados de libertad en el numerador y $n$ grados de libertado en el denominador si su función de densidad está dada por:
\begin{equation}
f_X(x)=\frac{\Gamma(\frac{m+n}{2})}{\Gamma(\frac{m}{2})\Gamma(\frac{n}{2})}\left(\frac{m}{n}\right)^{m/2}\frac{z^{\frac{m}{2}-1}}{\left(1+\frac{m}{n}z\right)^{\frac{m+n}{2}}}I_{(0,\infty)}(x),
\end{equation}
y se nota como $X\sim F^m_n$.
\end{Defi}
Otra definición equivalente pero más útil de la distribución $F$ es como sigue:
\begin{Defi}
Sea $X$ y $Y$ variables aleatorias independientes con distribuciones chi-cuadrado con $m$ y $n$ grados de libertad, respectivamente, entonces la variable $\dfrac{X/m}{Y/n}$ tiene distribución F con $m$ grados de libertad en el numerador y $n$ grados de libertado en el denominador.
\end{Defi}

En la Figura 1.31 se observa la función de densidad de la distribución $F$ para diferentes grados de libertad, podemos observar que ésta es similar a la de una distribución Gamma, no simétrica con cola larga y en algunos casos similar a la distribución exponencial.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{densidad_F.eps}
\caption{Funciones de densidad de la distribución $F$ con diferentes grados de libertad.}
\end{figure}

Algunas propiedades de la distribución F se dan a continuación.
\begin{Res}
Si $X$ es una variable aleatoria con distribución F con $m$ grados de libertad en el numerador y $n$ grados de libertado en el denominador, entonces
    \begin{enumerate}
        \item $E(X)=\frac{n}{n-2}$ para $n>2$.
        \item $Var(X)=\frac{2n^2(m+n-2)}{m(n-2)^2(n-4)}$ para $n>4$.
        \item La distribución $F$ no tiene función generadora de momentos.
    \end{enumerate}
\end{Res}

Usando la Definición 1.1.16, se tiene fácilmente el siguiente resultado.
\begin{Res}
Si $X\sim F^m_n$, entonces $1/X\sim F^n_m$.
\end{Res}

\subsubsection{Relaciones de las distribuciones}

\begin{figure}[!htb]
\centering
\includegraphics[bb=0 0 602 651, scale=0.4]{relacion.png}
\caption{\emph{Relaciones}}
\end{figure}
Abro el año con este gráfico realizado por John D. Cook acerca de las relaciones de las distribuciones de probabilidad. Según Cook, el gráfico está inspirado en un artículo de Lewis y McQueston publicado en TAS (The American Statistician) en donde los autores muestran en un gráfico mucho más extenso y completo cómo cada una de las distribuciones de probabilidad univariadas se relacionan y forman familias según las propiedades que tengan.

Entre las propiedades que parametrizan las distibuciones de probabilidad se encuentran las siguientes:

Propiedad de combinación lineal; por ejemplo, la suma de variables aleatorias normales independientes resulta tener una distribución normal
Propiedad de convolución; por ejemplo, una variable aleatoria con ditribución Chi cuadrado y con $n$ grados de libertad puede venir de la suma de $n$ variables independientes con distribución Chi cuadrado y con un grado de libertad
Propiedad del producto; por ejemplo, el producto de lognormales resulta tener una distribución lognormal
Propiedad de la inversa; por ejemplo, Si una variable aleatoria tiene distribución F, su inversa aritmética también tendrá distribución F
Este tipo de ayudas visuales son muy importantes y brindan al estadístico un nivel de comprensión mucho más amplio y son un claro ejemplo de que uno no siempre tiene que saber todo... Sin embargo, sí debe saber en dónde buscar.

\subsection{Percentiles}

Un concepto relacionado con las variables aleatorias que es muy importante para la inferencia estadística, específicamente la teoría de intervalo de confianza y las pruebas de hipótesis, es el concepto del percentil de una variable aleatoria. Definimos este concepto a continuación.

\begin{Defi}
Para una variable aleatoria $X$, el percentil $p$ de $X$, con $0<p<1$, se define como $\inf\{x:\ F_X(x)\geq p\}$ y se denota como $X_p$. Esto es, $X_p$ es el valor más pequeño es el valor más pequeño que acumula una probabilidad no inferior de $p$.
\end{Defi}

Para variables aleatorias continuas, la función de distribución correspondiente también es continua, y existe un único punto $x$ con $F(x)=p$, de donde podemos ver que el percentil $p$ de $X$ es simplemente $F^{-1}(p)$.

\begin{Eje}
Sea $X$ una variable aleatoria con distribución $Exp(\theta)$, entonces la función de distribución de $X$ está dada por
\begin{equation}
F_X(x)=
\begin{cases}
1-e^{-x/\theta}\ \ \ \ \ \text{si $x>0$}\\
0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{si no}\\
\end{cases}
\end{equation}

Podemos ver que $F_X$ tiene inversa para valores de $x$ en $(0,\infty)$ dada por $F^{-1}(x)=-\theta\ln(1-x)$, con $0<x<1$. De donde podemos calcular el percentil $p$ de la distribución $Exp(\theta)$ como $-\theta\ln(1-p)$. Por ejemplo, el percentil 0.3 de $Exp(4)$ está dada por $-4\ln(1-0.3)=1.4267$.

Por otro lado, en \verb"R", podemos obtener los percentiles de una distribución exponencial con la instrucción \verb"qexp" teniendo en cuenta que \verb"R" utiliza una parametrización diferente que la de este libro. El comando para calcular el percentil 0.3 de $Exp(4)$ y el resultado es como sigue
\begin{verbatim}
> qexp(0.3,1/4)
[1] 1.426700
\end{verbatim}
\end{Eje}

En algunas distribuciones continuas, como la distribución normal estándar, la función de distribución es de una forma muy complicada, y se requiere de algoritmos numéricos para calcular los percentiles de esta distribución. El comando en \verb"R" para ese fin es \verb"qnorm" y usa el algoritmo propuesto por \citeasnoun{Wichura}. En este texto, denotaremos el percentil $p$ de una distribución normal estándar como $z_p$, y presentamos los percentiles que se utilizarán a lo largo del texto.

\begin{table}[htb]
\centering
\begin{tabular}{|cc|cc|}\hline
$p$&$z_p$&$p$&$z_p$\\
\hline
0.005&-2.58&0.995&2.58\\
0.01&-2.32&0.99&2.32\\
0.025&-1.96&0.975&1.96\\
0.05&-1.64&0.95&1.64\\
0.1&-1.28&0.9&1.28\\
\hline
\end{tabular}
\caption{Algunos percentiles de la distribución normal estándar.}
\end{table}

\begin{table}[htb]
\centering
\begin{tabular}{|c|cccccccccc|}\hline
&\multicolumn{10}{c|}{$p$}\\
gl&0.005&0.01&0.025&0.05&0.1&0.9&0.95&0.975&0.99&0.995\\  \hline
1&-63.66&-31.82&-12.71&-6.31&-3.08&3.08&6.31&12.71&31.82&63.66\\
   2&-9.92&-6.96&-4.30&-2.92&-1.88&1.88&2.92& 4.30&6.96&9.92\\
   3&-5.84&-4.54&-3.18&-2.35&-1.64&1.64&2.35& 3.18&4.54&5.84\\
   4&-4.60&-3.75&-2.78&-2.13&-1.53&1.53&2.13& 2.78&3.75&4.60\\
   5&-4.03&-3.36&-2.57&-2.01&-1.47&1.47&2.01& 2.57&3.36&4.03\\
   6&-3.71&-3.14&-2.45&-1.94&-1.44&1.44&1.94& 2.45&3.14&3.71\\
   7&-3.50&-3.00&-2.36&-1.89&-1.41&1.41&1.89& 2.36&3.00&3.50\\
   8&-3.36&-2.90&-2.31&-1.86&-1.40&1.40&1.86& 2.31&2.90&3.36\\
   9&-3.25&-2.82&-2.26&-1.83&-1.38&1.38&1.83& 2.26&2.82&3.25\\
  10&-3.17&-2.76&-2.23&-1.81&-1.37&1.37&1.81& 2.23&2.76&3.17\\
  11&-3.11&-2.72&-2.20&-1.80&-1.36&1.36&1.80&2.20&2.72&3.11\\
  12&-3.05&-2.68&-2.18&-1.78&-1.36&1.36&1.78&2.18&2.68&3.05\\
  13&-3.01&-2.65&-2.16&-1.77&-1.35&1.35&1.77&2.16&2.65&3.01\\
  14&-2.98&-2.62&-2.14&-1.76&-1.35&1.35&1.76&2.14&2.62&2.98\\
  15&-2.95&-2.60&-2.13&-1.75&-1.34&1.34&1.75&2.13&2.60&2.95\\
  16&-2.92&-2.58&-2.12&-1.75&-1.34&1.34&1.75&2.12&2.58&2.92\\
  17&-2.90&-2.57&-2.11&-1.74&-1.33&1.33&1.74&2.11&2.57&2.90\\
  18&-2.88&-2.55&-2.10&-1.73&-1.33&1.33&1.73&2.10&2.55&2.88\\
  19&-2.86&-2.54&-2.09&-1.73&-1.33&1.33&1.73&2.09&2.54&2.86\\
  20&-2.84&-2.53&-2.09&-1.72&-1.33&1.33&1.72&2.09&2.53&2.84\\
  21&-2.83&-2.52&-2.08&-1.72&-1.32&1.32&1.72&2.08&2.52&2.83\\
  22&-2.82&-2.51&-2.07&-1.72&-1.32&1.32&1.72&2.07&2.51&2.82\\
  23&-2.81&-2.50&-2.07&-1.71&-1.32&1.32&1.71&2.07&2.50&2.81\\
  24&-2.80&-2.49&-2.06&-1.71&-1.32&1.32&1.71&2.06&2.49&2.80\\
  25&-2.79&-2.49&-2.06&-1.71&-1.32&1.32&1.71&2.06&2.49&2.79\\
  26&-2.78&-2.48&-2.06&-1.71&-1.31&1.31&1.71&2.06&2.48&2.78\\
  27&-2.77&-2.47&-2.05&-1.70&-1.31&1.31&1.70&2.05&2.47&2.77\\
  28&-2.76&-2.47&-2.05&-1.70&-1.31&1.31&1.70&2.05&2.47&2.76\\
  29&-2.76&-2.46&-2.05&-1.70&-1.31&1.31&1.70&2.05&2.46&2.76\\
  30&-2.75&-2.46&-2.04&-1.70&-1.31&1.31&1.70&2.04&2.46&2.75\\
  40&-2.70&-2.42&-2.02&-1.68&-1.30&1.30&1.68&2.02&2.42&2.70\\
  50&-2.68&-2.40&-2.01&-1.68&-1.30&1.30&1.68&2.01&2.40&2.68\\
  60&-2.66&-2.39&-2.00&-1.67&-1.30&1.30&1.67&2.00&2.39&2.66\\
  70&-2.65&-2.38&-1.99&-1.67&-1.29&1.29&1.67&1.99&2.38&2.65\\
  80&-2.64&-2.37&-1.99&-1.66&-1.29&1.29&1.66&1.99&2.37&2.64\\
  90&-2.63&-2.37&-1.99&-1.66&-1.29&1.29&1.66&1.99&2.37&2.63\\
\hline
\end{tabular}
\caption{Algunos percentiles de la distribución $t$ student.}
\end{table}

\begin{table}[htb]
\centering
{\small
\begin{tabular}{|c|cccccccccc|}\hline
&\multicolumn{10}{c|}{$p$}\\
gl&0.005&0.01&0.025&0.05&0.1&0.9&0.95&0.975&0.99&0.995\\  \hline
   1&0.00& 0.00&0.00& 0.00& 0.02& 2.71& 3.84& 5.02& 6.63& 7.88\\
   2&0.01& 0.02&0.05& 0.10& 0.21& 4.61& 5.99& 7.38& 9.21&10.60\\
   3&0.07& 0.11&0.22& 0.35& 0.58& 6.25& 7.81& 9.35&11.34&12.84\\
   4&0.21& 0.30&0.48& 0.71& 1.06& 7.78& 9.49&11.14&13.28&14.86\\
   5&0.41& 0.55&0.83& 1.15& 1.61& 9.24&11.07&12.83&15.09&16.75\\
   6&0.68& 0.87&1.24& 1.64& 2.20&10.64&12.59&14.45&16.81&18.55\\
   7&0.99& 1.24&1.69& 2.17& 2.83&12.02&14.07&16.01&18.48&20.28\\
   8&1.34& 1.65&2.18& 2.73& 3.49&13.36&15.51&17.53&20.09&21.95\\
   9&1.73& 2.09&2.70& 3.33& 4.17&14.68&16.92&19.02&21.67&23.59\\
  10&2.16& 2.56&3.25& 3.94& 4.87&15.99&18.31&20.48&23.21&25.19\\
  11&2.60& 3.05&3.82& 4.57& 5.58&17.28&19.68&21.92&24.72&26.76\\
  12&3.07& 3.57&4.40& 5.23& 6.30&18.55&21.03&23.34&26.22&28.30\\
  13&3.57& 4.11&5.01& 5.89& 7.04&19.81&22.36&24.74&27.69&29.82\\
  14&4.07& 4.66&5.63& 6.57& 7.79&21.06&23.68&26.12&29.14&31.32\\
  15&4.60& 5.23&6.26& 7.26& 8.55&22.31&25.00&27.49&30.58&32.80\\
  16&5.14& 5.81&6.91& 7.96& 9.31&23.54&26.30&28.85&32.00&34.27\\
  17&5.70& 6.41&7.56& 8.67&10.09&24.77&27.59&30.19&33.41&35.72\\
   18&6.26&7.01&8.23& 9.39&10.86&25.99&28.87&31.53&34.81&37.16\\
   19&6.84&7.63&8.91&10.12&11.65&27.20&30.14&32.85&36.19&38.58\\
   20&7.43&8.26&9.59&10.85&12.44&28.41&31.41&34.17&37.57&40.00\\
 21&8.03& 8.90&10.28&11.59&13.24&29.62&32.67&35.48&38.93&41.40\\
 22&8.64& 9.54&10.98&12.34&14.04&30.81&33.92&36.78&40.29&42.80\\
 23&9.26&10.20&11.69&13.09&14.85&32.01&35.17&38.08&41.64&44.18\\
 24&9.89&10.86&12.40&13.85&15.66&33.20&36.42&39.36&42.98&45.56\\
25&10.52&11.52&13.12&14.61&16.47&34.38&37.65&40.65&44.31&46.93\\
26&11.16&12.20&13.84&15.38&17.29&35.56&38.89&41.92&45.64&48.29\\
27&11.81&12.88&14.57&16.15&18.11&36.74&40.11&43.19&46.96&49.64\\
28&12.46&13.56&15.31&16.93&18.94&37.92&41.34&44.46&48.28&50.99\\
29&13.12&14.26&16.05&17.71&19.77&39.09&42.56&45.72&49.59&52.34\\
30&13.79&14.95&16.79&18.49&20.60&40.26&43.77&46.98&50.89&53.67\\
  40&20.71&22.16&24.43&26.51&29.05& 51.81& 55.76& 59.34& 63.69& 66.77\\
  50&27.99&29.71&32.36&34.76&37.69& 63.17& 67.50& 71.42& 76.15& 79.49\\
  60&35.53&37.48&40.48&43.19&46.46& 74.40& 79.08& 83.30& 88.38& 91.95\\
  70&43.28&45.44&48.76&51.74&55.33& 85.53& 90.53& 95.02&100.43&104.21\\
  80&51.17&53.54&57.15&60.39&64.28& 96.58&101.88&106.63&112.33&116.32\\
  90&59.20&61.75&65.65&69.13&73.29&107.57&113.15&118.14&124.12&128.30\\
\hline
\end{tabular}}
\caption{Algunos percentiles de la distribución $\chi^2$.}
\end{table}

\begin{table}[htb]
\centering
{\small
\begin{tabular}{|c|cccccccccc|}\hline
&\multicolumn{10}{c|}{$m$}\\
$n$&1&2&3&4&5&6&7&8&9&10\\  \hline
   1&39.86&49.50&53.59&55.83&57.24&58.20&58.91&59.44&59.86&60.19\\
   2& 8.53& 9.00& 9.16& 9.24& 9.29& 9.33& 9.35& 9.37& 9.38& 9.39\\
   3& 5.54& 5.46& 5.39& 5.34& 5.31& 5.28& 5.27& 5.25& 5.24& 5.23\\
   4& 4.54& 4.32& 4.19& 4.11& 4.05& 4.01& 3.98& 3.95& 3.94& 3.92\\
   5& 4.06& 3.78& 3.62& 3.52& 3.45& 3.40& 3.37& 3.34& 3.32& 3.30\\
   6& 3.78& 3.46& 3.29& 3.18& 3.11& 3.05& 3.01& 2.98& 2.96& 2.94\\
   7& 3.59& 3.26& 3.07& 2.96& 2.88& 2.83& 2.78& 2.75& 2.72& 2.70\\
   8& 3.46& 3.11& 2.92& 2.81& 2.73& 2.67& 2.62& 2.59& 2.56& 2.54\\
   9& 3.36& 3.01& 2.81& 2.69& 2.61& 2.55& 2.51& 2.47& 2.44& 2.42\\
  10& 3.29& 2.92& 2.73& 2.61& 2.52& 2.46& 2.41& 2.38& 2.35& 2.32\\
  11& 3.23& 2.86& 2.66& 2.54& 2.45& 2.39& 2.34& 2.30& 2.27& 2.25\\
  12& 3.18& 2.81& 2.61& 2.48& 2.39& 2.33& 2.28& 2.24& 2.21& 2.19\\
  13& 3.14& 2.76& 2.56& 2.43& 2.35& 2.28& 2.23& 2.20& 2.16& 2.14\\
  14& 3.10& 2.73& 2.52& 2.39& 2.31& 2.24& 2.19& 2.15& 2.12& 2.10\\
  15& 3.07& 2.70& 2.49& 2.36& 2.27& 2.21& 2.16& 2.12& 2.09& 2.06\\
  16& 3.05& 2.67& 2.46& 2.33& 2.24& 2.18& 2.13& 2.09& 2.06& 2.03\\
  17& 3.03& 2.64& 2.44& 2.31& 2.22& 2.15& 2.10& 2.06& 2.03& 2.00\\
  18& 3.01& 2.62& 2.42& 2.29& 2.20& 2.13& 2.08& 2.04& 2.00& 1.98\\
  19& 2.99& 2.61& 2.40& 2.27& 2.18& 2.11& 2.06& 2.02& 1.98& 1.96\\
  20& 2.97& 2.59& 2.38& 2.25& 2.16& 2.09& 2.04& 2.00& 1.96& 1.94\\
  21& 2.96& 2.57& 2.36& 2.23& 2.14& 2.08& 2.02& 1.98& 1.95& 1.92\\
  22& 2.95& 2.56& 2.35& 2.22& 2.13& 2.06& 2.01& 1.97& 1.93& 1.90\\
  23& 2.94& 2.55& 2.34& 2.21& 2.11& 2.05& 1.99& 1.95& 1.92& 1.89\\
  24& 2.93& 2.54& 2.33& 2.19& 2.10& 2.04& 1.98& 1.94& 1.91& 1.88\\
  25& 2.92& 2.53& 2.32& 2.18& 2.09& 2.02& 1.97& 1.93& 1.89& 1.87\\
  26& 2.91& 2.52& 2.31& 2.17& 2.08& 2.01& 1.96& 1.92& 1.88& 1.86\\
  27& 2.90& 2.51& 2.30& 2.17& 2.07& 2.00& 1.95& 1.91& 1.87& 1.85\\
  28& 2.89& 2.50& 2.29& 2.16& 2.06& 2.00& 1.94& 1.90& 1.87& 1.84\\
  29& 2.89& 2.50& 2.28& 2.15& 2.06& 1.99& 1.93& 1.89& 1.86& 1.83\\
  30& 2.88& 2.49& 2.28& 2.14& 2.05& 1.98& 1.93& 1.88& 1.85& 1.82\\
  40& 2.84&2.44&2.23&2.09&2.00&1.93&1.87&1.83&1.79&1.76\\
  50& 2.81&2.41&2.20&2.06&1.97&1.90&1.84&1.80&1.76&1.73\\
  60& 2.79&2.39&2.18&2.04&1.95&1.87&1.82&1.77&1.74&1.71\\
  70& 2.78&2.38&2.16&2.03&1.93&1.86&1.80&1.76&1.72&1.69\\
  80& 2.77&2.37&2.15&2.02&1.92&1.85&1.79&1.75&1.71&1.68\\
  90& 2.76&2.36&2.15&2.01&1.91&1.84&1.78&1.74&1.70&1.67\\
\hline
\end{tabular}}
\caption{Algunos percentiles 0.9 de la distribución $F$.}
\end{table}

\begin{table}[htb]
\centering
{\small
\begin{tabular}{|c|cccccccccc|}\hline
&\multicolumn{10}{c|}{$m$}\\
$n$&12&14&16&18&20&24&30&40&60&100\\  \hline
   1&60.71&61.07&61.35&61.57&61.74&62.00&62.26&62.53&62.79&63.01\\
   2& 9.41& 9.42& 9.43& 9.44& 9.44& 9.45& 9.46& 9.47& 9.47& 9.48\\
   3& 5.22& 5.20& 5.20& 5.19& 5.18& 5.18& 5.17& 5.16& 5.15& 5.14\\
   4& 3.90& 3.88& 3.86& 3.85& 3.84& 3.83& 3.82& 3.80& 3.79& 3.78\\
   5& 3.27& 3.25& 3.23& 3.22& 3.21& 3.19& 3.17& 3.16& 3.14& 3.13\\
   6& 2.90& 2.88& 2.86& 2.85& 2.84& 2.82& 2.80& 2.78& 2.76& 2.75\\
   7& 2.67& 2.64& 2.62& 2.61& 2.59& 2.58& 2.56& 2.54& 2.51& 2.50\\
   8& 2.50& 2.48& 2.45& 2.44& 2.42& 2.40& 2.38& 2.36& 2.34& 2.32\\
   9& 2.38& 2.35& 2.33& 2.31& 2.30& 2.28& 2.25& 2.23& 2.21& 2.19\\
  10& 2.28& 2.26& 2.23& 2.22& 2.20& 2.18& 2.16& 2.13& 2.11& 2.09\\
  11& 2.21& 2.18& 2.16& 2.14& 2.12& 2.10& 2.08& 2.05& 2.03& 2.01\\
  12& 2.15& 2.12& 2.09& 2.08& 2.06& 2.04& 2.01& 1.99& 1.96& 1.94\\
  13& 2.10& 2.07& 2.04& 2.02& 2.01& 1.98& 1.96& 1.93& 1.90& 1.88\\
  14& 2.05& 2.02& 2.00& 1.98& 1.96& 1.94& 1.91& 1.89& 1.86& 1.83\\
  15& 2.02& 1.99& 1.96& 1.94& 1.92& 1.90& 1.87& 1.85& 1.82& 1.79\\
  16& 1.99& 1.95& 1.93& 1.91& 1.89& 1.87& 1.84& 1.81& 1.78& 1.76\\
  17& 1.96& 1.93& 1.90& 1.88& 1.86& 1.84& 1.81& 1.78& 1.75& 1.73\\
  18& 1.93& 1.90& 1.87& 1.85& 1.84& 1.81& 1.78& 1.75& 1.72& 1.70\\
  19& 1.91& 1.88& 1.85& 1.83& 1.81& 1.79& 1.76& 1.73& 1.70& 1.67\\
  20& 1.89& 1.86& 1.83& 1.81& 1.79& 1.77& 1.74& 1.71& 1.68& 1.65\\
  21& 1.87& 1.84& 1.81& 1.79& 1.78& 1.75& 1.72& 1.69& 1.66& 1.63\\
  22& 1.86& 1.83& 1.80& 1.78& 1.76& 1.73& 1.70& 1.67& 1.64& 1.61\\
  23& 1.84& 1.81& 1.78& 1.76& 1.74& 1.72& 1.69& 1.66& 1.62& 1.59\\
  24& 1.83& 1.80& 1.77& 1.75& 1.73& 1.70& 1.67& 1.64& 1.61& 1.58\\
  25& 1.82& 1.79& 1.76& 1.74& 1.72& 1.69& 1.66& 1.63& 1.59& 1.56\\
  26& 1.81& 1.77& 1.75& 1.72& 1.71& 1.68& 1.65& 1.61& 1.58& 1.55\\
  27& 1.80& 1.76& 1.74& 1.71& 1.70& 1.67& 1.64& 1.60& 1.57& 1.54\\
  28& 1.79& 1.75& 1.73& 1.70& 1.69& 1.66& 1.63& 1.59& 1.56& 1.53\\
  29& 1.78& 1.75& 1.72& 1.69& 1.68& 1.65& 1.62& 1.58& 1.55& 1.52\\
  30& 1.77& 1.74& 1.71& 1.69& 1.67& 1.64& 1.61& 1.57& 1.54& 1.51\\
  40& 1.71& 1.68& 1.65& 1.62& 1.61& 1.57& 1.54& 1.51& 1.47& 1.43\\
  50& 1.68& 1.64& 1.61& 1.59& 1.57& 1.54& 1.50& 1.46& 1.42& 1.39\\
  60& 1.66& 1.62& 1.59& 1.56& 1.54& 1.51& 1.48& 1.44& 1.40& 1.36\\
  70& 1.64& 1.60& 1.57& 1.55& 1.53& 1.49& 1.46& 1.42& 1.37& 1.34\\
  80& 1.63& 1.59& 1.56& 1.53& 1.51& 1.48& 1.44& 1.40& 1.36& 1.32\\
  90& 1.62& 1.58& 1.55& 1.52& 1.50& 1.47& 1.43& 1.39& 1.35& 1.30\\
\hline
\end{tabular}}
\caption{Algunos percentiles 0.9 de la distribución $F$.}
\end{table}

\begin{table}[htb]
\centering
{\small
\begin{tabular}{|c|cccccccccc|}\hline
&\multicolumn{10}{c|}{$m$}\\
$n$&1&2&3&4&5&6&7&8&9&10\\  \hline
   1&161.45&199.50&215.71&224.58&230.16&233.99&236.77&238.88&240.54&241.88\\
   2& 18.51& 19.00& 19.16& 19.25& 19.30& 19.33& 19.35& 19.37& 19.38& 19.40\\
   3& 10.13&  9.55&  9.28&  9.12&  9.01&  8.94&  8.89&  8.85&  8.81&  8.79\\
   4&  7.71&  6.94&  6.59&  6.39&  6.26&  6.16&  6.09&  6.04&  6.00&  5.96\\
   5&  6.61&  5.79&  5.41&  5.19&  5.05&  4.95&  4.88&  4.82&  4.77&  4.74\\
   6&  5.99&  5.14&  4.76&  4.53&  4.39&  4.28&  4.21&  4.15&  4.10&  4.06\\
   7&  5.59&  4.74&  4.35&  4.12&  3.97&  3.87&  3.79&  3.73&  3.68&  3.64\\
   8&  5.32&  4.46&  4.07&  3.84&  3.69&  3.58&  3.50&  3.44&  3.39&  3.35\\
   9&  5.12&  4.26&  3.86&  3.63&  3.48&  3.37&  3.29&  3.23&  3.18&  3.14\\
  10&  4.96&  4.10&  3.71&  3.48&  3.33&  3.22&  3.14&  3.07&  3.02&  2.98\\
  11&  4.84&  3.98&  3.59&  3.36&  3.20&  3.09&  3.01&  2.95&  2.90&  2.85\\
  12&  4.75&  3.89&  3.49&  3.26&  3.11&  3.00&  2.91&  2.85&  2.80&  2.75\\
  13&  4.67&  3.81&  3.41&  3.18&  3.03&  2.92&  2.83&  2.77&  2.71&  2.67\\
  14&  4.60&  3.74&  3.34&  3.11&  2.96&  2.85&  2.76&  2.70&  2.65&  2.60\\
  15&  4.54&  3.68&  3.29&  3.06&  2.90&  2.79&  2.71&  2.64&  2.59&  2.54\\
  16&  4.49&  3.63&  3.24&  3.01&  2.85&  2.74&  2.66&  2.59&  2.54&  2.49\\
  17&  4.45&  3.59&  3.20&  2.96&  2.81&  2.70&  2.61&  2.55&  2.49&  2.45\\
  18&  4.41&  3.55&  3.16&  2.93&  2.77&  2.66&  2.58&  2.51&  2.46&  2.41\\
  19&  4.38&  3.52&  3.13&  2.90&  2.74&  2.63&  2.54&  2.48&  2.42&  2.38\\
  20&  4.35&  3.49&  3.10&  2.87&  2.71&  2.60&  2.51&  2.45&  2.39&  2.35\\
  21&  4.32&  3.47&  3.07&  2.84&  2.68&  2.57&  2.49&  2.42&  2.37&  2.32\\
  22&  4.30&  3.44&  3.05&  2.82&  2.66&  2.55&  2.46&  2.40&  2.34&  2.30\\
  23&  4.28&  3.42&  3.03&  2.80&  2.64&  2.53&  2.44&  2.37&  2.32&  2.27\\
  24&  4.26&  3.40&  3.01&  2.78&  2.62&  2.51&  2.42&  2.36&  2.30&  2.25\\
  25&  4.24&  3.39&  2.99&  2.76&  2.60&  2.49&  2.40&  2.34&  2.28&  2.24\\
  26&  4.23&  3.37&  2.98&  2.74&  2.59&  2.47&  2.39&  2.32&  2.27&  2.22\\
  27&  4.21&  3.35&  2.96&  2.73&  2.57&  2.46&  2.37&  2.31&  2.25&  2.20\\
  28&  4.20&  3.34&  2.95&  2.71&  2.56&  2.45&  2.36&  2.29&  2.24&  2.19\\
  29&  4.18&  3.33&  2.93&  2.70&  2.55&  2.43&  2.35&  2.28&  2.22&  2.18\\
  30&  4.17&  3.32&  2.92&  2.69&  2.53&  2.42&  2.33&  2.27&  2.21&  2.16\\
  40&  4.08&  3.23&  2.84&  2.61&  2.45&  2.34&  2.25&  2.18&  2.12&  2.08\\
  50&  4.03&  3.18&  2.79&  2.56&  2.40&  2.29&  2.20&  2.13&  2.07&  2.03\\
  60&  4.00&  3.15&  2.76&  2.53&  2.37&  2.25&  2.17&  2.10&  2.04&  1.99\\
  70&  3.98&  3.13&  2.74&  2.50&  2.35&  2.23&  2.14&  2.07&  2.02&  1.97\\
  80&  3.96&  3.11&  2.72&  2.49&  2.33&  2.21&  2.13&  2.06&  2.00&  1.95\\
  90&  3.95&  3.10&  2.71&  2.47&  2.32&  2.20&  2.11&  2.04&  1.99&  1.94\\
\hline
\end{tabular}}
\caption{Algunos percentiles 0.95 de la distribución $F$.}
\end{table}

\begin{table}[htb]
\centering
{\small
\begin{tabular}{|c|cccccccccc|}\hline
&\multicolumn{10}{c|}{$m$}\\
$n$&12&14&16&18&20&24&30&40&60&100\\  \hline
   1&243.91&245.36&246.46&247.32&248.01&249.05&250.10&251.14&252.20&253.04\\
   2& 19.41& 19.42& 19.43& 19.44& 19.45& 19.45& 19.46& 19.47& 19.48& 19.49\\
   3&  8.74&  8.71&  8.69&  8.67&  8.66&  8.64&  8.62&  8.59&  8.57&  8.55\\
   4&  5.91&  5.87&  5.84&  5.82&  5.80&  5.77&  5.75&  5.72&  5.69&  5.66\\
   5&  4.68&  4.64&  4.60&  4.58&  4.56&  4.53&  4.50&  4.46&  4.43&  4.41\\
   6&  4.00&  3.96&  3.92&  3.90&  3.87&  3.84&  3.81&  3.77&  3.74&  3.71\\
   7&  3.57&  3.53&  3.49&  3.47&  3.44&  3.41&  3.38&  3.34&  3.30&  3.27\\
   8&  3.28&  3.24&  3.20&  3.17&  3.15&  3.12&  3.08&  3.04&  3.01&  2.97\\
   9&  3.07&  3.03&  2.99&  2.96&  2.94&  2.90&  2.86&  2.83&  2.79&  2.76\\
  10&  2.91&  2.86&  2.83&  2.80&  2.77&  2.74&  2.70&  2.66&  2.62&  2.59\\
  11&  2.79&  2.74&  2.70&  2.67&  2.65&  2.61&  2.57&  2.53&  2.49&  2.46\\
  12&  2.69&  2.64&  2.60&  2.57&  2.54&  2.51&  2.47&  2.43&  2.38&  2.35\\
  13&  2.60&  2.55&  2.51&  2.48&  2.46&  2.42&  2.38&  2.34&  2.30&  2.26\\
  14&  2.53&  2.48&  2.44&  2.41&  2.39&  2.35&  2.31&  2.27&  2.22&  2.19\\
  15&  2.48&  2.42&  2.38&  2.35&  2.33&  2.29&  2.25&  2.20&  2.16&  2.12\\
  16&  2.42&  2.37&  2.33&  2.30&  2.28&  2.24&  2.19&  2.15&  2.11&  2.07\\
  17&  2.38&  2.33&  2.29&  2.26&  2.23&  2.19&  2.15&  2.10&  2.06&  2.02\\
  18&  2.34&  2.29&  2.25&  2.22&  2.19&  2.15&  2.11&  2.06&  2.02&  1.98\\
  19&  2.31&  2.26&  2.21&  2.18&  2.16&  2.11&  2.07&  2.03&  1.98&  1.94\\
  20&  2.28&  2.22&  2.18&  2.15&  2.12&  2.08&  2.04&  1.99&  1.95&  1.91\\
  21&  2.25&  2.20&  2.16&  2.12&  2.10&  2.05&  2.01&  1.96&  1.92&  1.88\\
  22&  2.23&  2.17&  2.13&  2.10&  2.07&  2.03&  1.98&  1.94&  1.89&  1.85\\
  23&  2.20&  2.15&  2.11&  2.08&  2.05&  2.01&  1.96&  1.91&  1.86&  1.82\\
  24&  2.18&  2.13&  2.09&  2.05&  2.03&  1.98&  1.94&  1.89&  1.84&  1.80\\
  25&  2.16&  2.11&  2.07&  2.04&  2.01&  1.96&  1.92&  1.87&  1.82&  1.78\\
  26&  2.15&  2.09&  2.05&  2.02&  1.99&  1.95&  1.90&  1.85&  1.80&  1.76\\
  27&  2.13&  2.08&  2.04&  2.00&  1.97&  1.93&  1.88&  1.84&  1.79&  1.74\\
  28&  2.12&  2.06&  2.02&  1.99&  1.96&  1.91&  1.87&  1.82&  1.77&  1.73\\
  29&  2.10&  2.05&  2.01&  1.97&  1.94&  1.90&  1.85&  1.81&  1.75&  1.71\\
  30&  2.09&  2.04&  1.99&  1.96&  1.93&  1.89&  1.84&  1.79&  1.74&  1.70\\
  40& 2.00& 1.95& 1.90& 1.87& 1.84& 1.79& 1.74& 1.69& 1.64& 1.59\\
  50& 1.95& 1.89& 1.85& 1.81& 1.78& 1.74& 1.69& 1.63& 1.58& 1.52\\
  60& 1.92& 1.86& 1.82& 1.78& 1.75& 1.70& 1.65& 1.59& 1.53& 1.48\\
  70& 1.89& 1.84& 1.79& 1.75& 1.72& 1.67& 1.62& 1.57& 1.50& 1.45\\
  80& 1.88& 1.82& 1.77& 1.73& 1.70& 1.65& 1.60& 1.54& 1.48& 1.43\\
  90& 1.86& 1.80& 1.76& 1.72& 1.69& 1.64& 1.59& 1.53& 1.46& 1.41\\
\hline
\end{tabular}}
\caption{Algunos percentiles 0.95 de la distribución $F$.}
\end{table}


\begin{table}[htb]
\centering
{\small
\begin{tabular}{|c|cccccccccc|}\hline
&\multicolumn{10}{c|}{$m$}\\
$n$&1&2&3&4&5&6&7&8&9&10\\  \hline
   1&647.79&799.50&864.16&899.58&921.85&937.11&948.22&956.66&963.28&968.63\\
   2& 38.51& 39.00& 39.17& 39.25& 39.30& 39.33& 39.36& 39.37& 39.39& 39.40\\
   3& 17.44& 16.04& 15.44& 15.10& 14.88& 14.73& 14.62& 14.54& 14.47& 14.42\\
   4& 12.22& 10.65&  9.98&  9.60&  9.36&  9.20&  9.07&  8.98&  8.90&  8.84\\
   5& 10.01&  8.43&  7.76&  7.39&  7.15&  6.98&  6.85&  6.76&  6.68&  6.62\\
   6&  8.81&  7.26&  6.60&  6.23&  5.99&  5.82&  5.70&  5.60&  5.52&  5.46\\
   7&  8.07&  6.54&  5.89&  5.52&  5.29&  5.12&  4.99&  4.90&  4.82&  4.76\\
   8&  7.57&  6.06&  5.42&  5.05&  4.82&  4.65&  4.53&  4.43&  4.36&  4.30\\
   9&  7.21&  5.71&  5.08&  4.72&  4.48&  4.32&  4.20&  4.10&  4.03&  3.96\\
  10&  6.94&  5.46&  4.83&  4.47&  4.24&  4.07&  3.95&  3.85&  3.78&  3.72\\
  11&  6.72&  5.26&  4.63&  4.28&  4.04&  3.88&  3.76&  3.66&  3.59&  3.53\\
  12&  6.55&  5.10&  4.47&  4.12&  3.89&  3.73&  3.61&  3.51&  3.44&  3.37\\
  13&  6.41&  4.97&  4.35&  4.00&  3.77&  3.60&  3.48&  3.39&  3.31&  3.25\\
  14&  6.30&  4.86&  4.24&  3.89&  3.66&  3.50&  3.38&  3.29&  3.21&  3.15\\
  15&  6.20&  4.77&  4.15&  3.80&  3.58&  3.41&  3.29&  3.20&  3.12&  3.06\\
  16&  6.12&  4.69&  4.08&  3.73&  3.50&  3.34&  3.22&  3.12&  3.05&  2.99\\
  17&  6.04&  4.62&  4.01&  3.66&  3.44&  3.28&  3.16&  3.06&  2.98&  2.92\\
  18&  5.98&  4.56&  3.95&  3.61&  3.38&  3.22&  3.10&  3.01&  2.93&  2.87\\
  19&  5.92&  4.51&  3.90&  3.56&  3.33&  3.17&  3.05&  2.96&  2.88&  2.82\\
  20&  5.87&  4.46&  3.86&  3.51&  3.29&  3.13&  3.01&  2.91&  2.84&  2.77\\
  21&  5.83&  4.42&  3.82&  3.48&  3.25&  3.09&  2.97&  2.87&  2.80&  2.73\\
  22&  5.79&  4.38&  3.78&  3.44&  3.22&  3.05&  2.93&  2.84&  2.76&  2.70\\
  23&  5.75&  4.35&  3.75&  3.41&  3.18&  3.02&  2.90&  2.81&  2.73&  2.67\\
  24&  5.72&  4.32&  3.72&  3.38&  3.15&  2.99&  2.87&  2.78&  2.70&  2.64\\
  25&  5.69&  4.29&  3.69&  3.35&  3.13&  2.97&  2.85&  2.75&  2.68&  2.61\\
  26&  5.66&  4.27&  3.67&  3.33&  3.10&  2.94&  2.82&  2.73&  2.65&  2.59\\
  27&  5.63&  4.24&  3.65&  3.31&  3.08&  2.92&  2.80&  2.71&  2.63&  2.57\\
  28&  5.61&  4.22&  3.63&  3.29&  3.06&  2.90&  2.78&  2.69&  2.61&  2.55\\
  29&  5.59&  4.20&  3.61&  3.27&  3.04&  2.88&  2.76&  2.67&  2.59&  2.53\\
  30&  5.57&  4.18&  3.59&  3.25&  3.03&  2.87&  2.75&  2.65&  2.57&  2.51\\
  40& 5.42& 4.05& 3.46& 3.13& 2.90& 2.74& 2.62& 2.53& 2.45& 2.39\\
  50& 5.34& 3.97& 3.39& 3.05& 2.83& 2.67& 2.55& 2.46& 2.38& 2.32\\
  60& 5.29& 3.93& 3.34& 3.01& 2.79& 2.63& 2.51& 2.41& 2.33& 2.27\\
  70& 5.25& 3.89& 3.31& 2.97& 2.75& 2.59& 2.47& 2.38& 2.30& 2.24\\
  80& 5.22& 3.86& 3.28& 2.95& 2.73& 2.57& 2.45& 2.35& 2.28& 2.21\\
  90& 5.20& 3.84& 3.26& 2.93& 2.71& 2.55& 2.43& 2.34& 2.26& 2.19\\
\hline
\end{tabular}}
\caption{Algunos percentiles 0.975 de la distribución $F$.}
\end{table}


\begin{table}[htb]
\centering
{\small
\begin{tabular}{|c|cccccccccc|}\hline
&\multicolumn{10}{c|}{$m$}\\
$n$&12&14&16&18&20&24&30&40&60&100\\  \hline
   1&976.71&982.53&986.92&990.35&993.10&997.25&1001.41&1005.60&1009.80&1013.17\\
   2& 39.41& 39.43& 39.44& 39.44& 39.45& 39.46&  39.46&  39.47&  39.48&  39.49\\
   3& 14.34& 14.28& 14.23& 14.20& 14.17& 14.12&  14.08&  14.04&  13.99&  13.96\\
   4&  8.75&  8.68&  8.63&  8.59&  8.56&  8.51&   8.46&   8.41&   8.36&   8.32\\
   5&  6.52&  6.46&  6.40&  6.36&  6.33&  6.28&   6.23&   6.18&   6.12&   6.08\\
   6&  5.37&  5.30&  5.24&  5.20&  5.17&  5.12&   5.07&   5.01&   4.96&   4.92\\
   7&  4.67&  4.60&  4.54&  4.50&  4.47&  4.41&   4.36&   4.31&   4.25&   4.21\\
   8&  4.20&  4.13&  4.08&  4.03&  4.00&  3.95&   3.89&   3.84&   3.78&   3.74\\
   9&  3.87&  3.80&  3.74&  3.70&  3.67&  3.61&   3.56&   3.51&   3.45&   3.40\\
  10&  3.62&  3.55&  3.50&  3.45&  3.42&  3.37&   3.31&   3.26&   3.20&   3.15\\
  11&  3.43&  3.36&  3.30&  3.26&  3.23&  3.17&   3.12&   3.06&   3.00&   2.96\\
  12&  3.28&  3.21&  3.15&  3.11&  3.07&  3.02&   2.96&   2.91&   2.85&   2.80\\
  13&  3.15&  3.08&  3.03&  2.98&  2.95&  2.89&   2.84&   2.78&   2.72&   2.67\\
  14&  3.05&  2.98&  2.92&  2.88&  2.84&  2.79&   2.73&   2.67&   2.61&   2.56\\
  15&  2.96&  2.89&  2.84&  2.79&  2.76&  2.70&   2.64&   2.59&   2.52&   2.47\\
  16&  2.89&  2.82&  2.76&  2.72&  2.68&  2.63&   2.57&   2.51&   2.45&   2.40\\
  17&  2.82&  2.75&  2.70&  2.65&  2.62&  2.56&   2.50&   2.44&   2.38&   2.33\\
  18&  2.77&  2.70&  2.64&  2.60&  2.56&  2.50&   2.44&   2.38&   2.32&   2.27\\
  19&  2.72&  2.65&  2.59&  2.55&  2.51&  2.45&   2.39&   2.33&   2.27&   2.22\\
  20&  2.68&  2.60&  2.55&  2.50&  2.46&  2.41&   2.35&   2.29&   2.22&   2.17\\
  21&  2.64&  2.56&  2.51&  2.46&  2.42&  2.37&   2.31&   2.25&   2.18&   2.13\\
  22&  2.60&  2.53&  2.47&  2.43&  2.39&  2.33&   2.27&   2.21&   2.14&   2.09\\
  23&  2.57&  2.50&  2.44&  2.39&  2.36&  2.30&   2.24&   2.18&   2.11&   2.06\\
  24&  2.54&  2.47&  2.41&  2.36&  2.33&  2.27&   2.21&   2.15&   2.08&   2.02\\
  25&  2.51&  2.44&  2.38&  2.34&  2.30&  2.24&   2.18&   2.12&   2.05&   2.00\\
  26&  2.49&  2.42&  2.36&  2.31&  2.28&  2.22&   2.16&   2.09&   2.03&   1.97\\
  27&  2.47&  2.39&  2.34&  2.29&  2.25&  2.19&   2.13&   2.07&   2.00&   1.94\\
  28&  2.45&  2.37&  2.32&  2.27&  2.23&  2.17&   2.11&   2.05&   1.98&   1.92\\
  29&  2.43&  2.36&  2.30&  2.25&  2.21&  2.15&   2.09&   2.03&   1.96&   1.90\\
  30&  2.41&  2.34&  2.28&  2.23&  2.20&  2.14&   2.07&   2.01&   1.94&   1.88\\
  40& 2.29&2.21& 2.15& 2.11& 2.07& 2.01& 1.94& 1.88& 1.80& 1.74\\
  50& 2.22&2.14& 2.08& 2.03& 1.99& 1.93& 1.87& 1.80& 1.72& 1.66\\
  60& 2.17&2.09& 2.03& 1.98& 1.94& 1.88& 1.82& 1.74& 1.67& 1.60\\
  70& 2.14&2.06& 2.00& 1.95& 1.91& 1.85& 1.78& 1.71& 1.63& 1.56\\
  80& 2.11&2.03& 1.97& 1.92& 1.88& 1.82& 1.75& 1.68& 1.60& 1.53\\
  90& 2.09&2.02& 1.95& 1.91& 1.86& 1.80& 1.73& 1.66& 1.58& 1.50\\
\hline
\end{tabular} }
\caption{Algunos percentiles 0.975 de la distribución $F$.}
\end{table}


Para variables aleatorias discretas, el cálculo de los percentiles no un poco más complicado, puesto que la función de distribución es una función escalonada, y por consiguiente no tiene inversa. Ilustramos el cálculo en el siguiente ejemplo.

\begin{Eje}
Sea $X$ una variable aleatoria con distribución $Bin(8,0.3)$, entonces tenemos que la función de densidad está dada por
\begin{equation}
f(x)=P(X=x)=\begin{cases}
0.058&\text{si $x=0$}\\
0.198&\text{si $x=1$}\\
0.296&\text{si $x=2$}\\
0.254&\text{si $x=3$}\\
0.136&\text{si $x=4$}\\
0.047&\text{si $x=5$}\\
0.01&\text{si $x=6$}\\
0.001&\text{si $x=7$}\\
0.00006&\text{si $x=8$}\\
0&\text{en otro caso}
\end{cases}
\end{equation}

Y su función de distribución está dada por
\begin{equation}
F(x)=P(X\leq x)=\begin{cases}
0&\text{si $x<0$}\\
0.058&\text{si $0\leq x<1$}\\
0.255&\text{si $1\leq x<2$}\\
0.552&\text{si $2\leq x<3$}\\
0.806&\text{si $3\leq x<4$}\\
0.942&\text{si $4\leq x<5$}\\
0.989&\text{si $5\leq x<6$}\\
0.999&\text{si $6\leq x<7$}\\
0.9999&\text{si $7\leq x<8$}\\
1&\text{si $x\geq8$}
\end{cases}
\end{equation}

Suponga que se quiere hallar la mediana de la distribución $Bin(8,0.3)$, esto es, el percentil 50\%, tenemos que
\begin{equation*}
X_{50}=\inf\{x:\ F(x)\geq 0.5\}=\inf\{x:\ x\geq2\}=2
\end{equation*}

Nótese que en este caso, el valor 2 no solo es el percentil 50\%, también es el percentil 51\%, ó 55\%.

El cómputo de los percentiles en \verb"R" para la distribución binomial se lleva a cabo usando el comando \verb"qbinom"
\begin{verbatim}
> qbinom(0.5,8,0.3)
[1] 2
\end{verbatim}

\end{Eje}

En algunos textos, como \citeasnoun{Canavos}, en la tabla concerniente a la distribución F, solo se dispone los percentiles de 90\%, 95\% y 99\% y no los percentiles 10\%, 5\% y 1\%. Sin embargo, por el anterior resultado, se puede ver fácilmente que el percentil $\alpha$ de una distribución $F^{n}_m$ es simplemente la inversa del percentil $1-\alpha$ de la distribución $F^{m}_n$. Para ver eso, suponga que $X\sim F^m_n$, y denotamos el percentil $\alpha$ de $X$ como $f^m_{n,\alpha}$, entonces tenemos que
\begin{equation*}
\alpha=P(X<f^m_{n,\alpha})=P\left(\frac{1}{X}>\frac{1}{f^m_{n,\alpha}}\right)=1-P\left(\frac{1}{X}<\frac{1}{f^m_{n,\alpha}}\right)
\end{equation*}

De donde tenemos que $1-\alpha=P\left(\frac{1}{X}<\frac{1}{f^m_{n,\alpha}}\right)$, y vemos que $\frac{1}{f^m_{n,\alpha}}$ es el percentil de $1/X$ que se distribuye como $F^n_m$. De lo anterior, $\frac{1}{f^m_{n,\alpha}}=f^n_{m,1-\alpha}$, y finalmente tenemos que $f^m_{n,\alpha}=\frac{1}{f^n_{m,1-\alpha}}$.


En la Tabla presentamos los comandos en \verb"R" para calcular percentiles

\begin{table}
\centering
{\small
\begin{tabular}{|c|c|c|c|}\hline
Distribución&Comando&\multicolumn{2}{|c|}{Ejemplo}\\\hline
Binomial $Bin(n,p)$&\verb"qbinom"&Percentil 0.1 de $Bin(12,0.4)$&\verb"qbinom(0.1,12,0.4)"\\
Poisson $P(\lambda)$&\verb"qpois"&Percentil 0.2 de $P(5)$&\verb"qpois(0.2,5)"\\
Uniforme continua $U(a,b)$&\verb"qunif"&Percentil 0.3 de $U(3,6)$&\verb"qunif(0.3,3,6)"\\
Gamma $Gammma(k,\theta)$&\verb"qgamma"&Percentil 0.4 de $Gamma(2,8)$&\verb"qgamma(0.4,2,8)"\\
Exponencial $Exp(\theta)$&\verb"qexp"&Percentil 0.5 de $Exp(2)$&\verb"qexp(0.5,1/2)"\\
Normal $N(\mu,\sigma^2)$&\verb"qnorm"&Percentil 0.6 de $N(2,9)$&\verb"qnorm(0.6,2,sqrt(9))"\\
Normal estándar $N(0,1)$&\verb"qnorm"&Percentil 0.7 de $N(0,1)$&\verb"qnorm(0.7)"\\
$t$ student $t_$&&&\\
&&&\\
&&&\\
\end{tabular} }
\end{table}
\section{Familia exponencial}

En esta parte, se introduce el concepto la familia exponencial que es muy útil en algunos temas tratados en este libro, estos son, teoría de estimación puntual y prueba de hipótesis; y además resulta ser útil en la teoría bayesiana que sin embargo no será tratada en este libro.

\subsection{Familia exponencial uniparamétrica}
En esta parte, se introduce la familia exponencial para distribuciones que depende solamente de un parámetro que se denominará familia exponencial uniparamétrica, y la definición se da a continuación.

\begin{Defi}
Una distribución de probabilidad con parámetro $\theta$ pertenece a la familia exponencial uniparamétrica si la función de densidad se puede escribir de la forma
    \begin{equation}\label{uniexpo}
    f_X(x,\theta)=h(x)c(\theta)\exp\{d(\theta)T(x)\}
    \end{equation}
donde $T(x)$ y $h(x)$ son funciones que dependen de $x$ únicamente, y $d(\theta)$ y $c(\theta)$ son funciones que depende de $\theta$ únicamente.
\end{Defi}
En algunos textos, la presentación en la familia exponencial es $f_X(x,\theta)=\exp\{d(\theta)T(x)-c(\theta)\}h(x)$, la cual es equivalente a la definición anterior. A continuación se ilustra dos ejemplos de distribuciones pertenecientes a esta familia.
\begin{Eje}
La distribución Poisson con parámetro $\theta$ pertenece a la familia exponencial uniparamétrica puesto que
\begin{align*}
f(x,\theta)&=\frac{e^{-\theta}\theta^x}{x!}I_{\{0,1,\cdots\}}(x)\\
           &=\exp\{x\ln\theta\}\exp\{-\theta\}\frac{I_{0,1,\cdots}(x)}{x!},
\end{align*}
en conclusión $f(x,\theta)$ es de la forma (\ref{uniexpo}) con $d(\theta)=\ln\theta$, $T(x)=x$, $c(\theta)=\exp\{-\theta\}$ y $h(x)=\dfrac{I_{\{0,1,\cdots\}}(x)}{x!}$.
\end{Eje}

\begin{Eje}
La distribución Gamma con parámetro de forma $k$ conocida pertenece a la familia exponencial uniparamétrica puesto que
\begin{align*}
f(x,\theta)&=\frac{x^{k-1}e^{-x/\theta}}{\theta^k\Gamma(k)}I_{(0,\infty)}(x)\\
           &=\exp\left\{-\frac{x}{\theta}\right\}\theta^{-k}\frac{x^{k-1}I_{(0,\infty)}(x)}{\Gamma(k)},
\end{align*}
el cual es de la forma (\ref{uniexpo}) con $d(\theta)=-1/\theta$, $T(x)=x$, $c(\theta)=\theta^{-k}$ y $h(x)=\dfrac{x^{k-1}I_{(0,\infty)}(x)}{\Gamma(k)}$.

Nótese que esta representación de la familia exponencial no es única, puesto que al definir $d(\theta)=1/\theta$ y $T(x)=-x$, también se puede concluir que la distribución Gamma con $k$ fijo pertenece a la familia exponencial uniparamétrica.
\end{Eje}

En casi toda la teoría estadística, se tratan más de una variable aleatoria que en la práctica, se observan los datos que corresponden a realizaciones de estas variables. En este caso, se examina la pertenencia de la familia exponencial de la función de densidad conjunta, y tenemos el siguiente resultado.
\begin{Res}
Si $X_1$, $\cdots$, $X_n$ son variables aleatorias independientes e idénticamente distribuidas con función de densidad común perteneciente a la familia exponencial uniparamétrica, entonces la función de densidad conjunta $f(x_1,\cdots,x_n)$ también pertenece a la familia exponencial uniparamétrica.
\end{Res}
\begin{proof}
\begin{align}\label{exponencial_muestra}
f(x_1,\cdots,x_n,\theta)&=\prod_{i=1}^nf(x_i,\theta)\notag\\
                        &=\prod_{i=1}^nh(x_i)c(\theta)\exp\{d(\theta)T(x_i)\}\notag\\
                        &=c(\theta)^n\left[\prod_{i=1}^nh(x_i)\right]\exp\left\{d(\theta)\sum_{i=1}^nT(x_i)\right\}
\end{align}
el cual es de la forma (\ref{uniexpo}).
\end{proof}

Usando el anterior resultado junto con el Ejemplo 1.2.1 donde se mostró que la distribución Poisson pertenece a la familia exponencial, podemos afirmar que cuando se tiene $n$ variables independientes e idénticamente distribuidas con distribución Poisson, la función conjunta también pertenece a la familia exponencial.

Otra utilidad del resultado es que cuando se necesita ver que una función de densidad conjunta pertenece a la familia exponencial, basta ver para la función de densidad marginal.

La estadística $\sum_{i=1}^nT(x_i)$ en (\ref{exponencial_muestra}) será de gran interés en los futuros capítulos, y estamos interesados en conocer sus propiedades como la esperanza y la varianza, y para eso solo necesitamos $E(T(X))$ y $Var(T(X))$ para $X$ con la misma distribución que $X_1$, $\cdots$, $X_n$. El siguiente resultado nos provee las respectivas fórmulas.

\begin{Res}
Si $X$ es una variable aleatoria con función de densidad perteneciente a la familia exponencial de la forma (\ref{uniexpo}), entonces
\begin{enumerate}
    \item $E\left(\dfrac{\partial d(\theta)}{\partial\theta}T(X)\right)=-\dfrac{\partial}{\partial\theta}\ln c(\theta)$
    \item $Var\left(\dfrac{\partial d(\theta)}{\partial\theta}T(X)\right)=-\dfrac{\partial^2}{\partial\theta^2}\ln c(\theta)-E\left(\dfrac{\partial^2d(\theta)}{\partial\theta^2}T(X)\right)$
\end{enumerate}
\end{Res}

Ahora, las distribuciones con dos parámetros, como la distribución $N(\mu,\sigma^2)$ puede considerarse como distribución uniparamétrica cuando $\mu$ o $\sigma^2$ se considera fijo conocido. Y se puede ver que en ambos casos, la distribución pertenece a la familia exponencial uniparamétrica.


\subsection{Familia exponencial multi-paramétrica}
Las distribuciones Gamma, normal y beta dependen de dos parámetros, y para estas distribuciones, se puede generalizar la definición de la familia exponencial uniparamétrica para distribuciones dependientes de más de un parámetro. La definición correspondiente se da a continuación.
\begin{Defi}
Una distribución de probabilidad pertenece a la familia exponencial multi-paramétrica si la función de densidad se puede escribir de la forma
    \begin{equation}\label{multiexpo}
    f_X(x,\btheta)=c(\btheta)h(x)\exp\{d(\btheta)'T(x)\}
    \end{equation}
donde $T(x)$ y $d(\btheta)$ son funciones vectoriales, $h(x)$ y $c(\btheta)$ son funciones reales.
\end{Defi}

\begin{Eje}
La distribución Gamma con parámetro de forma $k$ y parámetro de escala $\theta$ pertenece a la familia exponencial multi-paramétrica pues
\begin{align*}
f_X(x)&=\frac{x^{k-1}e^{-x/\theta}}{\theta^k\Gamma(k)}I_{(0,\infty)}(x)\\
      &=\exp\{-\frac{x}{\theta}+(k-1)\ln{x}\}\frac{1}{\theta^k\Gamma(k)}I_{(0,\infty)}(x)\\
      &=\exp\left\{\Bigl(-\frac{1}{\theta},k-1\Bigr)\begin{pmatrix}x\\ \ln{x}\end{pmatrix}\right\}\frac{1}{\theta^k\Gamma(k)}I_{(0,\infty)}(x)
\end{align*}
el cual es de la forma (\ref{multiexpo}) con $\btheta=(\theta,k)'$, $d(\btheta)=(-\frac{1}{\theta},k-1)'$,
$c(\btheta)=\frac{1}{\theta^k\Gamma(k)}$, $T(x)=(x,\ln{x})'$ y $h(x)=I_{(0,\infty)}(x)$.
\end{Eje}

Nótese que al igual a la familia exponencial uniparamétrica, la representación de la familia exponencial multi-paramétrica tampoco es única, pues en el ejemplo anterior también se puede tomar
$\eta(\mathbf{\theta})=(\frac{1}{\theta},k-1)'$ y $T(x)=(-x,\ln{x})'$.

Las distribuciones normal, gamma, exponencial, chi-cuadrado, beta, Bernoulli, binomial, binomial negativa, multinomial, Poisson y geométrica todas pertenecen a la familia exponencial. También la distribución Weibull pertenece a la familia exponencial cuando el parámetro de forma es conocida. Por el otro lado, las distribuciones Cauchy, Laplace, uniforme y Weibull cuando el parámetro de forma es desconocida no pertenecen a la familia exponencial.

La razón por la que las distribuciones de la familia uniforme no pertenecen a la familia exponencial va más allá de los objetivos de este libro,
pues se necesita conocimientos sobre teoría estadística basada en la teoría de la medida. La afirmación  de que la distribución uniforme no pertenece a la familia exponencial porque no se puede escribir de la forma (\ref{multiexpo}) no es una razón válida.

\section{Ejercicios}
1. (Captura-recaptura) Suponga que un piscicultor necesita conocer el número de peces que tiene en un estanque, pero por limitación de los recursos humanos y
económicos no puede sacar todos los peces del estanque y contar uno por uno, usted como conocer de la ciencia estadística, ¿cómo le puede ayudar? (Ayuda: Usa la distribución hipergeométrica).

2. Un vendedor de seguros realiza en promedio 10 visitas por semana a los posibles clientes, él por experiencia sabe que la probabilidad de que un cliente compre el seguro en una visita es de 0.2, y suponga que el resultado de una visita no se ve afectado por resultados de visitas anteriores.
\begin{enumerate}[(a)]
    \item ¿Cuál es la probabilidad de que el vendedor en una semana venda más de dos seguros?
    \item Suponga que el vendedor ha tenido semanas consecutivas con muy malas ventas, y que perderá el trabajo si en la próxima semana no vende por lo menos un seguro. Por lo tanto el vendedor decide aumentar el número de visitas a la semana para tratar al menos un seguro, ¿por lo menos cuántas visitas debe realizar la próxima semana para que la probabilidad de vender al menos un seguro sea superior a 90\%?
\end{enumerate}

3. Demuestre el Resultado 1.1.6. (Ayuda: primero encuentra $m_X(t)$, luego calulcar $E(X)$ y $Var(X)$, para encontrar $m_X(t)$ usar la expansión de Taylor de $e^x$.)

4. Demuestre el Resultado 1.1.12.

5. Para las distribuciones exponencial y chi-cuadrado, escriba cuál es el parámetro y cuál es el espacio paramétrico.

6. Sea $X\sim N(\mu,\sigma^2)$, y sea $Z_1=\frac{X-\mu}{\sigma}$ y $Z_2=\frac{\mu-X}{\sigma}$, compruebe que $Z_1$ tiene distribución normal estándar, y $Z_2$ también, es decir, la forma de estandarizar una variable con distribución normal no es única.

7. Sea $X_1$, $\cdots$, $X_n$ variables aleatorias independientes e idénticamente distribuidas con distribución $N(\mu,\sigma^2)$, encuentre la distribución de la variable $\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$.

8. Demuestre el Resultado 1.1.15.

9. Calcule las siguientes probabilidades:
\begin{enumerate}[(a)]
    \item $P(X>2)$, $P(X<-1)$ y $P(1<X<3)$ donde $X\sim N(1.5,4)$.
    \item $P(\frac{X-2Y}{3}>4)$ donde $X\sim N(1.5,4)$ y $Y\sim N(-1,2)$ son variables independientes.
\end{enumerate}

10. \begin{enumerate}[(a)]
    \item Calcule el percentil 5\%, 10\% y 98\% de una variable con distribución normal estándar.
    \item Calcule el percentil 5\% y 90\% de una variable con distribución $N(1,3)$.
    \item Encuentre valores $a$ y $b$ tales que $P(a<Z<b)=0.98$.
    \item Encuentre valores $a$ y $b$ tales que $P(a<X<b)=0.95$ donde $X\sim N(-2,5)$.
\end{enumerate}

11. Encuentre los percentiles 5\%, 98\% de una variable con distribución chi-cuadrado con 8 y 15 grados de libertad, respectivamente.

12. Encuentre los percentiles 5\%, 98\% de una variable con distribución t-student con 10 y 20 grados de libertad, respectivamente.

13. Encuentre los percentiles 5\%, 98\% de una variable con distribución $F_5^8$ y $F_8^12$, respectivamente.

14. Demuestre las partes 1 y 2 del resultado 1.1.16 usando la definición 1.1.11.

15.Demuestre que las siguientes distribuciones pertenecen a la familia exponencial identificando las funciones $d(\theta)$, $T(x)$, $c(\theta)$ y $h(x)$:
\begin{enumerate}[(a)]
    \item Binomial con $n$ conocido.
    \item Exponencial.
    \item Distribución normal con media $\mu$ conocido.
    \item Distribución normal con varianza $\sigma^2$ conocida.
    \item La función de densidad conjunta de $X_1$, $\cdots$, $X_n$, donde $X_i\sim_{iid}N(\mu,\sigma^2)$ con $\mu$ conocido.
    \item La función de densidad conjunta de $X_1$, $\cdots$, $X_n$, donde $X_i\sim_{iid}P(\lambda)$.
\end{enumerate}

16. La función de densidad de la distribución Weibull está dada por:
$$f(x)=\alpha\beta x^{\beta-1}\exp\{-\alpha x^\beta\}I_{(0,\infty)}(x)$$
donde $\alpha>0$ y $\beta>0$. Demuestre que cuando $\beta$ es conocido, la distribución pertenece a la familia exponencial uniparamétrica, identifica $d(\theta)$, $T(x)$, $c(\theta)$ y $h(x)$.

17. Considera la distribución $N(\mu,\sigma^2)$,
\begin{enumerate}[(a)]
    \item describa cómo es la forma del vector de parámetros y el correspondiente espacio paramétrico
    \item demuestre que esta distribución pertenece a la familia exponencial multiparamétrica, identifica $d(\theta)$, $T(x)$, $c(\theta)$ y $h(x)$.
\end{enumerate}

18. La función de densidad de la distribución Beta está dada por:
$$f(x)=\frac{x^{a-1}(1-x)^{b-1}}{B(a,b)}I_{(0,1)}(x)$$
donde $a>0$ y $b>0$, y $B(a,b)=\int_{0}^1x^{a-1}(1-x)^{b-1}dx$. Demuestre que la distribución Beta pertenece a la familia exponencial multiparamétrica, identifica $d(\theta)$, $T(x)$, $c(\theta)$ y $h(x)$.

19. Sean $X_1$, $\cdots$, $X_n$ variables aleatorias con $X_i\sim_{iid}N(\mu_1,\sigma^2)$ para $i=1,\cdots,n$ y Sean $Y_1$, $\cdots$, $Y_m$ variables con $Y_j\sim_{iid}N(\mu_2,\sigma^2)$ para $j=1,\cdots,m$, además las variables $X_i$ son independientes de las variables $Y_j$, demuestre que la función de densidad conjunta de $X_1$, $\cdots$, $X_n$, $Y_1$, $\cdots$, $Y_m$ pertenece a la familia exponencial con tres parámetros.

20. Comprobar que en una distribución normal estándar $z_p=-z_{1-p}$, corrobora lo anterior con los percentiles dados en la Tabla 1.1.

