\chapter[Estimación puntual]{Estimación puntual}

\section{Introducción}

Desde la revolución estadística de Pearson y Fisher, la inferencia estadística busca encontrar los valores que parametrizan a la distribución desconocida de los datos. El primer enfoque, propuesto por Pearson, afirmaba que si era posible observar a la variable de interés en todos y cada uno de los individuos de una población, entonces era posible calcular los parámetros de la distribución de la variable de interés; por otro lado, si sólo se tenía acceso a una muestra representativa, entonces era posible calcular una estimación de tales parámetros.

Fisher discrepó de tales argumentos, asumiendo que las observaciones están sujetas a un error de medición y por lo tanto, así se tuviese acceso a toda la población, es imposible calcular los parámetros de la distribución de la variable de interés.

Del planteamiento de Fisher resultaron una multitud de métodos estadísticos para la estimación de los parámetros teóricos. Es decir, si la distribución de $ X$ está para\-me\-tri\-za\-da por $ \theta \in \Theta$, con $ \Theta$ el espacio paramétrico inducido por el comportamiento de la variable de interés, el objetivo de la teoría estadística inferencial es calcular una estimación $ \hat{\theta}$ del parámetro $ \theta$ por medio de los datos observados. En este enfoque, los parámetros se consideran cantidades fijas y constantes. 

El anterior será el enfoque que seguiremos a lo largo del desarrollo de este libro, aunque el lector debe tener conocimiento de que no es el único enfoque que los estadísticos utilizan en términos de inferencia acerca de los parámetros de una distribución.

Nótese que en términos de inferencia estadística existen, por lo menos, el enfoque clásico, propuesto por Fisher y desarrollado en este libro, el enfoque bayesiano \cite{Gelman}, el enfoque no paramétrico \cite{Conover98} y el enfoque de inferencia en poblaciones finitas \cite{Gutierrez}.

\section{Conceptos básicos}

Muchos de los estudios estadísticos están enfocados en estudiar características de una población objetiva; por ejemplo, una empresa productora puede estar interesada en conocer el gasto promedio semanal en alimentos de las familias de estrato so\-cio\-eco\-nó\-mi\-co bajo, con el fin de diseñar una estrategia de mercadeo para promover la demanda en el mercado. Es claro que en la ciudad hay una gran cantidad de familias de este perfil, y por consiguiente resulta prácticamente imposible saber el gasto promedio semanal de cada una de estas familias.

En casos como el anterior, la solución es inferir acerca de la característica de la población usando información obtenida de un subconjunto o una muestra de la población, y técnicas de esta rama constituyen la inferencia estadística. A continuación se presentan algunos conceptos básicos para poder estudiar la teoría de la inferencia estadística.
\begin{Defi}
Una muestra aleatoria\index{Muestra aleatoria} de tamaño $n$ es un conjunto constituido por $n$ variables aleatorias independientes e idénticamente distribuidas, $X_1$, $\cdots$,$X_n$.
\end{Defi}

Una muestra aleatoria es utilizada para lograr el objetivo de estimar un parámetro teórico desconocido, y esto se logra usando una función o funciones de las variables aleatorias de la muestra, conocidas como estadísticas. La definición formal se enuncia a continuación.
\begin{Defi}
Una estadística\index{Estadística} es una función de variables aleatorias de una muestra aleatoria que no contiene parámetros desconocidos.
\end{Defi}

Algunas estadísticas comunes basadas en una muestra aleatoria $X_1$, $\cdots$m $X_n$ son:
\begin{itemize}
\item El promedio muestral o la media muestral\index{Media muestral}, definido como $\bar{X}=\sum_{i=1}^nX_i/n$.
\item Las varianzas muestrales, definidas como $S^2_{n}=\sum_{i=1}^n(X_i-\bar{X})^2/n$ y $S^2_{n-1}=\sum_{i=1}^n(X_i-\bar{X})^2/(n-1)$.
\item Mínimo y máximo de la muestra\index{Mínimo de una muestra}\index{Máximo de una muestra}, definidos como $X_{(1)}=\min\{X_1,\cdots,X_n\}$ y $X_{(n)}=\max\{X_1,\cdots,X_n\}$. Nótese que en general la estadística $X_{(1)}$, al igual que $X_{(n)}$, no es ninguna de las variables $X_1$, $\cdots$, $X_n$. Considere un experimento aleatorio con $\Omega=\{a,b,c\}$ y se definen dos variables aleatorias $X_1$ y $X_2$ sobre $\Omega$ con $X_1(a)=X_1(b)=1$, $X_1(c)=2$ y $X_2(a)=0$, $X_2(b)=X_2(c)=2$. Si denotamos al mínimo de $X_1$ y $X_2$ como $Y$, se puede ver que $Y(a)=0$, $Y(b)=1$ y $Y(c)=2$, y claramente $Y$ no es ninguna de las variables $X_1$ y $X_2$. De esta forma si la función de densidad de donde proviene la muestra es $f_X$, la función de densidad de $X_{(1)}$ y la de $X_{(n)}$ no corresponden a $f_X$.
    Veamos
    \begin{align*}
    Pr(X_{(1)}>x)&=Pr(X_1>x,\cdots,X_n>x)\\
    &=Pr(X_1>x)\cdots Pr(X_n>x)\\
    &=(1-F_{X_1}(x))\cdots (1-F_{X_n}(x))\\
    &=(1-F_X(x))^n
    \end{align*}
    donde $F_X$ denota la función de distribución común de la muestra. Por otro lado, $Pr(X_{(1)}>x)=1-F_{X_{(1)}}(x)$, de esta forma, tenemos que
    \begin{equation*}
        F_{X_{(1)}}(x)=1-(1-F_X(x))^n,
    \end{equation*}
    y podemos hallar la función de densidad para encontrar la función de densidad de $X_{(1)}$ como
    \begin{equation}
        f_{X_{(1)}}(x)=nf_X(x)(1-F_X(x))^{n-1}.
    \end{equation}
    Usando lo anterior, podemos encontrar que la función de densidad de $X_{(1)}$ en una muestra con distribución exponencial de media $\theta$, corresponde a
    \begin{equation*}
        f_{X_{(1)}}(x)=\frac{n}{\theta}e^{-nx/\theta}I_{(0,\infty)}(x).
    \end{equation*}
    Usando un razonamiento análogo, se puede ver (Ejercicio 2.1) que para la estadística máximo $X_{(n)}$, se tiene que
    \begin{equation}\label{F_max}
        F_{X_{(n)}}(x)=F_X(x)^n
    \end{equation}
    y
    \begin{equation}\label{f_max}
        f_{X_{(n)}}(x)=nf_X(x)F_X(x)^{n-1}.
    \end{equation}
\end{itemize}

Una vez definido el concepto de estadística, podemos definir lo que es un estimador. Aunque en muchos contextos, lo que se desea estimar es el parámetro de una distribución, $\theta$, en algunos casos, lo que nos interesa es una función del parámetro $g(\theta)$. Por ejemplo, suponga que en la línea de atención al cliente de una empresa, para describir el tiempo de espera para ser atendido por un asesor se emplea una distribución $Exp(\theta)$. En este caso, $\theta$ describe el tiempo de espera promedio que es el parámetro de interés; otra cantidad que puede resultar interesante es, por ejemplo, la probabilidad de que un cliente tenga que esperar menos de 5 minutos antes de ser atendida, la cual es $1-e^{-5/\theta}$, que es una función del parámetro $\theta$. Por esta razón, a continuación se presenta la definición general para un estimador de $g(\theta)$.

\begin{Defi}
Una estadística\index{Estimador} $T$ cuyos valores son utilizados para estimar una función del parámetro $g(\theta)$ es un estimador de $g(\theta)$ y las realizaciones del estimador se llaman estimaciones\index{Estimación} y se denotan por $t$.
\end{Defi}
Nótese que un estimador es una función de variables aleatorias, de tal manera que cuando las variables aleatorias se cambian de valor, el estimador también. Por lo tanto, cuando la muestra aleatoria cambia, el valor que toma el estimador, es decir, la estimación también cambia. Por lo tanto, un mismo estimador puede producir diferentes estimaciones si se cambia la muestra aleatoria. Y de lo anterior, debe quedar claro que un estimador es aleatorio, mientras que una estimación es un número, puesto que es la realización numérica del estimador. En la literatura estadística, se acostumbra denotar a los estimadores con letras mayúsculas, y a las estimaciones, minúsculas. De esta forma, las realizaciones de $\bar{X}$ se denotan como $\bar{x}$, las de $S^2_n$ como $s^2_n$ y análogamente para cualquier otro estimador.

Pongamos un ejemplo: suponga que se desea conocer la cantidad promedio de dinero que se gasta semanalmente una familia de estrato 2 en la compra de arroz. Un estadístico A seleccionó una muestra de 20 hogares, y utilizó como estimador el promedio muestral y tuvo como resultado $\bar{x}=5300$ pesos; ahora, otro estadístico B seleccionó una muestra de 30 personas, y utilizó el mismo estimador, es decir, $\bar{X}$ y tuvo como resultado $\bar{x}=4700$. En este caso, los dos usaron el mismo estimador; sin embargo, las dos estimaciones que obtuvieron no son iguales.

\section{Estimaciones puntuales}
El tópico de estimación puntual consiste en encontrar estimadores para estimar un cierto parámetro $\theta$ o una función de este $g(\theta)$. Dada la definición de estimador, es claro que cualquier estadística puede ser un estimador, y por consiguiente cada persona puede usar cualquier estadística para estimar según su antojo.

Tome el ejemplo de estimar el gasto promedio semanal en alimentos de familias de estrato socioeconómico bajo, si la muestra aleatoria es de tamaño 10, y los valores numéricos (en miles de pesos) son: 50, 62, 53, 65, 70, 64, 60, 58, 62 y 65, una forma razonable de estimar es tomar el promedio muestral $\bar{X}$ como estimador del media teórica, y arroja como resultado $\bar{x}=60900$ pesos, la que parece ser una estimación aceptable; pero esta no es la única forma de estimar, podemos definir otras estadísticas, por ejemplo: $\sum_{i=1}^nX_i$, $(X_{(N)}+X_{(1)})/2$, u otras estadísticas no tan lógicas como $\exp\{X_1+\cdots+X_n\}$, $\sum_{i=1}^nX_i^2$ o cualquier otra estadística que se nos viene a la mente. Sin embargo, en la literatura estadística existen, por lo menos, dos métodos estándares que nos ayudan a construir estimadores: el método de máxima verosimilitud y el método de momentos que se estudiará a continuación.

\subsection{Método de máxima verosimilitud\index{Método de máxima verosimilitud}}

Suponga que se desea estimar $g(\theta)$ basada en una muestra observada $x_1$, $\cdots$, $x_n$. La idea del método de máxima verosimilitud se basa en encontrar el valor de $g(\theta)$ que maximiza la probabilidad de observar la muestra $x_1$, $\cdots$, $x_n$. Esto es, el valor de $g(\theta)$ que hace más creíble a la muestra observada, y de allí viene el nombre de máxima verosimilitud.

Introducimos este método con un ejemplo muy sencillo, suponga que la alcaldía local está interesada en conocer el número promedio de homicidios mensuales ocurridos en la localidad de Usaquén de Bogotá. Dadas las características de esta variable de estudio, se puede pensar que ésta sigue una distribución Poisson con el parámetro desconocido $\theta$. Como $\theta$ es la esperanza de la distribución Poisson, entonces lo que interesa a la alcaldía es conocer el valor de $\theta$. Además, suponga que durante los últimos tres meses, el número de homicidios fue:  11, 9 y 7 respectivamente.

En la anterior situación, $\theta$ es el parámetro del modelo probabilístico que rige en la población, donde sólo están disponibles las realizaciones de tres variables, que al suponer que el tiempo no es un factor importante, constituyen una muestra aleatoria de tamaño 3, y los denotamos por $X_1$, $X_2$, $X_3$. Ahora podemos hacer la siguiente pregunta: ¿cuál es la probabilidad de que la muestra aleatoria tenga como realización los valores 11, 9 y 7? Es decir, ¿cuál es la probabilidad de observar lo que realmente sucedió? Es claro que esta probabilidad depende del parámetro desconocido $\theta$. Tenemos

Si $\theta=6$, entonces
\begin{equation*}
Pr(X_1=11,X_2=9,X_3=7)=\frac{e^{-6}6^{11}}{11!}\frac{e^{-6}6^9}{9!}\frac{e^{-6}6^7}{7!}=2.1\times10^{-4}.
\end{equation*}

Si $\theta=8$, entonces
\begin{equation*}
Pr(X_1=11,X_2=9,X_3=7)=\frac{e^{-8}8^{11}}{11!}\frac{e^{-8}8^9}{9!}\frac{e^{-8}8^7}{7!}=1.3\times10^{-3}.
\end{equation*}

Si calculamos esta misma probabilidad para otros valores de $\theta$, podemos obtener la Tabla 2.1, donde se observa que la probabilidad es más grande cuando $\theta=9$.\footnote{El parámetro $\theta$ puede tomar cualquier valor positivo, no necesariamente entero, aquí se con\-si\-de\-ró solo algunos valores para $\theta$, con el fin de introducir el método de máxima verosimilitud, más no es un procedimiento riguroso.} Ahora, como ya se observaron los valores 11, 9 y 7, es natural pensar que la probabilidad de asociada a estos valores fuera grande, y esto nos conduce a que el valor más plausible para $\theta$ debe ser 9.

\begin{table}[!htb]
\begin{tabular}{ccccccc}\hline
$\theta$&5&6&7&8&9&10\\\hline
Pr&$3.1\times10^{-5}$&$2.1\times10^{-4}$&$6.8\times10^{-4}$&$1.3\times10^{-3}$&$1.5\times10^{-3}$&$1.3\times10^{-3}$\\\hline
\end{tabular}
\caption[\textsl{Ilustración del estimador MV}]{\textsl{Probabilidad de observar la muestra de tamaño 3 conformado por 11,9,7 provenientes de una distribución $Pois(\theta)$ para diferentes valores de $\theta$.}}
\end{table}

El anterior razonamiento induce el método de máxima verosimilitud. Para estudiar este método, primero se da la siguiente definición.
\begin{Defi}
Dadas $n$ variables aleatorias $X_1$, $\cdots$, $X_n$, la función de verosimilitud\index{Función de verosimilitud} se define como la función de densidad conjunta de las $n$ variables, y se denota por $L(x_1,\cdots,x_n,\theta)$.
\end{Defi}

Aunque en este texto se trabaja solamente muestras aleatorias, la definición de la función de verosimilitud presentada anteriormente es válida en cualquier conjunto de variables aleatorias. En particular, cuando las $n$ variables conforman una muestra aleatoria, la función de verosimilitud queda expresada como
\begin{equation*}
L(x_1,\cdots,x_n,\theta)=f(x_1,\theta)\cdots f(x_n,\theta)=\prod_{i=1}^nf(x_i,\theta)
\end{equation*}

donde $f$ es la función de densidad común para las $n$ variables. También nótese que cuando solo se dispone de una variable aleatoria $X$, la función de verosimilitud es simplemente la función de densidad de $X$, esto es, $f_X(x)$.

Dada la definición de la función de verosimilitud, el método de máxima verosimilitud para un parámetro $\theta$ consiste en encontrar el valor de $\theta$ que maximice esta función, éste será el estimador de máxima verosimilitud de $\theta$\index{Estimador!de máxima verosimilitud} y lo denotaremos por $\hat{\theta}_{MV}$. Cuando la función de verosimilitud es una función continua de $\theta$ y además derivable, entonces podemos usar la primera y la segunda derivada para encontrar el estimador de máxima verosimilitud. Lo ilustramos con el siguiente ejemplo:

\begin{Eje}
\index{Estimador!de máxima verosimilitud!Poisson}Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con distribución $Pois(\theta)$, el estimador de máxima verosimilitud de $\theta$ es el promedio muestral $\bar{X}$. Para verificar esta afirmación, se calcula primero la función de verosimilitud:
\begin{align*}
L(x_1,\cdots,x_n,\theta)&=f(x_1,\theta)\cdots f(x_n,\theta)\\
                        &=\frac{e^{-\theta}\theta^{x_1}}{x_1!}I_{\{0,1,\cdots\}}(x_1)\cdots\frac{e^{-\theta}\theta^{x_n}}{x_1!}I_{\{0,1,\cdots\}}(x_n)\\
                        &=\frac{e^{-n\theta}\theta^{\sum_{i=1}^nx_i}}{\prod_{i=1}^nx_i!}\prod_{i=1}^nI_{\{0,1,\cdots\}}(x_i)
\end{align*}
Encontrar el valor de $\theta$ para maximizar la anterior expresión de $L(x_1,\cdots,x_n,\theta)$ es equivalente a maximizar $e^{-n\theta}\theta^{\sum_{i=1}^nx_i}$, pues ésta es la parte que depende de $\theta$. Ahora, encontrar el valor que maximiza una función es equivalente a encontrar el valor que maximiza el logaritmo natural de esta función, pues la función logarítmico es estrictamente creciente. Por lo tanto, basta encontrar el valor de $\theta$ que maximiza
\begin{equation*}
L'(\theta)=\ln(e^{-n\theta}\theta^{\sum_{i=1}^nx_i})=-n\theta+\sum_{i=1}^nx_i\ln\theta
\end{equation*}

En la distribución $Pois(\theta)$, el espacio paramétrico es $(0,\infty)$, y la función $L'(\theta)$ es función derivable, entonces la forma de hallar el máximo de la función será resolver la ecuación $\frac{\partial L'(\theta)}{\partial\theta}=0$. En este caso, la solución es $\theta=\sum x_i/n$ cuando $\sum x_i\neq0$, es decir, por lo menos una de las observaciones debe ser estrictamente mayor a 0; cuando $\sum x_i=0$, es decir, cuando $x_1=\ldots=x_n=0$, no existe el estimador de máxima verosimilitud de $\theta$. Supongamos que $\sum x_i\neq0$, calculamos la segunda derivada de $L(\theta)$ evaluada en la anterior solución, tenemos que
\begin{equation*}
\left.\frac{\partial^2L'(\theta)}{\partial\theta^2}\right|_{\theta=\sum x_i/n}=\left.\frac{-\sum x_i}{\theta^2}\right|_{\theta=\sum x_i/n}= \frac{-n^2}{\sum x_i}
\end{equation*}

las observaciones $x_i$ provienen de distribuciones tipo Poisson, lo cual garantiza que toman valores no negativos, de donde concluimos que la anterior expresión es negativa, lo cual verifica que la solución hallada $\theta=\sum x_i/n$ efectivamente maximiza la función de verosimilitud.

En conclusión, el estimador de máxima verosimilitud del parámetro $\theta$ de una distribución Poisson es el promedio muestral $\bar{X}$, y lo anterior también concuerda con el razonamiento, pues $\theta$ es el valor esperado o el media teórica, y es lógico estimar el media teórica con el promedio muestral.
\end{Eje}

Veamos una aplicación del anterior estimador.

\begin{Eje}
Suponga que una organización internacional de derechos humanos necesita conocer el número de muertes violentas que ocurren mensualmente en una determinada ciudad, y para eso se seleccionaron 15 de los 63 barrios donde los resultados son 1, 1, 5, 5, 2, 3, 3, 6, 4, 3, 2, 3, 2, 3 y 4. Si denotamos el número de muertes violentas mensuales en un barrio de la ciudad por $X$, entonces $X$ toma valor en $\{0,1,\cdots\}$; por consiguiente, una distribución apropiada para $X$ puede ser la distribución $Pois(\theta)$. Y por el anterior ejemplo, tenemos que la estimación de máxima verosimilitud para $\theta$ es el promedio muestral, esto es, $\hat{\theta}_{MV}=\bar{x}= 3.13$.

Aparte de estimar el parámetro $\theta$ que puede ser interpretado como el número promedio de muertes violentas mensuales que ocurren en un barrio de la ciudad, podemos estimar otras cantidades que permiten a la organización tener una mejor idea acerca de la ciudad. Por ejemplo, ¿cuál es la probabilidad de que en un barrio no ocurra ninguna muerte violenta durante un mes? Usando propiedades de la distribución Poisson, tenemos que esta probabilidad es igual a $Pr(X=0)=e^{-\theta}$. Dado que ya se obtuvo una estimación para $\theta$, podemos utilizarla para estimar $e^{-\theta}$ como $e^{-3.1}=0.045$. Más aún, podemos afirmar que esta estimación es de máxima verosimilitud (la teoría se verá más adelante).

No solo podemos hacer inferencia al nivel de los barrios sino también al nivel de la ciudad. Dado que esta está compuesta por 63 barrios, entonces el número de muertes violentas mensuales en la ciudad es la suma de los 63 barrios. Si denotamos con $Y_i$ el número de muertes violentas mensuales en el $i$-ésimo barrio, entonces $Y=\sum_{i=1}^{63}Y_i$ denota el número de muertes violentas en la ciudad. Y usando el Resultado 1.1.9, podemos ver que $Y\sim Pois(63\theta)$, por lo tanto, el número promedio mensual de muertes violentas es $63\theta$, y una estimación de ésta será $63\times3.13\approx197$. Y podemos usar esta estimación para estimar cantidades como probabilidad de que en un mes en la ciudad ocurra menos de 100 muertes violentas u otras probabilidades de interés.
\end{Eje}

En muestras provenientes de distribuciones como $Exp(\theta)$ o $Ber(\theta)$\index{Estimador!de máxima verosimilitud!exponencial}\index{Estimador!de máxima verosimilitud!Bernoulli}, el pro\-ce\-di\-mien\-to para encontrar el estimador $\hat{\theta}_{MV}$ es similar al ejemplo anterior y se puede ver fácilmente que $\hat{\theta}_{MV}=\bar{X}$, de nuevo se encuentra que el estimador de máxima verosimilitud del media teórica es el promedio muestral en estas dos distribuciones (Ejercicios 2.3 y 2.6).

\begin{Eje}
Suponga que una empresa de EPS para mascotas que cuenta con sedes en diferentes ciudades en Colombia tiene vendedores que hacen visita a clientes potenciales, estos son, los hogares que tienen mascota, para ofrecer los productos. Es claro que para fijar metas de venta, el gerente de la empresa debe conocer el rendimiento de los vendedores y así fijar un número de visitas que éstos deben realizar con el fin de lograr la meta de venta. En este caso, el gerente necesita conocer cuál es la probabilidad de que un vendedor logre obtener una venta exitosa en una visita.

Para economizar los recursos, el gerente hace seguimiento a 18 visitas, y en cada visita denota el éxito con 1 y fracaso con 0, de esta forma la muestra observada está constituida por 18 números de la forma 0 y 1, y por el contexto del problema, podemos identificar la distribución Bernoulli, y así estimar la probabilidad de éxito en cada ensayo usando $\bar{X}$, que en este caso corresponde al número de éxitos dividido por el número total de visitas. Así que si en las 18 visitas registradas el número de ventas exitosas es 3, la probabilidad estimada de que un vendedor de esta empresa logre una venta exitosa en una visita será $\hat{p}_{MV}=3/18\approx0.167$.

Ahora, suponga que un vendedor en un día promedio realiza 5 visitas, y el gerente está interesado en conocer qué tan probable es que en las 5 visitas el vendedor logre por lo menos una venta exitosa. Si denotamos el número de ventas exitosas en las 5 visitas por $X$, podemos calcular esta probabilidad como
\begin{align*}
Pr(X>1)=1-Pr(X=0)=1-(1-p)^5.
\end{align*}
Para encontrar una estimación de esta probabilidad, podemos pensar en usar la estimación de máxima verosimilitud de $p$ encontrada anteriormente, de esta forma, te\-ne\-mos que una estimación de $Pr(X_1)$ será $1-(1-0.167)^5=0.598$. Análogamente, también podemos estimar la probabilidad de vender un seguro en las cinco visitas. En este caso, esta probabilidad está dada por $5Pr(1-p)^4$, que puede ser estimada como $5*0.167*(1-0.167)^4=0.402$.

La pregunta interesante es ¿se puede afirmar que estas dos últimas estimaciones siguen siendo de máxima verosimilitud? Esta pregunta la responderemos más adelante.
\end{Eje}

\begin{Eje}
En muchas aerolíneas, se pueden comprar tiquetes por medio de llamadas telefónicas atendidas por operadores de la aerolínea. Si un cliente debe esperar mucho tiempo en la línea para ser atendido, es más probable que el cliente desista, con lo cual la aerolínea perdería un cliente potencial. Por lo tanto la aerolínea desea conocer el rendimiento de los operadores que atienden estas llamadas. Para eso, se observan aleatoriamente 20 llamadas y se registra el tiempo transcurrido antes de que fueran atendidas por un operador. Estos tiempos en minutos son 0.13, 0.06, 0.50, 0.41, 1.44, 0.60, 0.22, 1.08, 0.78, 0.92, 2.73, 0.83, 0.19, 0.21, 1.75, 0.79, 0.02, 0.05, 2.30 y 1.03. Dado el contexto, se desea estimar el tiempo promedio que debe esperar un cliente antes de ser atendido, es decir, el media teórica. Para eso, necesitamos, en primer lugar, suponer una distribución adecuada para los datos. Los datos a la mano son del tipo continuo, además solo toma valores positivos, de donde podemos proponer una distribución exponencial, gamma o una distribución normal \footnote{Aunque una distribución normal toma valores en todos los números reales, pero se concentra alrededor de la media, por lo tanto, una muestra de valores positivos también pueden provenir de la distribución normal.}.

Una forma de verificar la distribución de los datos es observar el histograma, el cual está dado en la Figura 2.1, donde podemos ver que la forma de las barras se asemeja a la función de densidad de una distribución exponencial. Otra forma de ver la distribución de los datos es usando las gráficas de QQ plot\index{Gráficas QQ plot}, y la presentamos en la Figura 2.2 para la distribución exponencial y la distribución normal \footnote{La inversa de la función de distribución de la distribución Gamma es difícil de hallar y por consiguiente no es posible encontrar el QQ plot para verificar que un conjunto de datos provienen de una distribución Gamma.}. Podemos ver que una vez más, la distribución exponencial parece ser apropiada para los datos. Entonces el problema se convierte en estimar el parámetro $\theta$ de una distribución $Exp(\theta)$, puesto que la esperanza de la distribución es $\theta$. Y como se observó anteriormente, $\hat{\theta}_{MV}=\bar{X}$, podemos tener que la estimación de máxima verosimilitud de $\theta$ es $\bar{x}=0.8$ minutos.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{Hist_ejemplo224.eps}
\caption{\textsl{Histograma de los datos del Ejemplo 2.3.4.}}
\end{figure}


\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{Ejemplo2.2.4.eps}
\caption{\textsl{QQ plot para verificar la distribución de los datos del Ejemplo 2.3.4.}}
\end{figure}

Ahora suponga que los directivos de la aerolínea han observado que si un cliente tiene que esperar más de 2 minutos, con toda seguridad cuelga la llamada; el 30\% de los clientes que tienen que esperar entre 1 minuto y medio y 2 minutos cuelgan la llamada y ningún cliente cuelga antes del minuto y medio. Entonces podemos estimar el porcentaje de clientes que cuelgan antes de ser atendidos, esto es, clientes potenciales que la aerolínea pierde. Para eso, se debe estimar el porcentaje de llamadas que necesitan más de 2 minutos para ser atendidas, éste se puede expresar como $Pr(X>2)$, donde $X$ denota el tiempo de espera de una llamada y $X\sim Exp(\theta)$. Entonces debe estimar $Pr(X>2)=e^{-2/\theta}$, la cual es una función de $\theta$, y como ya se ha encontrado una estimación de $\theta$ dada por $\hat{\theta}_{MV}=0.8$, podemos simplemente estimar $Pr(X>2)$ como $e^{-2/0.8}=0.08$, esto es, se estima que el 8\% de llamadas necesitan más de 2 minutos para ser atendidas, y por consiguiente este 8\% de clientes cuelga antes de ser atendido. Ahora, para estimar el porcentaje de llamadas que necesitan entre 1 minuto y medio y 2 minutos para ser atendidas como $e^{-1.5/0.8}-e^{-2/0.8}=0.07$, es decir, 7\% de llamadas requieren entre 1.5 y 2 minutos para ser atendidas, y por con\-si\-guien\-te $7\%\times 0.3=0.021=2.1\%$ de clientes cuelgan la llamada antes de ser atendida. Sumando el 8\% hallado anteriormente, podemos afirmar que se estima que la aerolínea pierde el 10.1\% de los clientes potenciales por no ser atendidos oportunamente. Más adelante, se verá que esta estimación sigue siendo de máxima verosimilitud.
\end{Eje}

Las distribuciones consideradas anteriormente tienen sólo un parámetro desconocido. Para distribuciones que tienen dos parámetros desconocidos, el procedimiento es levemente distinto, como lo ilustra el siguiente ejemplo con la distribución normal.
\begin{Eje}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con distribución $N(\mu,\sigma^2)$\index{Estimador!de máxima verosimilitud!normal}, el estimador de máxima verosimilitud del vector de parámetros $\btheta=(\mu,\sigma^2)'$ es $(\bar{X},S_n^2)'$. Tenemos las siguientes expresiones para la función de verosimilitud:
\begin{align*}
L(\btheta,x_1,\cdots,x_n)&=\frac{1}{\sqrt{\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(x_1-\mu)^2\right\}\cdots\frac{1}{\sqrt{\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(x_n-\mu)^2\right\}\\
                         &=(\pi\sigma^2)^{-n/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right\}
\end{align*}
Para facilitar la maximización de $L$, se calcula $\ln(L)$:
\begin{equation*}
\ln(L)=-\frac{n}{2}\ln(\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2.
\end{equation*}

Ahora para obtener valores de $\mu$ y $\sigma^2$ que maximicen a $\ln(L)$, se procede a resolver las dos siguientes ecuaciones:

\begin{equation}\label{norm1}
\frac{\partial\ln(L)}{\partial\mu}=0\end{equation} y
\begin{equation}\label{norm2}\frac{\partial\ln(L)}{\partial\sigma^2}=0\end{equation}
Resolviendo la ecuación (\ref{norm1}), se obtiene la solución de $\mu=\bar{x}$ y resolviendo (\ref{norm2}), se obtiene la solución de $\sigma^2=\sum(x_i-\mu)^2/n$, donde al reemplazar $\mu=\bar{x}$, se tiene que $\sigma^2=\sum(x_i-\bar{x})^2/n=s^2_n$.

Ahora debemos verificar que las anteriores soluciones halladas efectivamente ma\-xi\-mi\-cen la función $\ln(L)$. Dado que esta función tiene dos argumentos, es necesario hacer uso de la matriz Hessiana. La matriz se calcula de la siguiente manera:
\begin{align}\label{Hesiana_normal}
H(\ln(L))&=\begin{bmatrix}
\dfrac{\partial^2\ln(L)}{\partial\mu^2}&\dfrac{\partial^2\ln(L)}{\partial\mu\partial\sigma^2}\\
\dfrac{\partial^2\ln(L)}{\partial\sigma^2\partial\mu}&\dfrac{\partial^2\ln(L)}{\partial(\sigma^2)^2}
\end{bmatrix}\notag\\
&=\begin{bmatrix}
\dfrac{-n}{\sigma^2}&\dfrac{n\mu-\sum x_i}{\sigma^4}\\
\dfrac{n\mu-\sum x_i}{\sigma^4}&\dfrac{n}{2\sigma^4}-\dfrac{\sum(x_i-\mu)^2}{\sigma^6}
\end{bmatrix}.
\end{align}
Ahora reemplazamos las soluciones halladas $\mu=\bar{x}$ y $\sigma^2=s^2_n$, se tiene que la matriz Hessiana es:
\begin{equation*}
H=\begin{bmatrix}
\dfrac{-n^2}{\sum(x_i-\bar{x})^2}&0\\
0&\dfrac{-n^3}{2(\sum(x_i-\bar{x})^2)^2}
\end{bmatrix}.
\end{equation*}

Obsérvese que la matriz Hessiana es una matriz diagonal con valores negativas en la diagonal, lo cual demuestra que es definida negativa, con eso se concluye que las soluciones halladas efectivamente maximizan la función $\ln(L)$. En conclusión, los estimadores de máxima verosimilitud del vector de parámetros $\btheta=(\mu,\sigma^2)'$ son $(\bar{X},S_n^2)'$.
\end{Eje}

A continuación, se presenta una aplicación del anterior ejemplo.

\begin{Eje}
Suponga que una fábrica de vidrios tiene una línea de producción de láminas de vidrio templado de grosor de 3 cm. Para controlar la calidad de los vidrios producidos por esta línea, se seleccionan 12 láminas para inspección. Estas 12 láminas midieron (en cm)  3.56, 3.36, 2.99, 2.71, 3.31, 3.68, 2.78, 2.95, 2.82, 3.45, 3.42 y 3.15. Estos datos son, aparentemente, continuos, y podemos pensar que una distribución normal puede ser apropiada para los datos. Podemos, en primer lugar, observar la forma del histograma de estos datos presentando la Figura 2.3, donde aparentemente no se observa una forma similar a la función de densidad de una distribución normal.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.45]{histograma_vidrios.eps}
\caption{\textsl{Histograma de los datos del Ejemplo 2.3.6.}}
\end{figure}

Sin embargo, como el número de datos es relativamente pequeño, el histograma puede no reflejar la distribución verdadera de los datos, y por esta razón, usamos la gráfica de QQ plot\index{Gráficas QQ plot!distribución normal} para ver qué tan adecuada es la distribución normal. El comando en R está dado a continuación

\begin{verbatim}
    > vidrio<-c(3.56, 3.36, 2.99, 2.71, 3.31,3.68, 2.78, 2.95,
    2.82, 3.45, 3.42 ,3.15)
    > qqnorm(vidrio,main="QQ plot para distribución normal",xlab=
    "Cuantiles teoricos",ylab="Cuantiles muestrales")
    > qqline(vidrio)
\end{verbatim}

Esta gráfica se muestra en la Figura 2.4, donde podemos ver que una distribución normal parece ser apropiada. Por lo tanto, usando el anterior ejemplo, podemos estimar el grosor promedio de las láminas de esta línea como $\hat{\mu}_{MV}=\bar{x}=3.18\ cm$ y la varianza estimada en este caso es $\hat{\sigma}^2_{MV}=s^2_n=0.097\ cm^2$. Sin embargo, es difícil dar interpretación práctica a la varianza puesto que la unidad de ésta es la unidad de los datos al cuadrado, por esta razón en la práctica se usa con más frencuencia $\sigma$ como la medida de dispersión. En este caso, tenemos que $\hat{\sigma}=\sqrt{0.097\ cm^2}=0.31\ cm$.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.4]{qq_normal_ejemplo.eps}
\caption[\textsl{QQ plot para verificar la distribución de los datos del Ejemplo 2.3.6}]{\textsl{QQ plot para verificar la distribución normal de los datos del Ejemplo 2.3.6.}}
\end{figure}

Otra cantidad interesante que se quiere conocer es $\sigma/\mu$, que puede ser vista como una medida de dispersión teórica que está libre de las unidades de medición y por consiguiente, es útil en la práctica para comparar dos poblaciones. Esta cantidad se puede ver como una función del vector de parámetros $(\mu,\sigma^2)$, razón por la cual puede ser estimada por $\dfrac{\sqrt{S_n^2}}{\bar{X}}$ que en este ejemplo da como resultado 9.7\%.

Ahora, suponga que las láminas de grosor entre 2.8 cm y 3.2 cm son vendidas al mercado, las de grosor menor de 2.8 cm son desechadas y las de grosor mayor de 3.2 son usadas como materia prima para futuras producciones. Usando las estimaciones de $\mu$ y $\sigma$ podemos estimar las proporciones de láminas que serán vendidas, desechadas y usadas como materia prima. Si denotamos el grosor de una lámina como $X$, tenemos que la proporción de láminas que serán vendidas es igual a
\begin{align*}
Pr(2.8<X<3.2)&=Pr\left(\dfrac{2.8-\mu}{\sigma}<\dfrac{X-3.18}{0.31}<\dfrac{3.2-\mu}{\sigma}\right)\\
&=\Phi\left(\dfrac{3.2-\mu}{\sigma}\right)-\Phi\left(\dfrac{2.8-\mu}{\sigma}\right).
\end{align*}
Usando $\hat{\mu}_{MV}=3.18$ y $\hat{\sigma}=0.31$, podemos estimar la proporción de láminas que serán vendidas como $\Phi\left(\dfrac{3.2-3.18}{0.31}\right)-\Phi\left(\dfrac{2.8-3.18}{0.31}\right)$, el cual es igual a 0.42. Es decir, se estima que sólo el 42\% de las láminas producidas serán vendidas.

Análogamente, se puede encontrar que el 11\% serán desechadas y el 47\% serán usadas como materia prima. Es claro que según los datos muestrales y las estimaciones obtenidas de éstos, el uso de esta línea de producción no parece ser muy rentable, puesto que menos de la mitad de las láminas producidas pueden ser vendidas. Para aumentar la proporción de láminas que son aptas para la venta, la fábrica debe mejorar la línea de producción con la ayuda de los expertos para
\begin{itemize}
    \item Disminuir el grosor promedio de las láminas, puesto que en la muestra se observó un promedio de 3.18 cm, y se podrá pensar que el grosor real de las láminas es superior al valor especificado de 3 cm. Suponga que después de una mejora de la línea de producción, el promedio muestral fuera $\bar{x}=3.05$ y $\hat{\sigma}$ se mantiene igual. Se puede ver que en este caso, la proporción estimada de láminas para venta se aumentará a 48\%.
    \item Estabilizar las láminas en término del grosor; de esta forma, la estimación de $\sigma$ será más pequeña y la proporción de láminas para venta se incrementará. Suponga que después de una mejora de la línea de producción, $\hat{\sigma}=0.2\ cm$ y $\hat{\mu}$ se mantiene igual. Se puede ver que la proporción estimada de láminas para venta se aumentará a 51\%.
\end{itemize}
Finalmente, si se puede lograr que $\mu$ sea más cercano a 3 cm y al mismo tiempo disminuir el valor de $\sigma$, la proporción de láminas para venta será aún mayor, y la línea de producción será más rentable.
\end{Eje}

En el anterior ejemplo, el estimador de máxima verosimilitud de $\mu$ es $\bar{X}$, y esto es válido aún cuando la varianza teórica $\sigma^2$ es conocida; por otro lado, cuando $\mu$ es conocido, el estimador de máxima verosimilitud de $\sigma^2$ ya no es $S^2_n$ sino $\sum_{i=1}^n(X_i-\mu)^2/n$, esto es, se mide la dispersión tomando las diferencia entre cada variable con la media teórica $\mu$ (Ejercicio 2.5).

El problema de maximizar una función puede, en algunos casos, resultar complicado, y peor aún, puede no encontrar una solución explícita. Un ejemplo de ello es la distribución gamma cuando ambos parámetros son desconocidos. Para estos casos, es necesario usar métodos numéricos para encontrar el máximo de la función de verosimilitud.

Ahora, para las distribuciones con dos parámetros como normal o gamma, cuando uno de los dos parámetros es fijo conocido, entonces sólo habrá necesidad de estimar el otro parámetro y el procedimiento es similar al presentado anteriormente, y lo ilustramos en el siguiente ejemplo.

\begin{Eje}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con distribución Gamma\index{Estimador!de máxima verosimilitud!Gamma} con parámetro de forma $k$ conocido y parámetro de escala $\theta$ desconocido, se tiene que el estimador de máxima verosimilitud de $\theta$ es $\dfrac{\sum_{i=1}^nX_i}{nk}$. Para la verificación, primero calculamos $\ln(L)$ que es la función que se necesita maximizar:
\begin{align*}
\ln(L)&=\ln\left(\dfrac{\prod x_i^{k-1}e^{-\sum x_i/\theta}}{\theta^{nk}\Gamma(k)^n}\right)\\
      &=(k-1)\sum\ln(x_i)-\sum x_i/\theta-nk\ln\theta-n\ln(\Gamma(k))\\
\end{align*}
cuya primera derivada parcial con respecto a $\theta$ está dada por:
\begin{equation*}
\frac{\partial\ln(L)}{\partial\theta}=\frac{\sum x_i}{\theta^2}-\frac{nk}{\theta},
\end{equation*}

el cual al igualar a 0, se obtiene la solución de $\theta=\dfrac{\sum x_i}{nk}$. Ahora, al calcular la segunda derivada de $\ln(L)$ y evaluar en la anterior solución, se tiene que
\begin{equation*}
\frac{\partial^2\ln(L)}{\partial\theta^2}=\frac{-nk}{\theta^2}<0,
\end{equation*}

con lo cual se concluye que el estimador de máxima verosimilitud de $\theta$ es $\dfrac{\sum X_i}{nk}$.
\end{Eje}

Teniendo en cuenta que la distribución exponencial es un caso particular de la distribución Gamma cuando $k=1$, entonces del anterior ejemplo se puede concluir que el estimador de máxima verosimilitud del parámetro $\theta$ de una distribución exponencial es $\bar{X}=\dfrac{\sum X_i}{n}$, estimador que también se puede obtener maximizando directamente la función de verosimilitud.

Cuando la función de verosimilitud no es función continua o derivable del parámetro $\theta$, el problema de maximizar no se puede llevar a cabo haciendo el uso de la derivada de la manera habitual, los dos siguientes ejemplos ilustran tales situaciones.

\begin{Eje}
\index{Estimador!de máxima verosimilitud!hipergeométrica}En situaciones donde la característica de interés es el tamaño de una población $N$, puede ser, por ejemplo, la cantidad de cierto tipo de animales en un determinado lugar (puede ser un bosque). Una forma de determinar $N$ es, en primer lugar, identificar $R$ de los $N$ individuos ($R<N$); en el caso de los animales, puede ser conveniente capturar $R$ de ellos y marcarlos de alguna forma. Después de eso, se espera que los $R$ individuos se mezclen bien con los otros $N-R$, y se seleccionan aleatoriamente $n$ individuos ($n<N$), y se cuenta cuántos de los $R$ individuos fueron seleccionados. Para ver cómo este procedimiento nos puede ayudar a encontrar el valor de $N$, primero identifiquemos el contexto en términos de las distribuciones probabilísticas.

Sea $X$ la variable aleatoria que denota el número de los $R$ individuos que fueron seleccionados en la muestra de tamaño $n$, entonces podemos ver que $X\sim Hg(n,R,N)$, donde $n$ y $R$ son conocidos, y se quiere encontrar el estimador de máxima verosimilitud de $N$. En este caso la función de verosimilitud es la misma función de densidad de la variable $X$, esto es:
\begin{equation}\label{L_hipergeo}
L(N,x)=\frac{\binom{R}{x}\binom{N-R}{n-x}}{\binom{N}{n}}I_{\{0,1,\cdots,n\}}(x).
\end{equation}

En esta función, el argumento $N$ toma valores discretos y no se puede derivar $L$ con respecto a $N$ para hallar el máximo. La forma de encontrar el valor de $N$ que maximiza a $L$ es encontrar para qué valores de $N$, la función $L$ es creciente y para qué valores de $N$ es decreciente. Para lograr este fin, se despeja el valor de $N$ en $L(N)/L(N-1)>1$, como sigue:
\begin{equation*}
\begin{gathered}
\dfrac{L(N)}{L(N-1)}=\dfrac{\binom{N-R}{n-x}\binom{N-1}{n}}{\binom{N}{n}\binom{N-R-1}{n-x}}>1\\
\dfrac{(N-R)!(N-1)!(N-n)!(N-R-1-n+x)!}{(N-R-1)!N!(N-n-1)!(N-R-n+x)!}>1\\
(N-R)(N-n)>N(N-R-n+x)\\
Rn/x>N.\\
\end{gathered}
\end{equation*}
Análogamente, se obtiene que $L(N)/L(N-1)<1$ cuando $N>Rn/x$. Lo anterior indica que la función $L$ es creciente para valores de $N$ menores que $Rn/x$ y decreciente para valores mayores que $Rn/x$. Pero no podemos afirmar que $Rn/X$ es el estimador de máxima verosimilitud para $L$ puesto que este cociente puede no ser entero, y en este caso la estimación no ubicaría dentro del espacio paramétrico de $N$. Lo que sí se puede afirmar es que el estimador de máxima verosimilitud de $N$ es $[Rn/X]$ o $[Rn/X]+1$ donde $[\cdot]$ es la función parte entera. Shao (2003) afirma que $\hat{N}_{MV}=[Rn/X]$. Esto es verdadero, puesto que hemos encontrado anteriormente que $L(N)<L(N-1)$ si $N>Rn/x$, entonces podemos concluir que como $[Rn/x]+1>Rn/x$, entonces $L([Rn/x]+1)<L([Rn/x])$. Y podemos afirmar que el estimador de máxima verosimilitud de $N$ es $[Rn/X]$.

Otra aplicación interesante de la distribución hipergeométrica es el caso donde se conoce el tamaño poblacional $N$, y se desea estimar el número de individuos que tienen cierta característica basada en una muestra de tamaño $n$; por ejemplo, se conoce que en un estanque hay $N$ peces, y se sabe que una parte de ellos están infectados por un tipo de parásito, y se quiere saber cuántos peces tienen dicho parásito con base en la observación de una muestra de tamaño $n$. En estos casos, estamos interesados en estimar $R$ con $N$ y $n$ conocidos. Un razonamiento análogo al caso de estimar $N$ conduce al siguiente estimador de $R$ \footnote{Para más detalles, consulte \citeasnoun{Zhang1}}.
\begin{equation*}
\hat{R}_{MV}=\begin{cases}
\dfrac{x(N+1)}{n}-1\ \text{ó}\ \dfrac{x(N+1)}{n}\ \ \ \ \ \ \text{si $\dfrac{x(N+1)}{n}$ es entero}\\
[\dfrac{x(N+1)}{n}]\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{si $\dfrac{x(N+1)}{n}$ no es entero}
\end{cases}
\end{equation*}

Retomando el problema de estimar el tamaño de un subgrupo considerado en el Capítulo 1, donde se supone que en una ciudad existen 2396 empresas que pueden clasificar en empresas grandes, medianas o pequeñas según el número de empleados, si en una muestra aleatoria simple sin reemplazos de tamaño 200 se encuentran 28 empresas grandes, para tener una estimación del número total de empresas grandes en la ciudad, calculamos en primer lugar $28*(2396+1)/200=335.58$. Este número no es entero, de donde concluimos que la estimación de máxima verosimilitud del número de empresas grandes en la ciudad es de 335.
\end{Eje}

Finalmente, consideramos las distribuciones donde los valores que toma la variable aleatoria $X$ depende del parámetro $\theta$; por ejemplo, las distribuciones del tipo uniforme. Para este tipo de distribuciones, el procedimiento para encontrar $\hat{\theta}_{MV}$ en general se puede resumir en los siguientes pasos:
\begin{enumerate}[(1)]
\item Calcular la función de verosimilitud, sin omitir las funciones indicadoras, pues éstas dependen de $\theta$.
\item Encontrar el rango de valores de $\theta$ donde la función de verosimilitud no toma el valor 0. Generalmente este rango depende del máximo y/o el mínimo de la muestra: $x_{(n)}$ y $x_{(1)}$.
\item Dentro del rango encontrado en el paso anterior, mediante empleo de derivadas o simplemente observación directa, buscar el valor de $\theta$ que maximice la función de verosimilitud.
\end{enumerate}
Ilustramos el anterior procedimiento en el siguiente ejemplo.

\begin{Eje}
\index{Estimador!de máxima verosimilitud!uniforme}Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una población con distribución uniforme continua sobre el intervalo $[0,\theta]$, se quiere encontrar el estimador de máxima verosimilitud del parámetro $\theta$. En primer lugar, obsérvese que en la función de verosimilitud $L$ existe un término de la función indicadora que depende de $\theta$, puesto que $L(\theta)=\theta^{-n}\prod_{i=1}^nI_{[0,\theta]}(x_i)$.

Tenemos que
\begin{equation*}
L\neq0\Leftrightarrow0\leq x_i\leq\theta\ \text{para todo } i=1,\cdots,n\ \Leftrightarrow \theta\geq x_{(n)}
\end{equation*}
donde $x_{(n)}=\max\{x_1,\cdots,x_n\}$. Entonces se concluye que el rango de valores de $\theta$ para que la función de verosimilitud sea diferente de 0 es $[x_{(n)},\infty)$. Ahora, observe que dentro de este rango, $L=\theta^{-n}$, que es una función decreciente de $\theta$, entonces para valores pequeños de $\theta$, $L$ toma valores grandes. Pero el valor más pequeño que puede tomar $\theta$ dentro del rango $[x_{(n)},\infty)$ es $x_{(n)}$, entonces se concluye que $\hat{\theta}_{MV}=X_{(n)}$.
\end{Eje}

Ahora, retomando situaciones donde la cantidad que se quiere estimar es una función del parámetro, $g(\theta)$, textos como \citeasnoun{Mood} establecen que el estimador de máxima verosimilitud de $g(\theta)$ es simplemente $g(\hat{\theta}_{MV})$ siempre y cuando $g$ es una función uno a uno.

Otra forma de ver esto es mediante la reparametrización de la función $L(\theta)$. Por ejemplo, suponga que se desea estimar $\lambda=e^{-\theta}$ en una muestra proveniente de una distribución $Pois(\theta)$. Podemos escribir la función de verosimilitud en término de $\lambda$ como
\begin{equation*}
L(\lambda)=\dfrac{\lambda^n(-n\ln\lambda)^{\sum_{i=1}^nx_i}}{\prod_{i=1}^nx_i}\prod_{i=1}^nI_{\{0,1,\cdots\}}(x_i),
\end{equation*}

de donde
\begin{equation*}
\frac{\partial\ln L(\lambda)}{\partial\lambda}=\frac{n}{\lambda}+\frac{\sum_{i=1}^nx_i}{\ln\lambda}\frac{1}{\lambda},
\end{equation*}

igualando la anterior expresión a 0 y resolviendo para $\lambda$, se tiene que $\hat{\lambda}_{MV}=e^{-\bar{X}}$. Ahora, recordando que $\hat{\theta}_{MV}=\bar{X}$, lo cual coincide con la conclusión dada anteriormente.

Aunque lo planteado es válido para el caso cuando la función $g$ es una función uno a uno, existe el siguiente resultado que establece la invarianza del estimador de máxima verosimilitud\index{Estimador!de máxima verosimilitud!invarianza} para cualquier función $g$.
\begin{Res}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una po\-bla\-ción con distribución $f(x,\btheta)$ donde $\btheta$ es el vector de parámetros, y suponga que $T=T(X_1,\cdots,X_n)$ es el estimador de máxima verosimilitud de $\btheta$, y $g$ es una función del vector de parámetros, entonces el estimador de máxima verosimilitud de $g(\btheta)$ es la estadística $g(T)$.
\end{Res}
\begin{proof}
\citeasnoun[p. 172 y p. 321]{Casella}.
\end{proof}

Dado el anterior resultado, podemos ver que todas las estimaciones de los Ejemplos 2.3.2, 2.3.3, 2.3.4 y 2.3.6 son estimaciones de máxima verosimilitud.

Otra situación interesante es cuando se dispone de dos muestras aleatorias independientes, esto es, cualquier conjunto de variables de la primera muestra es independiente de cualquier conjunto de variables de la segunda, y el objetivo es estimar los parámetros concernientes a las dos muestras.

Consideramos, en primer lugar, dos muestras provenientes de distribuciones normales.

\index{Estimador!de máxima verosimilitud!normal!dos muestras}Suponga que se tienen dos muestras aleatorias independientes $X_1$, $\ldots$, $X_{n_X}$ y $Y_1$, $\ldots$, $Y_{N_Y}$ provenientes de $N(\mu_X,\sigma^2_{X})$ y $N(\mu_Y,\sigma^2_Y)$, respectivamente. Y se desean estimar algunos de los parámetros $\mu_X$, $\mu_Y$, $\sigma^2_X$ y $\sigma^2_Y$. En primer lugar, calculamos la función de verosimilitud, la cual está dada por
\begin{equation*}
L=(2\pi\sigma^2_X)^{-n_X/2}(2\pi\sigma^2_Y)^{-n_Y/2}\exp\left\{-\frac{1}{2\sigma^2_X}\sum_{i=1}^{n_X}(x_i-\mu_X)^2-\frac{1}{2\sigma^2_Y}\sum_{j=1}^{n_Y}(y_j-\mu_Y)^2\right\}
\end{equation*}

En el caso de que las dos muestras provenientes de la misma distribución, esto es, si $\mu_X=\mu_Y=\mu$ y $\sigma^2_X=\sigma^2_Y=\sigma^2$, entonces el proceso de la estimación de máxima verosimilitud de $\mu$ y $\sigma^2$ se llevan a cabo, simplemente, usando conjuntamente las variables de las dos muestras. Y tenemos que
\begin{equation}\label{MV_mu_comun1}
\hat{\mu}_{MV}=\dfrac{\sum_{i=1}^{n_X}X_i+\sum_{j=1}^{n_Y}Y_j}{n_X+n_Y},
\end{equation}
y
\begin{equation*}
\hat{\sigma}^2_{MV}=\dfrac{\sum_{i=1}^{n_X}(X_i-\hat{\mu}_{MV})^2+\sum_{j=1}^{n_Y}(Y_j-\hat{\mu}_{MV})^2}{n_X+n_Y}.
\end{equation*}
\begin{Eje}
Retomamos el Ejemplo 2.3.6, donde se disponía una muestra de 12 láminas. Ahora suponga que se selecciona una nueva muestra de 10 láminas de la misma línea de producción con grosor 3.56, 3.17, 2.98, 2.95, 3.03, 2.87, 3.58, 3.73, 2.83 y 3.43. Dado que las dos muestras son productos de una misma línea de producción, entonces podemos afirmar que las dos muestras provienen de una misma distribución normal $N(\mu,\sigma^2)$, y podemos estimar el grosor promedio de esta línea como 3.2 cm y la desviación estándar como 0.31 cm.
\end{Eje}

\index{Estimador!de máxima verosimilitud!normal!dos muestras}Otra situación que puede surgir en la práctica es cuando las dos muestras provienen de distribuciones con la misma esperanza, pero diferentes varianzas, esto es, $\mu_X=\mu_Y=\mu$ y $\sigma^2_X\neq\sigma^2_Y$. Supongamos, en primer lugar, que $\sigma^2_X$ y $\sigma^2_Y$ son conocidas, y tenemos que la función de verosimilitud está dada por
\begin{equation*}
L(\mu)=(2\pi\sigma^2_X)^{-n_X/2}(2\pi\sigma^2_Y)^{-n_Y/2}\exp\left\{-\frac{1}{2\sigma^2_X}\sum_{i=1}^{n_X}(x_i-\mu)^2-\frac{1}{2\sigma^2_Y}\sum_{j=1}^{n_Y}(y_j-\mu)^2\right\}
\end{equation*}
de donde
\begin{equation*}
\dfrac{\partial\ln L(\mu)}{\partial\mu}=\frac{\sum_{i=1}^{n_X}(x_i-\mu)}{\sigma^2_X}+\frac{\sum_{j=1}^{n_Y}(y_j-\mu)}{\sigma^2_Y}.
\end{equation*}
Igualando la anterior derivada a cero y despejando $\mu$, se encuentra que la solución está dada por
$\mu=\dfrac{\sigma^2_Yn_X\bar{x}+\sigma^2_Xn_Y\bar{y}}{n_X\sigma^2_Y+n_Y\sigma^2_X}$. Ahora, es claro que
\begin{equation*}
\dfrac{\partial^2\ln L(\mu)}{\partial\mu^2}=-\dfrac{n_X}{\sigma^2_X}-\frac{n_Y}{\sigma^2_Y}<0,
\end{equation*}

y en conclusión, se tiene que
\begin{align}\label{MV_mu_comun}
\hat{\mu}_{MV}&=\dfrac{\sigma^2_Yn_X\bar{X}+\sigma^2_Xn_Y\bar{Y}}{n_X\sigma^2_Y+n_Y\sigma^2_X}\notag\\
&=\dfrac{n_X\bar{X}+n_Y\bar{Y}\frac{\sigma^2_X}{\sigma^2_Y}}{n_X+n_Y\frac{\sigma^2_X}{\sigma^2_Y}}.
\end{align}
Nótese que la anterior expresión se asemeja a un promedio ponderado, donde entre más grande sea la varianza teórica de la segunda población $\sigma^2_Y$, menos peso tienen las variables de la muestra correspondiente. Esto es muy natural, puesto que en una distribución normal, una varianza grande indica que los valores de la distribución tienden a tomar valores muy alejados a la media. Entonces, si $\sigma^2_X/\sigma^2_Y<1$, los valores de las variables $Y_1$, $\cdots$, $Y_{n_Y}$ tienden a estar más lejos de $\mu$ que las variables $X_1$, $\cdots$, $X_{n_X}$, lo cual las hacen menos confiables, y por esta razón se les asigna un peso menor. Cuando $\sigma^2_X=\sigma^2_Y$, la anterior estimación de $\mu$ se reduce a (\ref{MV_mu_comun1}).

\begin{Eje}
Considera la fábrica vidrios del Ejemplo 2.3.6, y suponga que hay, en total, dos líneas de producción de láminas de vidrio templado de 3 cm, y además por ajuste inapropiado de temperatura, la línea A tiene una desviación estándar del 0.6 cm, mucho mayor que la línea B cuya desviación estándar es del 0.3 cm. Si se desea estimar el grosor promedio de las láminas de vidrio del grosor nominal del 3 cm, se debe seleccionar una muestra de las láminas de la línea A, y una muestra de la línea B. Suponga que el grosor de 10 láminas de cada línea corresponde a 3.80, 2.81, 2.98, 2.97, 3.69, 2.77, 3.08, 2.98, 2.37, 3.00 y 2.87, 3.48, 2.65, 3.38, 2.75, 2.99, 2.81, 2.54, 2.84, 2.79, respectivamente, entonces asignando un peso mayor a las observaciones de la línea B según la teoría expuesta anteriormente, se tiene que $\hat{\mu}_{MV}=2.955$ cm.
\end{Eje}

\index{Estimador!de máxima verosimilitud!normal!dos muestras}Finalmente, consideramos el caso donde se supone que las dos varianzas teóricas coinciden, $\sigma^2_X=\sigma^2_Y=\sigma^2$ y $\mu_X\neq\mu_Y$. En este caso, tenemos que los estimadores de máxima verosimilitud de $\mu_X$, $\mu_Y$ y $\sigma^2$ son $\bar{X}$, $\bar{Y}$ y $[\sum_{i=1}^{n_X}(X_i-\bar{X})^2+\sum_{j=1}^{n_Y}(Y_j-\bar{Y})^2]/(n_X+n_Y)$, respectivamente (Ejercicio 2.25), esto es, las observaciones de las dos muestras se utilizan separadamente para estimar las medias teóricas, mientras que la varianza se estima usando las muestras conjuntamente con la misma ponderación. En general, cuando no se puede asumir que $\sigma^2_X=\sigma^2_Y$, los estimadores de máxima verosimilitud de $\mu_X$ y $\mu_Y$ siguen siendo $\bar{X}$ y $\bar{Y}$, respectivamente.

\begin{Eje}
Suponga que se desea comparar dos institutos de capacitación en término de calificación obtenida por sus respectivos alumnos. Las calificaciones (sobre 100 puntos) para 15 alumnos del centro A es: 75, 87, 83, 73, 74, 88, 88, 74, 64, 92, 73, 87, 91, 83 y 84; y para 13 alumnos del centro B es: 64, 85, 72, 64, 74, 93, 70, 79, 79, 75, 66, 83 y 74. Antes de entrar a a analizar los datos, consideremos una distribución que puede ser apropiada para estos datos. Dada la naturaleza del problema, los datos son enteros entre 0 y 100, y por consiguiente debe ser realización de una variable discreta; sin embargo, a veces, una distribución continua también puede ser apropiada para describir datos discretos. En la Figura 2.5, se muestran las gráficas QQ plot de la distribución normal para estos dos conjuntos de datos, donde podemos ver que la distribución normal puede ser apropiada para estos datos.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{Ejemplo_normal_2_muestra.eps}
\caption{\textsl{Gráficas de QQ plot para los datos del Ejemplo 2.3.12.}}
\end{figure}

Ahora, dado que el objetivo es comparar los dos institutos, no se puede asumir la igualdad entre las dos medias teóricas; de esta forma, las dos medias teóricas se estiman mediante las medias muestrales dadas por $\hat{\mu}_A=81$ y $\hat{\mu}_B=75$. Observamos que la estimación para el media teórica del centro A es superior a la del centro B. En los capítulos 3 y 4 se estudiarán herramientas que nos permiten concluir si esta diferencia es significativa o puede considerarse como insignificante, dado que las estimaciones no se pueden tratar como si fueran los valores verdaderos de los parámetros.
\end{Eje}

En algunas situaciones, se desea comparar dos poblaciones en término de la dispersión tal como lo muestra el siguiente ejemplo.

\begin{Eje}
Suponga que se desea estudiar el precio por metro cuadrado de viviendas en el centro de la capital de los países Colombia y Ecuador. La información disponible para el caso de Colombia basada en 80 viviendas es: $\bar{x}=700$ (miles de pesos colombianos) y $s_{x,n}=95$ (miles de pesos colombianos); y para el caso ecuatoriano basado en 50 viviendas es: $\bar{y}=1023$ (bolívares venezolanos) y $s_{y,m}=300$ (bolívares venezolanos).

Dados los anteriores datos, no se puede comparar la dispersión de los dos países usando directamente las desviaciones estándares, puesto que éstas tienen unidades diferentes. Una alternativa es calcular los respectivos coeficientes de variación dados por $\rho_x=95/700=13.57\%$ y $\rho_y=300/1023=29.32\%$. Estos coeficientes de variación están libres de unidad de los datos originales y pueden ser usados directamente para comparar la dispersión. Y podemos concluir que el precio de la vivienda del centro del capital de país vecino es mucho más inestable que en el caso colombiano.
\end{Eje}

\subsection{Método de los momentos\index{Método de los momentos}}
Otro método común para encontrar estimadores de un parámetro es el método de los momentos. Para estudiar este método, primero introducimos algunas definiciones útiles.
\begin{Defi}
Dada una variable aleatoria $X$, se define el $k$-ésimo momento\index{Momento} de $X$ como $\mu_k=E(X^k)$.
\end{Defi}

La anterior definición es al nivel poblacional. Cuando se dispone de una muestra aleatoria, se definen los momentos muestrales como sigue.
\begin{Defi}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$, se define el $k$-ésimo momento muestral\index{Momento muestral} como $M_k=\sum_{i=1}^nX_i^k/n$.
\end{Defi}
Nótese que dada una muestra aleatoria, los momentos muestrales son variables aleatorias; más aun, son estadísticas. Y podemos utilizarlos para estimar los respectivos momentos teóricos. Ahora, si logramos escribir a los parámetros desconocidos en términos de los momentos teóricos, podemos obtener fácilmente estimadores de los parámetros simplemente reemplazando los momentos teóricos por los muestrales. Los estimadores obtenidos de esta manera se llaman estimadores de momentos\index{Estimador!de momentos}, se denotará por $\hat{\theta}_{mom}$. En particular, tenemos el siguiente resultado que es válido en muestras provenientes de cualquier distribución.

\begin{Res}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con esperanza común $\mu$ y varianza común $\sigma^2$, se tiene que $\hat{\mu}_{mom}=\bar{X}$ y $\hat{\sigma^2}_{mom}=S^2_n$.\index{Estimador!de momentos!media teórica}\index{Estimador!de momentos!varianza teórica}
\end{Res}
\begin{proof}
En primer lugar, nótese que la esperanza $\mu$ es simplemente el primer momento teórico, es decir, $\mu=\mu_1$, el cual se estima con el primer momento muestral $M_1$, entonces se tiene que $\hat{\mu}_{mom}=M_1=\bar{X}$.

Ahora, la varianza teórica $\sigma^2$ se puede escribir en términos de los dos primeros momentos teóricos, $\sigma^2=\mu_2-(\mu_1)^2$, que se estimará con $M_2-(M_1)^2$, esto es: $\frac{1}{n}\sum_{i=1}^nX_i^2-\bar{X}^2$, y con un poco de operación algebraica, se puede ver que ésta es $S^2_n=\frac{1}{n}\sum_{i=1}^nX_i^2-\bar{X}^2$. En conclusión, $\hat{\sigma^2}_{mom}=S^2_n$.
\end{proof}

Una aplicación inmediata del anterior resultado es en una muestra proveniente de una distribución normal donde los parámetros $\mu$ y $\sigma^2$ corresponden a la esperanza y la varianza de la distribución.
\begin{Eje}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución $N(\mu,\sigma^2)$, el estimador de momentos de $\mu$ y $\sigma^2$ es simplemente la media y la varianza muestral: $\bar{X}$ y $S^2_n=\sum(X_i-\bar{X})^2/n$. Nótese que en este caso, los estimadores de momentos coinciden con los de máxima verosimilitud.\index{Estimador!de momentos!normal}
\end{Eje}

Otra utilidad del Resultado 2.3.2 es la siguiente forma para encontrar un estimador de momentos.
\begin{enumerate}[(a)]
\item Cuando hay que estimar un sólo parámetro $\theta$, escribir a $\theta$ en término de la media teórica $\mu$: $\theta=g(\mu)$, y al estimar $\mu$ con la media muestral: $\bar{X}$, se obtiene un estimador de momentos para $\theta$. Esto es, $\hat{\theta}_{mom}=g(M_1)$.\footnote{Este método es válido siempre y cuando se pueda escribir al parámetro en término de la media teórica. En casos donde esto no es posible, por ejemplo en distribuciones donde no existe la media teórica o ésta no depende del parámetro, se debe recurrir a momentos teóricos superiores.} (también se puede escribir $\theta$ en término de la varianza teórica $\sigma^2$, y al estimar $\sigma^2$ con $S^2_n$, se tiene un estimador de momentos de $\theta$).
\item Cuando hay que estimar dos parámetros $\theta_1$ y $\theta_2$, escribir a cada uno de ellos en término de la media y la varianza teórica $\theta_1=g_1(\mu,\sigma^2)$ y $\theta_2=g_2(\mu,\sigma^2)$, y al estimar $\mu$ y $\sigma^2$ con la media muestral $\bar{X}$ y la varianza muestral $S^2_n$, se obtiene un estimador de momentos para $\theta_1$ y $\theta_2$. Esto es, $\hat{\theta}_{1,mom}=g_1(\bar{X},S^2_n)$ y $\hat{\theta}_{2,mom}=g_2(\bar{X},S^2_n)$.
\end{enumerate}

\textbf{Nota}: la parte (a) nos ilustra que el estimador de momentos al igual que el estimador de máxima verosimilitud, puede no ser único. Un ejemplo es cuando la muestra aleatoria proviene de la distribución $Pois(\lambda)$, se sabe que $\lambda=E(X)$, entonces un estimador de momentos de $\lambda$ es, naturalmente, $\bar{X}$. Pero también se sabe que $\lambda=Var(X)$; de esta manera, se tiene otro estimador de $\lambda$ que es $S^2_n$.\index{Estimador!de momentos!Poisson}

De esta forma, para los datos del Ejemplo 2.3.2, podemos tener dos estimaciones de momentos para el número promedio de muertes violentas por ciudad: $\bar{x}=3.13$, la cual coincide con la estimación de máxima verosimilitud; y la otra estimación de momentos corresponde a $s^2_n=1.98$, que es muy diferente a la estimación obtenida usando $\bar{x}$. La pregunta natural ahora es ¿cuál de las dos estimaciones es mejor?, es decir, ¿cuál valor se acerca más al valor verdadero de $\lambda$?. No podemos responder esta pregunta directamente, puesto que no conocemos el valor de $\lambda$. En la siguiente sección, se introducen conceptos que nos permiten evaluar la calidad de los estimadores. A pesar de que hasta ahora no tenemos herramientas para escoger entre las dos estimaciones, un simple ejercicio de simulación nos permite escoger de forma empírica. Se simula, en primer lugar, muestras provenientes de una distribución $Pois(5)$ y $Pois(15)$ con tamaño de muestra $n=5,\cdots,300$, y en cada muestra simulada, se calculan las dos estimaciones de momentos $\bar{x}$ y $s^2_n$, y se observa cuál es más cercano al valor verdadero del parámetro. Los resultados se visualizan en la gráfica superior de la Figura 2.6, donde la línea negra horizontal denota el valor verdadero del parámetro $\lambda$. El comando en R de estas simulaciones es como sigue

\begin{verbatim}
> set.seed(123)
> n<-5:300
> est.mean<-rep(NA,length(n))
> est.var<-rep(NA,length(n))
> for(i in 1:length(n)){
+ x<-rpois(n[i],5)
+ est.mean[i]<-mean(x)
+ est.var[i]<-var(x)*(n[i]-1)/n[i]
+ }
>
> est1.mean<-rep(NA,length(n))
> est1.var<-rep(NA,length(n))
> for(i in 1:length(n)){
+ y<-rpois(n[i],15)
+ est1.mean[i]<-mean(y)
+ est1.var[i]<-var(y)*(n[i]-1)/n[i]
+ }
>
>
> par(mfrow=c(2,1))
> plot(n,est.var,xlab="n",ylab="Estimación",main="Población P(5)",
type="l",col="blue",ylim=c(2,9))
> abline(5,0)
> lines(n,est.mean,col="red")
> legend(200,9.2,c("Media","Varianza"),lty=c(1,1),col=c("red","blue"),
+  box.col=0)
>
> plot(n,est1.var,xlab="n",ylab="Estimación",main="Población P(15)",
type="l",col="blue")
> abline(15,0)
> lines(n,est1.mean,col="red")
> legend(200,45,c("Media","Varianza"),lty=c(1,1),col=c("red","blue"),
+  box.col=0)
\end{verbatim}

Se puede ver claramente, de la Figura 2.6, que la media $\bar{X}$ comparada con $S^2_n$ estima mejor el parámetro de la distribución, puesto que las estimaciones de $\bar{X}$ parecen estar más cercanas del valor de $\lambda$ en ambas gráficas. Y por consiguiente, podemos intuir que para los datos del Ejemplo 2.3.2, la estimación $\bar{x}=3.13$ debe ser más cercana al valor verdadero de $\lambda$. En la siguiente sección, se discutirán métodos formales acerca de escogencia entre estimadores, y se verá que el estimador $\bar{X}$ tiene mejores propiedades que $S^2_n$.
\begin{figure}[!htb]
\centering
\includegraphics[scale=0.7]{estimadores_Poisson.eps}
\caption[\textsl{Media y la varianza muestral como estimador de $\lambda$ en Poisson}]{\textsl{Comparación entre la media y la varianza muestral como estimador de $\lambda$ en muestras provenientes de dos distribuciones Poisson.}}
\end{figure}

En los siguientes ejemplos, se ilustra la forma general de encontrar estimadores de momentos para distribuciones con dos parámetros desconocidos.

\begin{Eje}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución gamma\index{Estimador!de momentos!Gamma} con parámetro de forma $k$ y parámetro de escala $\theta$, entonces la media y la varianza teórica están dadas por $\mu=k\theta$ y $\sigma^2=k\theta^2$, de donde podemos expresar $k$ y $\theta$ en término de $\mu$ y $\sigma^2$ como $k=\frac{\mu^2}{\sigma^2}$ y $\theta=\frac{\sigma^2}{\mu}$. Por lo tanto, los estimadores de momentos serán
\begin{equation}\label{k_gamma}
\hat{k}_{mom}=\dfrac{\bar{X}^2}{S^2_n}
\end{equation}

y
\begin{equation}\label{theta_gamma}
\hat{\theta}_{mom}=\dfrac{S^2_n}{\bar{X}}
\end{equation}
.

Como una aplicación de lo anterior, suponga que un instituto técnico enfrenta problemas financieros y plantea un aumento en el costo de las matrículas. Es claro que ante un aumento del valor de matrícula, los estudiantes que no tengan la capacidad de pago pueden retirarse del instituto, lo que representa una pérdida económica para éste. Por lo anterior es necesario que el instituto conozca el nivel de ingreso de los estudiantes con el fin de decidir sobre el aumento de las matrículas que no cause la deserción estudiantil y a la vez pueda representar una ganancia económica para el instituto.

Con el fin de conocer el nivel de ingreso de los estudiantes, el instituto planeó una encuesta donde 127 estudiantes del instituto suministraron el valor de sus ingresos mensuales. Es usual pensar que una distribución Gamma puede ser apropiada para variables como ingreso puesto que en primer lugar, esta variable toma valores positivos, y en segundo lugar, es altamente no simétrica, ya que para la mayoría de la población la variable ingreso toma valores intermedios o bajos, pero existe una minoría de la población que tiene ingresos bastantes altos. Para observar el comportamiento del ingreso en los 127 datos muestrales, observamos el histograma presentado en la Figura 2.7, donde se puede ver estas características reflejas, y finalmente asumimos la distribución Gamma como la distribución teórica.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{ejemplo_gamma.eps}
\caption{\textsl{Histograma de los datos del Ejemplo 2.3.15.}}
\end{figure}

Para estimar los parámetros de la distribución Gamma, calculamos los valores de las estadísticas $\bar{X}$ y $S_n$, éstas son \$1.220.195 y \$479.670.8, y usando éstas calculamos los parámetros estimados usando (\ref{k_gamma}) y (\ref{theta_gamma}), y tenemos que $\hat{k}=6.96$ y $\hat{\theta}=175094.6$. Para visualizar el ajuste de la distribución $Gamma(6.96,175094.6)$ a los datos, graficamos la función de densidad de esta distribución usando los siguientes códigos en R y la gráfica resultante se muestra en la Figura 2.8 donde podemos ver que la función de densidad se asemeja bastante al histograma de los datos presentado anteriormente.
\begin{verbatim}
> gama.densidad<-function(x){
+ fx<-dgamma(x,shape=k,scale=the)
+ }
>
> hist(x,breaks=20,freq=F)
> curve(gama.densidad(x),add=T)
## donde x contiene los 127 datos muestrales
\end{verbatim}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{ejemplo_gamma1.eps}
\caption[\textsl{Densidad estimada Gamma de los datos del Ejemplo 2.3.15}]{\textsl{Función de densidad de la distribución Gamma estimada de los datos del Ejemplo 2.3.15.}}
\end{figure}

Ahora, suponga que el instituto conoce que con el aumento los estudiantes con ingreso superior a \$2.000.000 no tendrán dificultad para pagar el costo de la nueva matrícula, entonces una estimación del porcentaje de estos estudiantes puede ayudar a los directivos del instituto a prever los efectos del aumento. Como la distribución teórica identificada es la distribución $Gamma(6.96,175094.6)$, podemos calcular este porcentaje como $Pr(X>1500000)$ donde $X\sim Gamma(6.96,175094.6)$. Esta probabilidad se puede calcular usando el comando
\begin{verbatim}
> pgamma(2000000,shape=6.96, scale=175094.6,lower.tail = F)
[1] 0.06111038
\end{verbatim}
Y tenemos que aproximadamente el 6.1\% de estudiantes de este instituto tiene un ingreso mensual superior a \$2.000.000. Dado que este porcentaje es bastante pequeño, los directivos deberían plantear un aumento menos drástico para evitar el efecto de deserción estudiantil.
\end{Eje}

\newpage
\begin{Eje}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución $Beta(a,b)$, entonces la esperanza y la varianza de la distribución teórica están dadas por\index{Estimador!de momentos!Beta}
\begin{equation}\label{mu_beta}
\mu=\frac{a}{a+b}
\end{equation}
y
\begin{equation*}
\sigma^2=\frac{ab}{(a+b+1)(a+b)^2}
\end{equation*}
Para encontrar los estimadores de momentos de $a$ y $b$ se debe escribir a estos en términos de $\mu$ y $\sigma^2$. Para eso tenemos que
\begin{align*}
\sigma^2&=\mu\frac{b}{(a+b-1)(a+b)}\\
&=\mu(1-\mu)\frac{1}{a+b-1}
\end{align*}

De donde
\begin{equation*}
a+b=\frac{\mu(1-\mu)}{\sigma^2}-1
\end{equation*}

Reemplazando lo anterior en (\ref{mu_beta}) tenemos que
\begin{equation*}
a=\mu(a+b)=\frac{\mu^2(1-\mu)}{\sigma^2}-\mu
\end{equation*}
y
\begin{equation*}
b=\frac{\mu(1-\mu)}{\sigma^2}-1-a=(1-\mu)\left(\frac{\mu(1-\mu)}{\sigma^2}-1\right)
\end{equation*}

Y utilizando los principios del método de los momentos de estimar $\mu$ y $\sigma^2$ con $\bar{X}$ y $S^2_n$, tenemos los siguientes estimadores de momentos de $a$ y $b$
\begin{equation*}
\hat{a}_{mom}=\frac{\bar{X}^2(1-\bar{X})}{S^2_n}-\bar{X}
\end{equation*}

y
\begin{equation*}
b=(1-\bar{X})\left(\frac{\bar{X}(1-\bar{X})}{S^2_n}-1\right)
\end{equation*}

Como una aplicación de la distribución Beta, suponga que un almacén de cadena de ropa femenina investiga acerca de las prendas que son devueltas por los clientes después de la venta. En el caso de que se observe un gran porcentaje de devoluciones, los directivos del almacén estudiarán las causas que pueden ser mala calidad de las prendas por parte del proveedor, precios muy altos comparados con productos de la misma categoría o inclusive vendedores muy hábiles pueden disuadir a los clientes aún cuando no quieren realizar la compra y éstos pueden arrepentir posteriormente y efectuar la devolución de la compra.

Con el fin de llevar a cabo la investigación, los directivos disponen de porcentaje de prendas devueltas en un mes para diferentes sucursales. Estos datos son:  0.7\%, 0.14\%, 19.7\%, 0.1\%, 12.4\%, 1.1\%, 0.5\%, 18.9\%, 5.0\%, 0.3\%, 0.6\%, 5.4\%, 6.7\% y 0.9\%. Dado que los datos son porcentajes y los porcentajes siempre están dentro del intervalo $[0,1]$, podemos pensar que una distribución Beta puede ser apropiada para describir a estos datos. Observemos el histograma en la Figura 2.9 de los datos para verificar si esta distribución es adecuada o no.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{ejemplo_beta1.eps}
\caption{\textsl{Histograma de los datos del Ejemplo 2.3.16.}}
\end{figure}

Podemos ver que la gráfica es altamente no simétrica, de donde se descarta la distribución normal para describir a los datos. Sin embargo, podríamos pensar que una distribución exponencial puede ser apropiada dado que el histograma de los datos se asemeja a la función de densidad de esta distribución. Tenemos una herramienta útil para verificar si una distribución exponencial puede ser apropiada para los datos que es la gráfica QQ plot. Esta gráfica para los datos de este ejemplo se muestra en la Figura 2.10 donde se observa que la mayoría de datos están situados por debajo de la recta, lo cual indica que la distribución exponencial no describe bien el comportamiento de los datos, y por consiguiente también está descartada.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{ejemplo_beta2.eps}
\caption[\textsl{QQ plot exponencial para datos del Ejemplo 2.3.16.}]{\textsl{QQ plot para verificar la distribución exponencial para los datos del Ejemplo 2.3.16.}}
\end{figure}

Dado lo anterior, podemos intentar utilizar la distribución Beta para describir este conjunto de datos. Primero calcularemos las estimaciones de los parámetros de la distribución Beta mediante el siguiente código.
\newpage
\begin{verbatim}
> x<-c(0.7, 1.4, 19.7, 0.1, 12.4, 1.1, 0.5, 18.9, 5.0, 0.3,
+   0.6, 5.4, 6.7, 0.9)/100
> n<-length(x)
> va<-var(x)*(n-1)/n
> bar<-mean(x)
> a<-bar^2*(1-bar)/va-bar
> a
[1] 0.5447021
> b<-(1-bar)*(bar*(1-bar)/va-1)
> b
[1] 9.80242
\end{verbatim}

De lo anterior, tenemos las estimaciones de 0.03 y 0.46 para los parámetros de la distribución Beta. Podemos visualizar la forma de la función de densidad Beta con estos parámetros y ver que tenga aspectos similares con el histograma de los datos. Lo anterior se puede llevar a cabo usando los siguientes códigos.
\begin{verbatim}
> hist(x,breaks=20,freq=F)
> curve(dbeta(x,a,b),add=T)
\end{verbatim}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{ejemplo_beta.eps}
\caption[\textsl{Datos y densidad estimada del Ejemplo 2.3.16}]{\textsl{Histograma de los datos (a) y la función de densidad estimada (b) de los datos del Ejemplo 2.3.16.}}
\end{figure}

Y como se puede observar en la Figura 2.11, la función de densidad de la distribución $Beta(0.03,0.46)$ parece ser apropiada para los datos observados. Ahora, si se quiere estimar el porcentaje promedio de prendas devueltas, esto es, la esperanza de la distribución teórica, podemos utilizar simplemente $\bar{x}=0.052=5.2\%$, o equivalentemente la expresión (\ref{mu_beta}) dada por $\hat{a}/(\hat{a}+\hat{b})=0.052=5.2\%$.
\end{Eje}

En los ejemplos anteriores, se vio que en la distribución normal, los estimadores de momentos coinciden con los de máxima verosimilitud, situación que también ocurre en la distribución Poisson, exponencial y Bernoulli\footnote{En general, el estimador de momentos no es único, por ejemplo en la distribución Poisson, el estimador de momentos $\bar{X}$ coincide con el obtenido con el método de máxima verosimilitud, pero puede haber otros estimadores de momentos diferentes que $\bar{X}$} (Ejercicios 2.3 y 2.6). Sin embargo, en muestras provenientes de algunas distribuciones del tipo uniforme, el estimador de momentos puede no coincidir con el estimador de máxima verosimilitud, como lo ilustra el siguiente ejemplo.

\begin{Eje}
\index{Estimador!de momentos!uniforme}Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución uniforme continua sobre $(0,\theta)$. Para encontrar el estimador de momentos de $\theta$, se tiene en cuenta que $\mu=\theta/2$, de donde $\theta=2\mu$, entonces se concluye que $\hat{\theta}_{mom}=2\bar{X}$, el cual es diferente que el estimador de máxima verosimilitud dada por $\hat{\theta}_{MV}=X_{(n)}$. En la siguiente sección, se estudiará cuál de estos dos estimadores es mejor. Sin embargo, podemos realizar un pequeño estudio simulación: se simulan muestras de tamaño 5, $\cdots$, 300 de las distribuciones $Unif(0,3)$ y $Unif(0,5)$, en cada muestra simulada se calculan el estimador de momentos y de máxima verosimilitud. Estas simulaciones se pueden llevar a cabo modificando levemente los códigos en R presentados en la página 90 . Las estimaciones resultantes se observan en la Figura 2.12, donde la línea negra horizontal denota el valor verdadero del parámetro. Podemos ver que con el estimador de máxima verosimilitud siempre se obtuvieron valores más cercanos al parámetro sin importar el tamaño muestral, aunque las estimaciones de máxima verosimilitud parecen estar por debajo del $\theta$ verdadero, situación que no sucede con las estimaciones de momentos. Este hecho se confirmará en la siguiente sección mediante desarrollos teóricos.
\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{estimadores1_uniforme.eps}
\caption[\textsl{El estimador de MV y el de momentos en muestras uniforme}]{\textsl{Comparación entre el estimador de máxima verosimilitud y el de momentos en muestras provenientes de distribuciones $Unif(0,3)$ y $Unif(0,5)$.}}
\end{figure}
\end{Eje}

Análogo al método de máxima verosimilitud, en el método de los momentos también podemos garantizar la invarianza del\index{Estimador!de momentos!invarianza} estimador obtenido para cualquier función del parámetro $g(\theta)$. Suponga que el tiempo de llegada de un bus a cierta estación contada desde las ocho de la mañana sigue una distribución $Unif(0,\theta)$. Con esta distribución, estamos suponiendo que el tiempo de llegada toma valor en cualquier intervalo de longitud fijo de $(0,\theta)$ con la misma probabilidad. Se ha visto en el ejemplo anterior que $\hat{\theta}_{mom}=2\bar{X}$, y si la cantidad que se desea estimar es la probabilidad de que el bus llegue en menos de un minuto, entonces estamos interesados en estimar $p=1/\theta$, y $p$ puede ser escrito en función del primer momento, puesto que $p=1/(2\mu)$. De esta forma, aplicando los principios del método de los momentos, se tiene que $\hat{p}_{mom}=1/(2\bar{X})$.

Para concluir el método de los momentos, damos dos ejemplos interesantes acerca de la distribución uniforme.

\begin{Eje}
\index{Estimador!de momentos!uniforme}Suponga que una muestra aleatoria $X_1$, $\cdots$, $X_n$ proviene de la distribución $U[-\theta,\theta]$, en este caso la esperanza es 0, y no depende del parámetro $\theta$, por lo tanto, no hay forma de escribir a $\theta$ en función de la esperanza. Pero podemos recurrir a la varianza de la distribución uniforme, notando que la varianza es $\theta^2/3$, de donde se tiene que $\theta=\sqrt{3\sigma^2}$ (la solución $\theta=-\sqrt{3\sigma^2}$ claramente no puede ser el parámetro de la distribución, puesto que $\theta$ debe ser positivo), en conclusión un estimador de momentos de $\theta$ es $\sqrt{3S^2_n}$. Por otro lado, se puede ver que el estimador de máxima verosimilitud de $\theta$ está dado por $\max\{-X_{(1)},X_{(n)}\}$ (Ejercicio 2.11).

En la Figura 2.13, se observan resultados de muestras de tamaño 5, $\cdots$, 300 simulados de una distribución $Unif[-3,3]$ y en cada muestra simulada se calcula el estimador de momentos y de máxima verosimilitud. Podemos observar que en muestras pequeñas los resultados obtenidos con el estimador de máxima verosimilitud casi siempre están por debajo del parámetro causando el problema de subestimación. A medida que las muestras se hacen grandes el método de máxima verosimilitud parece funcionar mejor; por otro lado, aunque los valores obtenidos con el método de los momentos no parecen tener el problema de subestimación que sí tiene el de máxima verosimilitud, los valores obtenidos con el método de los momentos son muy dispersos, y hay muestras donde la estimación puede estar realmente lejos del parámetro.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{estimadores_uniforme.eps}
\caption[\textsl{El estimador de MV y el de momentos en muestras uniforme}]{\textsl{Comparación entre el estimador de máxima verosimilitud y el de momentos en muestras provenientes de la distribución $Unif(-3,3)$.}}
\end{figure}
\end{Eje}

En el anterior ejemplo se vio que en algunas ocasiones en el método de los momentos puede no ser útil evaluar el primer momento sino usando el segundo momento (o equivalentemente la varianza de la distribución teórica). En el siguiente ejemplo, ilustramos un caso donde el procedimiento estándar del método de los momentos arroja dos soluciones y se debe tener en cuenta el espacio paramétrico de los parámetros de interés para escoger la solución apropiada.

\begin{Eje}
\index{Estimador!de momentos!uniforme}Suponga que una muestra aleatoria $X_1$, $\cdots$, $X_n$ proviene de la distribución $U[\theta_1,\theta_2]$, donde $\theta_1$ y $\theta_2$ son desconocidos. Para estimar estos parámetros vía el método de los momentos, necesitamos escribirlos en término de la media $\mu$ y la varianza $\sigma^2$. Usando el Resultado 1.1.11, tenemos que
\begin{equation*}
\begin{cases}
\mu=\dfrac{\theta_1+\theta_2}{2}\\
\sigma^2=\dfrac{(\theta_2-\theta_1)^2}{12},
\end{cases}
\end{equation*}
y tenemos dos soluciones para $\theta_1$ y $\theta_2$, estas son $\begin{cases}
\theta_1=\mu-\sqrt{3}\sigma\\
\theta_2=\mu+\sqrt{3}\sigma
\end{cases}$
o $\begin{cases}
\theta_1=\mu+\sqrt{3}\sigma\\
\theta_2=\mu-\sqrt{3}\sigma
\end{cases}.$
Nótese que en la segunda solución $\theta_1>\theta_2$, no cumple con el supuesto de una distribución $U[\theta_1,\theta_2]$, y por consiguiente, usaremos la primera solución, y los estimadores de momentos de $\theta_1$ y $\theta_2$ serán $\bar{X}-\sqrt{3}S_n$ y $\bar{X}+\sqrt{3}S_n$, respectivamente. Se puede ver fácilmente que los estimadores de máxima verosimilitud de $\theta_1$ y $\theta_2$ son $X_{(1)}$ y $X_{(n)}$ (Ejercicio 2.13).
\end{Eje}

\subsection{Método de mínimos cuadrados\index{Método de mínimos cuadrados}}
El método de mínimos cuadrados es un método muy común en la teoría del modelamiento estadístico, aquí se hace una breve introducción utilizando este método para estimar la esperanza de una distribución teórica.

\begin{Defi}
Dado un conjunto de datos $x_1$, $\cdots$, $x_n$, una medida de dispersión es una función que satisface las siguientes propiedades
\begin{enumerate}
\item $D(\mathbf{x}+\mathbf{1}_nc)=D(\mathbf{x})$
\item $D(-\mathbf{x})=D(\mathbf{x})$
\end{enumerate}

donde $\mathbf{x}$ es el vector conformado por los $n$ datos, $\mathbf{1}_n$  es el vector de unos de tamaño $n$ y $c$ es cualquier número real.
\end{Defi}

\begin{Eje}
Las varianzas $S^2_n$, $S^2_{n-1}$ y las respectivas desviaciones estándares son medidas de dispersión. Puesto que en primer lugar, si se traslada a un conjunto de datos agregando una constante $c$ $S^2_n$ y $S^2_{n-1}$ no cambian de valor; en segundo lugar, estas varianzas tampoco cambian de valor si se multiplica por $-1$ a todos los datos.
\end{Eje}

Ahora introducimos el método de mínimos cuadrados para estimar la esperanza de una distribución teórica $\mu$ basándonos en una muestra $X_1$, $\cdots$, $X_n$. Dado que $\mu$ es la media teórica, se espera que los valores de la muestra observada estén cercanos a $\mu$, por lo tanto, podemos proponer estimar $\mu$ como el valor que más se acerque a los datos. Una forma de medir la distancia entre dos puntos es la distancia euclidiana, así podemos estimar $\mu$ como aquel que minimiza la cantidad
\begin{equation*}
Q=\sum_{i=1}^n(x_i-\mu)^2.
\end{equation*}

De lo anterior, es claro el origen del nombre de estimador de mínimos cuadrados\index{Estimador!de mínimos cuadrados}.

\begin{Res}
Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria proveniente de una distribución con media teórica $\mu$, entonces el estimador de mínimos cuadrados de $\mu$ es $\bar{X}$.\index{Estimador!de mínimos cuadrados!media teórica}
\end{Res}

\begin{proof}
Derivando $Q$ con respecto a $\mu$ e igualando a cero, tenemos que
\begin{equation}
\frac{\partial Q}{\partial\mu}=-2\sum_{i=1}^n(x_i-\mu)=0
\end{equation}

el cual conduce a la solución de $\mu=\bar{X}$, de donde tenemos que el estimador de la media teórico bajo cualquier distribución está dado por
\begin{equation*}
\hat{\mu}_{MC}=\bar{X}.
\end{equation*}
\end{proof}

\section{Propiedades de estimadores puntuales}

En la anterior sección, se observó que para estimar un parámetro $\theta$, el método de máxima verosimilitud y el de momentos pueden conducir a estimadores diferentes; más aun, se pueden crear muchos otros tipos de estimadores para $\theta$, pues un estimador es simplemente una estadística que es usada para estimar. Por ejemplo, dada una muestra aleatoria $X_1$, $\cdots$, $X_{20}$ con media teórica $\mu$ desconocido, un estimador razonable para $\mu$ es la media muestral $\bar{X}$; sin embargo, alguien puede querer usar la estadística $\sum X_i$ para estimar $\mu$, otro puede preferir algo como $\exp\{\sum X_i\}$, otra persona puede inventar su propio estimador, entonces ¿cómo se puede escoger el mejor estimador entre un conjunto de estimadores?, ¿qué aspectos y propiedades se deben tener en cuenta para esa escogencia? El objetivo de este capítulo es introducir conceptos que contestan estas preguntas.

\subsection{Error cuadrático medio}

Consideramos la siguiente situación hipotética. Suponga que para estimar un parámetro $\theta$, se disponen de tres estimadores $T_1$, $T_2$ y $T_3$, y además suponga que las respectivas estimaciones en 7 muestras observadas de la población son los valores dados en la Tabla 2.2.
\begin{table}
\centering
\begin{tabular}{cccc}\hline
Muestra&$T_1$&$T_2$&$T_3$\\\hline
1&4.1&5.5&5.1\\
2&4.3&5.6&5.0\\
3&5.6&5.4&4.8\\
4&5.3&5.5&4.9\\
5&4.5&5.4&5.2\\
6&4.7&5.6&5.0\\
7&5.7&5.5&4.9\\\hline
promedio&4.88&5.5&4.99\\
desviación&0.64&0.08&0.13\\\hline
\end{tabular}\caption{\textsl{Valores de tres estimadores en 7 muestras diferentes.}}
\end{table}

Y además suponga que el valor verdadero de $\theta$ es 5, ¿cuál estimador es mejor dadas las anteriores estimaciones? Para responder esta pregunta, observamos lo siguiente con respecto a los tres estimadores:
\begin{itemize}
      \item Los valores que toma $T_1$ en promedio están cerca del 5, pero estos están muy alejados entre sí, es decir, tienen una dispersión grande. Esta dispersión grande es una propiedad indeseada del estimador, pues generalmente en la práctica, tenemos solo una muestra, una dispersión grande entre los valores de $T_1$ implica que hay mayor probabilidad de que $T_1$ tome un valor alejado del parámetro en una muestra.
      \item Los valores que toma $T_2$ están alrededor del 5.5, muy por encima del valor verdadero de $\theta$, 5, esta situación se llama la sobreestimación. Por otro lado, en términos de la dispersión, se observa que los valores están altamente concentrados.
      \item Los valores que toma $T_3$, en primer lugar, están alrededor del 5, además de tener una dispersión pequeña. Lo anterior indica que en todas las muestras, el valor de $T_3$ está cercano del valor de $\theta$. Y podemos concluir que el mejor estimador de los tres es el $T_3$.
\end{itemize}

La anterior situación nos ilustra que un buen estimador $T$ debe tener dos propiedades
\begin{enumerate}
    \item Los valores que toma $T$ en promedio deben ser cercanos al parámetro $\theta$. Teniendo en cuenta que la esperanza de una variable puede ser interpretada como un promedio ponderado de todos los valores que toma la variable, podemos concluir que $T$ debe cumplir con $E(T)=\theta$,
    \item La varianza de $T$ debe ser pequeña.
\end{enumerate}

Ahora damos la siguiente definición que describe la propiedad 1.
\begin{Defi}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución con parámetro desconocido $\theta$, y sea $T$ un estimador de $\theta$, se define el sesgo\index{Sesgo} de $T$ como $B_T=E(T)-\theta$.

Cuando $B_T=0$ o equivalentemente $E(T)=\theta$, se dice que el estimador $T$ es insesgado\index{Estimador!insesgado} para $\theta$. Cuando $B_T>0$ o equivalentemente $E(T)>\theta$, se dice que $T$ sobreestima\index{Estimador!sobreestimación} a $\theta$, es decir, en promedio la estimación obtenida usando $T$ es mayor que $\theta$. Análogamente se dice que $T$ subestima\index{Estimador!subestimación} a $\theta$ cuando $B_T<0$.
\end{Defi}

Dada la anterior definición, en primera instancia, se necesitan estimadores con sesgo pequeño, y si es posible, insesgados. Adicionalmente, se espera que un buen estimador tenga varianza pequeña. De esta forma, si entre dos estimadores $T_1$ y $T_2$, $T_1$ tiene sesgo y varianza ambos menores que $T_2$, podemos concluir fácilmente que $T_1$ es mejor que $T_2$. Pero cuando $T_1$ tiene sesgo menor, pero varianza mayor que $T_2$, no es fácil determinar cuál es mejor. En este caso, podemos usar el siguiente criterio que combina tanto al sesgo como a la varianza de un estimador.

\begin{Defi}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución con parámetro desconocido $\theta$, y sea $T$ un estimador de $\theta$, se define el error cuadrático medio de $T$ como $ECM_T=E(T-\theta)^2$\index{Error cuadrático medio}.
\end{Defi}

Nótese que en la anterior definición, cuando un estimador $T$ es insesgado para $\theta$, se tiene que $ECM_T=E(T-E(T))^2$, esto es, el error cuadrático medio es la varianza del estimador $T$.

Más aun, el criterio del error cuadrático medio combina al sesgo y la varianza, tenemos que
\begin{align*}
ECM_T&=E[T-E(T)+E(T)-\theta]^2\\
     &=E[(T-E(T))^2+2(T-E(T))(E(T)-\theta)+(E(T)-\theta)^2]\\
     &=E[(T-E(T))^2]+2(E(T)-\theta)\underbrace{E[T-E(T)]}_{\text{igual a 0}}+(E(T)-\theta)^2\\
     &=Var(T)+B_T^2
\end{align*}
Entonces un buen estimador debe tener el error cuadrático medio pequeño, y para los estimadores insesgados, se necesita que la varianza sea pequeña.

Ahora, al principio del capítulo, afirmaba que es natural estimar la media teórica $\mu$ con la media muestral $\bar{X}$, ¿qué tan buena es esta idea? El siguiente resultado nos permite examinar el comportamiento de $\bar{X}$ como estimador de $\mu$.

\begin{Res}
Sea una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución con media $\mu$ y varianza $\sigma^2$, entonces
\begin{enumerate}
    \item si se considera a $\bar{X}$ como el estimador de $\mu$, se tiene que $\bar{X}$ es insesgado para $\mu$, es decir, $E(\bar{X})=\mu$ y además $Var(\bar{X})=\sigma^2/n$.\index{Estimador!insesgado!media muestral}
    \item si se considera a $S^2_n$ y $S^2_{n-1}$ como estimadores de $\sigma^2$, se tiene que $S^2_n$ es sesgado para $\sigma^2$ donde el sesgo es $-\dfrac{\sigma^2}{n}$; y $S^2_{n-1}$ es insesgado para $\sigma^2$.
\end{enumerate}

\end{Res}
\begin{proof}
La demostración de la parte 1 es trivial, y se deja como ejercicio. Para ver la parte 2, tenemos que
\begin{align*}
E(S^2_n)&=\dfrac{1}{n}E\left(\sum_{i=1}^n(X_i-\bar{X})^2\right)\\
&=\dfrac{1}{n}E\left(\sum_{i=1}^nX_i^2-n\bar{X}^2\right)\\
&=\dfrac{1}{n}\left\{\sum_{i=1}^n\left[Var(X_i)+(E(X_i))^2\right]-nE(\bar{X})^2\right\}\\
&=\dfrac{1}{n}\left\{n\sigma^2+n\mu^2-n\left[\dfrac{\sigma^2}{n}+\mu^2\right]\right\}\\
&=\dfrac{n-1}{n}\sigma^2,
\end{align*}
de donde se concluye que $S^2_n$ es sesgado para $\sigma^2$ con sesgo $-\dfrac{\sigma^2}{n}$. Con respecto a $S^2_{n-1}$, al observar que \begin{equation*}
S^2_{n-1}=\dfrac{n}{n-1}S^2_n,
\end{equation*}
se tiene que $E(S^2_{n-1})=\sigma^2$ y por consiguiente es insesgado para $\sigma^2$.
\end{proof}

Nótese en primer lugar que en el anterior resultado, no se ha especificado la distribución de probabilidad en la población, entonces podemos aplicarlo para muestras que provienen de cualquier distribución de probabilidad. En particular, en muestras provenientes de la distribución $Exp(\theta)$, $Pois(\theta)$, $Bernoulli(\theta)$, $N(\theta,\sigma^2)$, el estimador de máxima verosimilitud coincide con el de momentos $\bar{X}$. Usando el anterior resultado, podemos concluir que $\bar{X}$ es insesgado para el parámetro $\theta$ en cualquiera de estas cuatro distribuciones.

Por otro lado, observe que $Var(\bar{X})$ es inversamente proporcional al tamaño muestral $n$, es decir, a medida que la muestra crece, las estimaciones son más concentradas alrededor de $\mu$. En la Figura 2.14, se muestra un estudio de simulación donde se simularon muestras de tamaños 1, $\cdots$, 300, provenientes de distribución normal y exponencial, y en cada muestra se calcula el promedio muestral. Se observa que entre más grande sea el valor de $n$, más concentradas están las estimaciones alrededor de la media teórica $\mu=5$.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.6]{graf1.eps}
\caption{\textsl{Relación entre la estimación de $\mu$ y el tamaño muestral $n$.}}
\end{figure}

Otra observación interesante es que en el contexto del resultado anterior, la variable $X_1$, vista como un estimador de $\mu$ también es insesgada, puesto que por definición de $\mu$, se tiene que $E(X_1)=\mu$; la misma conclusión se tiene para $X_i$ para $i=2,\cdots,n$. Es decir, el estimador insesgado para un parámetro puede no ser único \footnote{De hecho, si tomamos cualquier subconjunto de la muestra aleatoria, el promedio muestral de este subconjunto será un estimador insesgados para $\mu$}. Pero la varianza de $X_i$ con $i=1,\cdots,n$ es $\sigma^2$, que es mayor que $Var(\bar{X})$, de donde se concluye que estos no son tan buenos estimadores como $\bar{X}$.

Ahora, revisamos los estimadores $S^2_n$ y $S^2_{n-1}$ como estimadores de $\sigma^2$. Aunque $S^2_n$ resulta ser sesgado para $\sigma^2$ , el sesgo se hace pequeño cuando el tamaño muestral crece; más aun,
\begin{equation*}
\lim_{n\rightarrow\infty}B_{S^2_n}=\lim_{n\rightarrow\infty}-\dfrac{\sigma^2}{n}=0.
\end{equation*}

Estimadores sesgados que cumplen la propiedad $\lim_{n\rightarrow\infty}B_{S^2_n}=0$ son llamados \textsl{asintóticamente insesgados}\index{Estimador!asintóticamente insesgado}. Para ilustrar los estimadores $S^2_n$ y $S^2_{n-1}$ en término de estimación, podemos simular muestras provenientes de una distribución normal de media 5 y desviación estándar 9 con tamaños de muestral $n=2,20,30,50,300,$ y en cada muestra calculamos los dos estimadores. El comando utilizado en R es

\begin{verbatim}
>  set.seed(1)
>  n<-c(2,10,30, 50,100, 300, 1000,5000)
>  var1<-rePr(NA,length(n))
>  var2<-rePr(NA,length(n))
>  for(k in 1:length(n))
+   {
+   data<-rnorm(n[k],5,9)
+   var1[k]<-var(data)
+   var2[k]<-(n[k]-1)*var(data)/n[k] }
>   plot(var1,type="b", col=4,ylim=c(min(var2),130),xlab="Tamaño de
+   muestra", ylab="Estimación de la varianza", xaxt="n")
>   lines(var2,type="b", col=2, pch=2)
>   abline(h=81)
>   axis(1, 1:length(n), n)
>   legend(3,120,c("Insesgado","Sesgado"), col=c(4,2), lty=c(1,1),
+   pch=c(1,2))
\end{verbatim}

Y como resultado, obtenemos la Figura 2.15, donde podemos observar que las estimaciones del estimador sesgado $S^2_n$ siempre son inferiores que los del estimador insesgado $S^2_{n-1}$; en segundo lugar, la diferencia entre los dos estimadores se hace cada vez más pequeña y los valores de ambos estimadores se acercan al parámetro teórico a medida que el tamaño muestral crece. Por otro lado, aunque $S^2_n$ subestima la varianza teórica, en la gráfica podemos observar que en la muestra simulada del tamaño 300, 1000 y 5000, las estimaciones de $S^2_n$ estuvieron por encima de la varianza teórica, esto no es ninguna contradicción con el hecho de que $S^2_n$ subestima a $\sigma^2$, ya que el concepto de subestimación de un estimador indica que promediando todos los valores del estimador, da un valor inferior al parámetro, mas no indica que todas las estimaciones obtenidas son inferiores al parámetro.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{est1.eps}
\caption[\textsl{Estimaciones de $S^2_n$ y $S^2_{n-1}$ en muestras de $N(5,9^2)$}]{\textsl{Ilustración de las estimaciones de $S^2_n$ y $S^2_{n-1}$ como estimadores de $\sigma^2$ en muestras provenientes de $N(5,9^2)$.}}
\end{figure}

Finalmente, de la parte dos del Resultado 2.4.1, también se puede concluir que los estimadores obtenidos mediante el método de máxima verosimilitud o el de momentos pueden ser sesgados, puesto que una muestra proveniente de la distribución normal, se ha visto que cuando $\mu$ es desconocido, $\hat{\sigma^2}_{MV}=\hat{\sigma^2}_{mom}=S^2_n$, y ésta es sesgada para $\sigma^2$. Sin embargo, en la demostración del Resultado 2.4.1, se vio que en algunos casos, una pequeña modificación al estimador de máxima verosimilitud o el de momentos puede corregir el sesgo y obtener un estimador insesgado.

El Resultado 2.4.1 es válido para muestras provenientes de cualquier distribución; sin embargo, cuando la muestra proviene de una distribución normal, existe el siguiente resultado que nos permite ver que $S^2_n$ es sesgado para $\sigma^2$. Lo presentamos, pues es de gran utilidad para la teoría desarrollada en los capítulos siguientes.

\begin{Res}
Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria proveniente de $N(\mu,\sigma^2)$, y sea $Y=\dfrac{\sum_{i=1}^n(X_i-\bar{X})^2}{\sigma^2}$, entonces se tiene que $Y\sim\chi^2_{n-1}$.
\end{Res}
\begin{proof}
En primer lugar, consideramos la variable $\sum_{i=1}^n(X_i-\mu)^2$, tenemos
\begin{align*}
\sum_{i=1}^n(X_i-\mu)^2&=\sum_{i=1}^n(X_i-\bar{X}+\bar{X}-\mu)^2\\
                       &=\sum_{i=1}^n(X_i-\bar{X})^2+2(\bar{X}-\mu)\underbrace{\sum_{i=1}^n(X_i-\bar{X})}_{=0}+n(\bar{X}-\mu)^2\\
                       &=\sum_{i=1}^n(X_i-\bar{X})^2+n(\bar{X}-\mu)^2
\end{align*}

Dividiendo $\sigma^2$ en ambos lados, se tiene que \begin{equation*}
\underbrace{\sum_{i=1}^n\dfrac{(X_i-\mu)^2}{\sigma^2}}_{A}=\underbrace{\sum_{i=1}^n\dfrac{(X_i-\bar{X})^2}{\sigma^2}}_{Y}+\underbrace{\dfrac{n(\bar{X}-\mu)^2}{\sigma^2}}_{B}.
\end{equation*}

Si podemos suponer que las variables $S^2_n$ y $\bar{X}$ son independientes, podemos concluir que $\sum_{i=1}^n\dfrac{(X_i-\bar{X})^2}{\sigma^2}$ y $\dfrac{n(\bar{X}-\mu)^2}{\sigma^2}$ también son independientes. Entonces existe la siguiente relación entre las funciones generadora de momentos de las variables $A$, $Y$ y $B$: $m_A(t)=m_Y(t)m_B(t)$, de donde se obtiene que
\begin{equation}\label{Gene}
m_Y(t)=m_A(t)/m_B(t).
\end{equation}

Ahora, $\dfrac{X_i-\mu}{\sigma}$ son variables con distribución normal estándar para $i=1,\cdots,n$, entonces $\sum_{i=1}^n\dfrac{(X_i-\mu)^2}{\sigma^2}$ tiene distribución $\chi^2_{n}$ con función generadora de momentos $(1-2t)^{-n/2}$. Por el otro lado $\bar{X}\sim N(\mu,\sigma^2/n)$, entonces $\dfrac{\sqrt{n}(\bar{X}-\mu)}{\sigma}\sim N(0,1)$, de donde se tiene que $\dfrac{n(\bar{X}-\mu)^2}{\sigma^2}\sim\chi^2_1$ cuya función generadora de momentos es $(1-2t)^{-1/2}$. Reemplazando lo anterior en (\ref{Gene}), se tiene que $m_Y(t)=(1-2t)^{-(n-1)/2}$, lo cual indica que $Y\sim\chi^2_{n-1}$.
\end{proof}

Para completar la demostración del anterior resultado, es necesitario probar la independencia entre $\bar{X}$ y $S^2_n$. Tenemos el siguiente resultado.
\begin{Res}
Dada $X_1$, $\cdots$, $X_n$ una muestra aleatoria proveniente de $N(\mu,\sigma^2)$, se tiene que $\bar{X}$ y $S^2_n$ son independientes.
\end{Res}
\begin{proof}
La demostración de este resultado es tomada de \citeasnoun{Casella}. Se probará que $\bar{X}$ y $\sum_{i=1}^n(X_i-\bar{X})^2$ son independientes. Tenemos
\begin{align*}
\sum_{i=1}^n(X_i-\bar{X})^2&=(X_1-\bar{X})^2+\sum_{i=2}^n(X_i-\bar{X})^2\\
&=\left[\sum_{i=1}^n(X_i-\bar{X})-\sum_{i=2}^n(X_i-\bar{X})\right]^2+\sum_{i=2}^n(X_i-\bar{X})^2\\
&=\left[\sum_{i=2}^n(X_i-\bar{X})\right]^2+\sum_{i=2}^n(X_i-\bar{X})^2.
\end{align*}
De lo anterior, se observa que $\sum_{i=1}^n(X_i-\bar{X})^2$ puede verse como una función de las variables $X_2-\bar{X}$, $\cdots$, $X_n-\bar{X}$, por lo tanto, basta ver que estas variables son independientes de $\bar{X}$. Sin embargo, las variables $X_1$, $\cdots$, $X_n$ tienen distribución $N(\mu,\sigma^2)$, y la presencia de estos dos parámetros complica un poco los cálculos, por lo que se trabajará con las variables estandarizadas, $Z_1$, $\cdots$, $Z_n$, donde el promedio está dado por
\begin{align*}
\bar{Z}&=\dfrac{1}{n}\sum_{i=1}^nZ_i\\
&=\dfrac{1}{n}\sum_{i=1}^n\dfrac{X_i-\mu}{\sigma}\\
&=\dfrac{1}{n\sigma}\sum_{i=1}^nX_i-\dfrac{\mu}{\sigma}\\
&=\dfrac{\bar{X}}{\sigma}-\dfrac{\mu}{\sigma},
\end{align*}
además $Z_i-\bar{Z}=\dfrac{X_i-\bar{X}}{\sigma}$ para todo $i=2,\cdots,n$. Por lo tanto, para ver que las variables $X_2-\bar{X}$, $\cdots$, $X_n-\bar{X}$ son independientes de $\bar{X}$, basta ver que $Z_2-\bar{Z}$, $\cdots$, $Z_n-\bar{Z}$ son independientes de $\bar{Z}$.

Para esto, utilizamos la función de densidad conjunta de las variables $Z_1$, $\cdots$, $Z_n$ dada por
\begin{equation*}
f(z_1,\cdots,z_n)=(2\pi)^{-n/2}\exp\left\{-\frac{1}{2}\sum_{i=1}^nz_i^2\right\},
\end{equation*}

Ahora, se define la transformación $Y_1=\bar{Z}$, y $Y_i=Z_i-\bar{Z}$ para $i=2,\cdots,n$, con jacobiano igual a $n^{-1}$. Podemos ver que $Z_1=Y_1-\sum_{i=2}^nY_i$ y $Z_i=Y_i+\bar{Y}$ para $i=2,\cdots,n$. Usando el teorema de transformación, se tiene que la función de densidad conjunta de $Y_1$, $\cdots$, $Y_n$ está dada por
\begin{align*}
f(y_1,\cdots,y_n)&=n(2\pi)^{-n/2}\exp\left\{-\dfrac{1}{2}(y_1-\sum_{i=2}^ny_i)^2\right\}\exp\left\{-\dfrac{1}{2}\sum_{i=2}^n(y_i+\bar{y})^2\right\}\\
&=n(2\pi)^{-n/2}\exp\left\{-\dfrac{n}{2}y_1^2\right\}\exp\left\{\sum_{i=2}^ny_i^2+\left[\sum_{i=2}^ny_i\right]^2\right\},
\end{align*}

la cual es producto entre dos funciones, una que depende solo de $y_1$ y la otra de $y_i$ con $i=2,\cdots,n$\footnote{ver el teorema 4.6.11 de \citeasnoun{Casella}}. Entonces podemos concluir que $Y_2$, $\cdots$, $Y_n$ y $Y_1$ son independientes y el resultado queda demostrado.

Existe otra forma de probar esta independencia utilizando el denominado teorema de Basu; sin embargo, no hemos introducido algunos conceptos necesarios para este teorema, por esta razón, será presentado más adelante.
\end{proof}

Usando el Resultado 2.4.2 y propiedades de la distribución $\chi^2$, se tiene que
\begin{equation*}
E\left(\sum_{i=1}^n\dfrac{(X_i-\bar{X})^2}{\sigma^2}\right)=n-1,
\end{equation*}

de donde \begin{equation}\label{varianza}
E(S^2_n)=E\left(\dfrac{\sum_{i=1}^n(X_i-\bar{X})^2}{n}\right)=\dfrac{n-1}{n}\sigma^2.
\end{equation}

Es decir, el estimador $S^2_n$ es sesgado para $\sigma^2$, mientras que $S^2_{n-1}$ es insesgado para $\sigma^2$.

Ahora, recordamos que en muestras aleatorias provenientes de una distribución exponencial, Poisson o normal, el estimador de máxima verosimilitud es igual al estimador de momentos, pero no siempre es así, como ocurre en muestras provenientes de distribuciones uniformes continuas. Considere una muestra proveniente de $Unif[0,\theta]$, el estimador de máxima verosimilitud de $\theta$ es $\hat{\theta}_{MV}=X_{(n)}$ y el de momentos está dado por $\hat{\theta}_{mom}=2\bar{X}$. Para saber cuál de estos dos estimadores es mejor, comparamos los dos estimadores en términos del sesgo y la varianza en el siguiente ejemplo.

\begin{Eje}
Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria proveniente de una distribución uniforme continua sobre $[0,\theta]$, el estimador de máxima verosimilitud de $\theta$ es $\hat{\theta}_{MV}=X_{(n)}$ y el estimador de momentos es $\hat{\theta}_{mom}=2\bar{X}$. Primero revisamos el desempeño de los estimadores en término del sesgo, es decir, calcularemos la esperanza de ambos estimadores. Para calcular $E(X_{(n)})$ es necesario conocer la función de densidad de probabilidad o la función de distribución de $X_{n}$. Para eso, usamos la propiedad (\ref{F_max}), de donde para $x\in[0,\theta]$ tenemos:\index{Máximo de una muestra!función de distribución}
\begin{equation*}
F_{X_{(n)}}(x)=\frac{x^n}{\theta^n}.
\end{equation*}

Dada la función de distribución de $X_{(n)}$, podemos obtener la función de densidad dada por\index{Máximo de una muestra!función de densidad}
\begin{equation*}
f_{X_{(n)}}(x)=\frac{nx^{n-1}}{\theta^n}I_{[0,\theta]}(x).
\end{equation*}

Ahora calculamos $E(X_{(n)})$ como
\begin{equation}\label{uniforme_MV}
E(X_{(n)})=\int_{0}^\theta\frac{nx^{n-1}}{\theta^n}dx=\frac{n}{n+1}\theta.
\end{equation}

Lo anterior concluye que $X_{(n)}$ como estimador de $\theta$, es sesgado. Más aun, subestima a $\theta$, hecho que se había observado en la Figura 2.12. También nótese que en la expresión (\ref{uniforme_MV}), cuando el tamaño de la muestra $n\rightarrow\infty$, el sesgo tiende a cero, esto es, $X_{(n)}$ es un estimador asintóticamente insesgado. Ahora, miramos cómo es el sesgo del estimador de momentos, tenemos
\begin{equation*}
E(2\bar{X})=2E(\bar{X})=2\frac{\theta}{2}=\theta,
\end{equation*}

pues la esperanza de $\bar{X}$ es igual a la esperanza de la distribución (ver Resultado 2.4.1). En conclusión, el estimador $2\bar{X}$ es insesgado para $\theta$. En el término del sesgo, el estimador $2\bar{X}$ es mejor que $X_{(n)}$, aunque cuando $n$ es grande, los dos son muy similares. Ahora miramos cuál es mejor en término de la varianza. Tenemos
\begin{align*}
Var(X_{(n)})&=E(X_{(n)})^2-(EX_{(n)})^2\\
&=\int_{0}^\theta \frac{nx^{n+1}}{\theta^n}dx-\left(\frac{n\theta}{n+1}\right)^2\\
&=\frac{n\theta^2}{n+2}-\frac{n^2\theta^2}{(n+1)^2}\\
&=\frac{n\theta^2}{(n+2)(n+1)^2}.
\end{align*}
Por el otro lado,
\begin{equation*}
Var(2\bar{X})=4Var(\bar{X})=4\frac{\theta^2}{12n}=\frac{\theta^2}{3n}.
\end{equation*}

Algunas operaciones algebraicas indican que $Var(X_{(n)})$ es mucho más pequeña que la de $2\bar{X}$, y este aspecto ventajoso de $X_{(n)}$ puede recompensar con su sesgo que de todas formas es despreciable para valores grandes de $n$. Por lo tanto, se recomienda usar $X_{(n)}$ para estimar $\theta$. Nótese que la anterior observación con respecto a la varianza también es reflejada en la Figura 2.12.
\end{Eje}

Hasta este punto, hemos concluido que en muchas situaciones, se prefiere en primera instancia a los estimadores insesgados (o por lo menos asintóticamente insesgados) y entre ellos, aquel que tiene menor varianza. Una pregunta interesante que surge ahora es si se dispone de una estimador insesgado para una función del parámetro $g(\theta)$, cómo podemos modificarlo para que siga siendo insesgado, pero con varianza menor. Para eso necesitamos el concepto de suficiencia de un estimador.

\subsection{Suficiencia}

El concepto de la suficiencia de un estimador está ligado con la idea de reducción de datos\index{Estimador!suficiente}. Una muestra aleatoria provee información acerca del parámetro desconocido que se desea estimar, pero esta información está contenida en un conjunto de variables aleatorias. Si hay una manera de encontrar una función de estas variables, que contiene la misma cantidad de información para el propósito de estimación, se lograría la reducción de datos. Una variable que logra esta reducción y que además es usada para estimar el parámetro es un estimador suficiente para el parámetro. Siguiendo a esta idea, es natural pensar que toda la información contenida en la muestra $X_1$, $\cdots$, $X_n$ está contenida en el estimador suficiente ($T$), entonces una vez conocido el valor que toma $T$, la muestra ya no provee ninguna información acerca del parámetro.

La definición rigurosa de un estimador suficiente se presenta a continuación.

\begin{Defi}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con función de densidad $f(x_i,\theta)$, y sea $T=T(X_1,\cdots,X_n)$ un estimador de $\theta$, se dice que $T$ es suficiente para $\theta$ si la distribución condicional de $X$ dados valores de $T$ no depende de $\theta$.
\end{Defi}
En algunos textos, establecen que un estimador $T$ es suficiente para $\theta$ si $Pr(X_1=x_1,\cdots,X_n=x_n|T=t)$ no depende de $\theta$, lo cual no es del todo riguroso, puesto que cuando las variables $X_i$ con $i=1,\cdots,n$ son continuas, la anterior probabilidad condicional (cuando está bien definida) siempre es igual a cero, que no depende de $\theta$. Por otra part, también el estimador $T$ como función de las variables de la muestra puede ser continuo, entonces $Pr(T=t)=0$ y no se puede definir la esperanza condicional. Claro que cuando las variables $X_i$ y $T$ son discretas, podemos usar esta definición sin problema e ilustramos la forma de verificar que un estimador sea suficiente en el siguiente ejemplo.

\begin{Eje}
\index{Estimador!suficiente!Poisson}Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria con distribución $Pois(\lambda)$, se ha visto que el estimador de máxima verosimilitud y de momentos de $\lambda$ está dado por $\bar{X}$. Además éste es insesgado para $\lambda$ por el Resultado 2.4.1. Ahora veamos que también es un estimador suficiente para $\lambda$. Como la distribución Poisson es discreta, entonces para verificar la suficiencia de $\bar{X}$ podemos verificar que $Pr(X_1=x_1,\cdots,X_n=x_n|\bar{X}=x)$ no depende de $\lambda$, tenemos:
\begin{align*}
&\ \ \ \ Pr(X_1=x_1,\cdots,X_n=x_n|\bar{X}=x)\\
&=\frac{Pr(X_1=x_1,\cdots,X_n=x_n,\bar{X}=x)}{Pr(\bar{X}=x)}\\
&=\frac{Pr(X_1=x_1,\cdots,X_n=x_n,\sum_{i=1}^nX_i=nx)}{Pr(\sum_{i=1}^nX_i=nx)}\\
&=\frac{Pr(X_1=x_1,\cdots,X_{n-1}=x_{n-1},X_n=nx-\sum_{i=1}^{n-1}x_i)}{Pr(\sum_{i=1}^nX_i=nx)}\\
&=\frac{Pr(X_1=x_1)\cdots Pr(X_{n-1}=x_{n-1})Pr(X_n=nx-\sum_{i=1}^{n-1}x_i)}{Pr(\sum_{i=1}^nX_i=nx)}\\
&=\dfrac{\dfrac{e^{-n\lambda}\lambda^{x_1}\cdots\lambda^{x_{n-1}}\lambda^{nx-\sum_{i=1}^{n-1}x_i}}{x_1!\cdots x_{n-1}!(nx-\sum_{i=1}^{n-1}x_i)!}}{\dfrac{e^{-n\lambda}(n\lambda)^{nx}}{(nx)!}}\\
&=\frac{(nx)!}{n^{nx}x_1!\cdots x_{n-1}!(nx-\sum_{i=1}^{n-1}x_i)!},
\end{align*}
claramente la anterior probabilidad condicional no depende de $\lambda$, de donde se concluye que $\bar{X}$ es suficiente para $\lambda$. Utilizando un razonamiento completamente análogo, se puede ver que $\sum_{i=1}^nX_i$ también es suficiente para $\lambda$.
\end{Eje}

Ahora, como se vio en el anterior ejemplo, utilizar la definición para demostrar que un estimador es suficiente puede resultar un poco tedioso, pues el cómputo de una probabilidad condicional, en general, no es sencillo. El siguiente teorema, conocido como el criterio o el teorema de factorización de Fisher-Neyman\index{Teorema!de factorización de Fisher-Neyman}, es útil para verificar que un estimador es suficiente para el parámetro desconocido.

\begin{Res}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con función de densidad $f(x_i,\theta)$, y sea $T=T(X_1,\cdots,X_n)$ un estimador de $\theta$, entonces $T$ es suficiente para $\theta$, si y solo si, se puede factorizar la función de verosimilitud $L(\theta,x_1,\cdots,x_n)$ como
\begin{equation*}
L(\theta,x_1,\cdots,x_n)=g(t(x_1,\cdots,x_n),\theta)h(x_1,\cdots,x_n)
\end{equation*}
\end{Res}
\begin{proof}
Se hará la prueba para cuando las variables $X_1$, $\cdots$, $X_n$ y $T$ son discretas, la demostración es como sigue:

$(\Leftarrow)$ Primero supongamos que se tiene la factorización
 \begin{equation*}
 L(\theta,x_1,\cdots,x_n)=g(t(x_1,\cdots,x_n),\theta)h(x_1,\cdots,x_n),
y\end{equation*}
 veamos que $T$ es suficiente para $\theta$, es decir, veamos que $Pr(X_1=x_1,\cdots,X_n=x_n|T=t)$ no depende de $\theta$. En primer lugar, si $t\neq T(x_1,\cdots,x_n)$ entonces la probabilidad vale 0 y por consiguiente no depende de $\theta$. Ahora si $t=T(x_1,\cdots,x_n)$, tenemos:
    \begin{align*}
    Pr(X_1=x_1,\cdots,X_n=x_n|T=t)&=\frac{Pr(X_1=x_1,\cdots,X_n=x_n,T=t)}{Pr(T=t)}\\
    &=\dfrac{Pr(X_1=x_1,\cdots,X_n=x_n)}{Pr(T=t)}\\
    &=\dfrac{g(t(x_1,\cdots,x_n),\theta)h(x_1,\cdots,x_n)}{Pr(T=t)}
    \end{align*}

Al definir $A$ como el conjunto de valores de $x_1$, $\cdots$, $x_n$ que son enviados al valor $t$ mediante la variable $T$, tenemos que
    \begin{align*}
    Pr(X_1=x_1,\cdots,X_n=x_n|T=t)&=\dfrac{g(t(x_1,\cdots,x_n),\theta)h(x_1,\cdots,x_n)}{\sum_{A}Pr(X_1=x_1,\cdots,X_n=x_n)}\\
    &=\dfrac{g(t(x_1,\cdots,x_n),\theta)h(x_1,\cdots,x_n)}{\sum_{A}g(t(x_1,\cdots,x_n),\theta)h(x_1,\cdots,x_n)}\\
    &=\dfrac{g(t,\theta)h(x_1,\cdots,x_n)}{\sum_{A}g(t,\theta)h(x_1,\cdots,x_n)}\\
    &=\dfrac{h(x_1,\cdots,x_n)}{\sum_{A}h(x_1,\cdots,x_n)},
    \end{align*}
el cual no depende del valor $\theta$.

$(\Rightarrow)$ Ahora supongamos que $T$ es suficiente para $\theta$, veamos que se tiene la facto\-ri\-zación $L(\theta,x_1,\cdots,x_n)=g(t(x_1,\cdots,x_n),\theta)h(x_1,\cdots,x_n)$ para algunas funciones $g$ y $h$. Tenemos que
    \begin{align*}
    &\ \ \ \ L(\theta,x_1,\cdots,x_n)\\
    &=Pr(X_1=x_1,\cdots,X_n=x_n)\\
    &=Pr(X_1=x_1,\cdots,X_n=x_n,T=t(x_1,\cdots,x_n))\\
    &=\underbrace{Pr(T=t(x_1,\cdots,x_n))}_{g}\underbrace{Pr(X_1=x_1,\cdots,X_n=x_n|T=t(x_1,\cdots,x_n))}_{h},
    \end{align*}

la primera probabilidad no depende de $\theta$ por la suficiencia de $T$, y la segunda probabilidad depende de $t(x_1,\cdots,x_n)$ y de $\theta$, y hemos logrado obtener la factorización de $L(\theta,x_1,\cdots,x_n)$.

La prueba para cuando $X_1$, $\cdots$, $X_n$ y $T$ son continuas es más complicada, el lector puede consultarla en \citeasnoun[p. 20]{Lehmann}.
\end{proof}
\newpage
Ahora, retomamos el Ejemplo 2.4.2. utilizando el criterio de factorización para ilustrar la utilidad del resultado. \index{Estimador!suficiente!Poisson}Tenemos:
\begin{align*}
L(\lambda,x_1,\cdots,x_n)&=\frac{e^{-n\lambda}\lambda^{\sum_{i=1}^nx_i}}{\prod_{i=1}^nx_i!}\prod_{i=1}^nI_{\{0,1,\cdots\}}(x_i)\\
&=\underbrace{e^{-n\lambda}\lambda^{n\bar{x}}}_{g(\bar{x},\lambda)}\underbrace{\frac{\prod_{i=1}^nI_{\{0,1,\cdots\}}(x_i)}{\prod_{i=1}^nx_i!}}_{h(x_1,\cdots,x_n)},
\end{align*}

con la anterior expresión se logra escribir la función de verosimilitud en forma del Resultado 2.4.4., de donde se concluye que $\bar{X}$ es suficiente para $\lambda$. Nótese que la anterior factorización no es única, pues también se tiene que:
\begin{equation*}
L(\lambda,x_1,\cdots,x_n)=\underbrace{e^{-n\lambda}\lambda^{\sum x_i}}_{g(\sum x_i,\lambda)}\underbrace{\frac{\prod_{i=1}^nI_{\{0,1,\cdots\}}(x_i)}{\prod_{i=1}^nx_i!}}_{h(x_1,\cdots,x_n)},
\end{equation*}

de donde se concluye que también $\sum_{i=1}^nX_i$ es suficiente para $\lambda$.

Utilizando este criterio, se puede verificar fácilmente que en muestras provenientes de las distribuciones $Exp(\theta)$, $Bernoulli(\theta)$, $N(\theta,\sigma^2)$ con $\sigma^2$ conocida, las estadísticas $\bar{X}$ y $\sum_{i=1}^nX_i$ son suficientes para $\theta$\index{Estimador!suficiente!Bernoulli}\index{Estimador!suficiente!normal}\index{Estimador!suficiente!exponencial}.

El criterio de factorización de Fisher-Neyman\index{Teorema!de factorización de Fisher-Neyman} presentado en el Resultado 2.4.4. cubre solamente a las distribuciones con un parámetro desconocido, también existe la versión general para distribuciones con más de un parámetro. Dado que en la mayoría de los casos no se trabaja con distribuciones con más de dos parámetros, se presenta únicamente la versión para distribuciones con dos parámetros.

\begin{Res}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con función de densidad $f(x_i,\theta_1,\theta_2)$, y sea $T_1=T_1(X_1,\cdots,X_n)$ y $T_2=T_2(X_1,\cdots,X_n)$ son estimadores de $\theta_1$ y $\theta_2$, entonces $T_1$ y $T_2$ son suficientes para $\theta_1$ y $\theta_2$, si y solo si, se puede factorizar la función de verosimilitud $L(\theta,x_1,\cdots,x_n)$ como
\begin{equation*}
L(\theta,x_1,\cdots,x_n)=g(t_1(x_1,\cdots,x_n),\theta_1,t_2(x_1,\cdots,x_n),\theta_2)h(x_1,\cdots,x_n)
\end{equation*}
\end{Res}

La utilidad del resultado se ilustra en el siguiente ejemplo.
\begin{Eje}
En una distribución beta, la función de densidad de probabilidad está dada por:\index{Estimador!suficiente!Beta}
\begin{equation*}
f(x)=\dfrac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}I_{(0,1)}(x).
\end{equation*}

Dada una muestra aleatoria de tamaño $n$, la función de verosimilitud está dada por
\begin{align*}
L(\alpha,\beta)&=\dfrac{\Gamma(\alpha+\beta)^n}{\Gamma(\alpha)^n\Gamma(\beta)^n}\prod_{i=1}^nx_i^{\alpha-1}\prod_{i=1}^n(1-x_i)^{\beta-1}\prod_{i=1}^nI_{(0,1)}(x_i)\\
&=\underbrace{\dfrac{\Gamma(\alpha+\beta)^n}{\Gamma(\alpha)^n\Gamma(\beta)^n}\left(\prod_{i=1}^nx_i\right)^{\alpha-1}\left(\prod_{i=1}^n(1-x_i)\right)^{\beta-1}}_{g(t_1,t_2,\alpha,\beta)}\underbrace{\prod_{i=1}^nI_{(0,1)}(x_i)}_{h(x_1,\cdots,x_n)}.
\end{align*}
Y podemos concluir que las estadísticas $\prod_{i=1}^nX_i$ y $\prod_{i=1}^n(1-X_i)$ son suficientes para $\alpha$ y $\beta$.
\end{Eje}

Para las distribuciones pertenecientes a la familia exponencial\index{Estimador!suficiente!en familia exponencial}, siempre podemos encontrar estadísticas suficientes para el parámetro, el resultado se da a continuación.
\begin{Res}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución $f(x,\theta)$ perteneciente a la familia exponencial, es decir,
\begin{equation*}
f(x,\theta)=h(x)c(\theta)\exp\{d(\theta)T(x)\},
\end{equation*}

entonces la estadística $\sum_{i=1}^nT(X_i)$ es una estadística suficiente para $\theta$.
\end{Res}

\begin{proof}
El resultado es trivial usando el criterio de factorización de Fisher-Neyman. Por (\ref{exponencial_muestra}), tenemos que la función de verosimilitud de una muestra aleatoria con función de densidad perteneciente a la familia exponencial está dada por
\begin{equation*}
L(\theta,x_1,\cdots,x_n)=c(\theta)^n\left[\prod_{i=1}^nh(x_i)\right]\exp\left\{d(\theta)\sum_{i=1}^nT(x_i)\right\}.
\end{equation*}
Al tomar $\sum_{i=1}^nT(X_i)$ como la estadística $T$ y $c(\theta)^n\exp\left\{d(\theta)\sum_{i=1}^nT(x_i)\right\}$ como la función $g(t(x_1,\cdots,x_n),\theta)$, y el restante como $h(x_1,\cdots,x_n)$, se tiene que $\sum_{i=1}^nT(X_i)$ es suficiente para $\theta$.
\end{proof}


Para ilustrar la utilidad del resultado, consideramos una muestra proveniente de la distribución $Ber(p)$\index{Estimador!suficiente!Bernoulli}, esta distribución pertenece a la familia exponencial, puesto que
\begin{align*}
f(x,p)&=p^x(1-p)^{1-x}I_{\{0,1\}}(x)\\
&=(\frac{p}{1-p})^xI_{\{0,1\}}(x)\\
&=(1-p)I_{\{0,1\}}(x)\exp\left\{x\ln\frac{p}{1-p}\right\},
\end{align*}
entonces $T(x)=x$, y por el anterior resultado, se tiene que $\sum_{i=1}^nT(X_i)=\sum_{i=1}^nX_i$ es una estadística suficiente para $p$. Teniendo en cuenta que una estadística suficiente resume toda la información contenida en una muestra acerca de un parámetro $\theta$, lo anterior nos indica que en un conjunto de observaciones del tipo 0 y 1 provenientes de $Ber(p)$, para el efecto de estimación de $p$, basta con observar la suma de las observaciones, de esta forma podemos reducir un gran volumen de datos en solo un dato, y la estimación obtenida de $p$ no se ve afectada \footnote{Vea el Ejemplo 2.3.3. donde la estimación de $p$ se llevó a cabo usando solamente la suma de las observaciones.}.

Por otro lado, como la presentación de una función de densidad de la familia exponencial no es única, entonces podemos encontrar diferentes estadísticas suficientes para un mismo parámetro. En efecto, la densidad de la distribución $Ber(p)$ también puede escribirse como:
\begin{equation*}
f(x,p)=(1-p)I_{\{0,1\}}(x)\exp\left\{\frac{x}{n}\left[n\ln\frac{p}{1-p}\right]\right\},
\end{equation*}

de esta manera\index{Estimador!suficiente!Bernoulli}, $T(x)=x/n$, así también se probó que $\bar{X}=\sum_{i=1}^nX_i/n$ es una estadística suficiente para $p$.

Ahora, en distribuciones biparamétricas también podemos encontrar fácilmente estadísticas suficientes si éstas pertenecen a la familia exponencial. El siguiente resultado es el análogo al Resultado 2.4.6. para distribuciones biparamétricas\index{Estimador!suficiente!en familia exponencial}.

\begin{Res}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución $f(x_i,\theta_1,\theta_2)$ perteneciente a la familia exponencial biparamétrica de la forma
\begin{equation*}
f(x_i,\theta_1,\theta_2)=c(\btheta)h(x)\exp\{d(\btheta)'T(x)\},
\end{equation*}

donde $\btheta=(\theta_1,\theta_2)$, $d(\btheta)=(d_1(\btheta),d_2(\btheta))'$ y $T(x)=(T_1(x),T_2(x))'$, entonces las estadísticas $\sum_{i=1}^nT_1(X_i)$ y $\sum_{i=1}^nT_2(X_i)$ son estadísticas suficientes para $\theta_1$ y $\theta_2$.
\end{Res}

\begin{proof}
La prueba es análoga al caso para distribuciones uniparamétricas, usando el criterio de factorización de Fisher-Neyman. Tenemos que la función de verosimilitud está dada por
\begin{align*}
&\ \ \ \ \ L(\theta_1,\theta_2,x_1,\cdots,x_n)\\
&=c(\btheta)^n\left\{\prod_{i=1}^nh(x_i)\right\}\exp\left\{d(\btheta)'\sum_{i=1}^nT(x_i)\right\}\\
&=c(\theta_1,\theta_2)^n\left\{\prod_{i=1}^nh(x_i)\right\}\exp\left\{(d_1(\btheta),d_2(\btheta))'\sum_{i=1}^n\begin{pmatrix}T_1(x_i)\\T_2(x_i)\end{pmatrix}\right\}\\
&=\underbrace{\left\{\prod_{i=1}^nh(x_i)\right\}}_{h(x_1,\cdots,x_n)}\underbrace{c(\theta_1,\theta_2)^n\exp\left\{d_1(\btheta)\sum_{i=1}^nT_1(x_i)+d_2(\btheta)\sum_{i=1}^nT_2(x_i)\right\}}_{g(t_1,\theta_1,t_2,\theta_2)},
\end{align*}
de esta forma, tenemos que $\sum_{i=1}^nT_1(X_i)$ y $\sum_{i=1}^nT_2(X_i)$ son suficientes para $\theta_1$ y $\theta_2$.
\end{proof}

Ilustramos la aplicación del resultado en el siguiente ejemplo.
\begin{Eje}
\index{Estimador!suficiente!normal}Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria con distribución $N(\mu,\sigma^2)$, el anterior resultado servirá para encontrar estadísticas suficientes para $\mu$ y $\sigma^2$. La función de densidad pertenece a la familia exponencial biparamétrica pues se puede escribir de la forma
\begin{equation*}
f(x,\mu,\sigma^2)=\exp\left\{\left(\frac{\mu}{\sigma^2},-\frac{1}{2\sigma^2}\right)\begin{pmatrix}
x\\x^2
\end{pmatrix}\right\}\exp\left\{-\frac{\mu^2}{2\sigma^2}\right\}(2\pi\sigma^2)^{-1/2},
\end{equation*}
entonces $T_1(x)=x$ y $T_2(x)=x^2$, entonces el resultado anterior indica que las estadísticas $\sum_{i=1}^nX_i$ y $\sum_{i=1}^nX_i^2$ son suficientes y completas para $\mu$ y $\sigma^2$.
\end{Eje}

Volviendo al tópico de la evaluación de la calidad de un estimador, una inquietud que había surgido al tener en cuenta que un buen estimador debe ser insesgado con varianza pequeña es: ''dado un estimador insesgado, cómo construir otro insesgado con varianza menor''. El siguiente teorema de Rao-Blackwell\index{Teorema!Rao-Blackwell} \footnote{El teorema fue establecido por el estadístico hindú Calyampudi Radhakrishna Rao y por el americano David Blackwell} afirma que al combinar un estimador insesgado con una estadística suficiente, se puede lograr un estimador insesgado con una varianza menor.
\begin{Res}
Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria con función de densidad $f(x_i,\theta)$, si $T_1$ es un estimador insesgado para una función de $\theta$, $g(\theta)$, y $T_2$ es suficiente para $\theta$, entonces el estimador $T=E(T_1|T_2)$ es insesgado para $g(\theta)$ y tiene varianza menor que $T_1$.
\end{Res}
\begin{proof}
En primer lugar $E(T)=E(E(T_1|T_2))=E(T_1)=g(\theta)$. Ahora, en término de varianza, tenemos
\begin{align*}
Var(T_1)&=Var(E(T_1|T_2))+E(Var(T_1|T_2))\\
&=Var(T)+E(Var(T_1|T_2))\\
&\geq Var(T)
\end{align*}
\end{proof}

La importancia de este teorema radica en que para estimar una función de un parámetro desconocido $g(\theta)$ si tenemos un estimador insesgado $T_1$ podemos, con base en este, construir un mejor estimador que $T_1$, siempre y cuando se disponga de un estimador suficiente para $\theta$.

\index{Esperanza condicional}Para un mejor entendimiento del teorema revisamos, en primer lugar, el concepto de la esperanza condicional. Lo más importante que hay que aclarar es que la expresión $E(T_1|T_2)$ en el resultado anterior no es una constante, sino una variable aleatoria. Para ilustrar esto, considere el siguiente ejemplo:
\begin{Eje}
Dadas variables aleatorias $X$ e $Y$ con función de densidad de probabilidad conjunta dada por:
\begin{equation*}
f(x,y)=\begin{cases}
e^{-y}\ \ \ \text{si}\ 0<x<y\\
0\ \ \ \ \ \text{en otro caso}
\end{cases},
\end{equation*}

Para calcular $E(X|Y)$ primero recordamos que ésta es una función con dominio igual al rango de $Y$ y a cada valor $y$ lo envía a la esperanza $E(X|Y=y)$. Entonces dado $y$, para calcular $E(X|Y=y)$, primero se calcula la función de densidad condicional $f_{X|Y}(x|y)=f(x,y)/f_Y(y)$, en nuestro caso,
\begin{equation*}
f_{X|Y}(x|y)=\begin{cases}
y^{-1}\ \ \ \ \text{si}\ 0<x<y\\
0\ \ \ \ \ \ \text{en otro caso}
\end{cases},
\end{equation*}

de donde para un valor particular que toma la variable $Y$, se puede calcular $E(X|Y=y)$ como
\begin{equation*}
E(X|Y=y)=\int_{-\infty}^\infty xf_{X|Y}(x|y)dx,
\end{equation*}

nótese que la anterior esperanza condicional es un número, función de $y$. En nuestro caso, tenemos que $E(X|Y=y)=y/2$. Entonces $E(X|Y)$ envía cada valor $y$ a $y/2$, es decir $E(X|Y)=Y/2$, la cual claramente es una variable aleatoria.
\end{Eje}

En general, calcular una esperanza condicional $E(X|Y)$ puede implicar cálculos tediosos, pero en algunos casos puede ser trivial como lo indica el siguiente resultado.
\begin{Res}
\index{Esperanza condicional}Si $X$ e $Y$ son variables aleatorias, y $X$ puede escribirse como una función de $Y$, entonces $E(X|Y)=X$.
\end{Res}

Aclarado el concepto de la esperanza condicional, volvemos al teorema de Rao Blackwell. En la demostración no se utilizó el hecho de que $T_2$ sea una estadística suficiente, por lo tanto, podemos intuir que al condicionar $T_1$ en cualquier otra estadística, digamos $Q$, también se puede mejorar la calidad del estimador, en el término de que $Var(T_1|S)$ será menor que $Var(T_1)$. Lo anterior es cierto, pero puede suceder que $T_1|S$ dependa del parámetro $\theta$ y deja de ser un estimador (\citeasnoun[Ejemplo 7.3.18, p. 343]{Casella}. Para un ejemplo donde $T_1|S$ depende de $\theta$.). Por esta razón, se necesita que el condicionamiento sea sobre una estadística suficiente para garantizar que la resultante esperanza condicional no dependa del parámetro y pueda ser usada como un estimador.

\subsection{Estimadores UMVUE}

El teorema de Rao Blackwell plantea la posibilidad de un proceso continuo de construcción de estimadores insesgados con varianzas cada vez menores, la inquietud que surge ahora es si podemos construir estimadores de varianza cada vez menor o podemos encontrar un estimador insesgado $T$ de tal forma que ya no existe ningún otro estimador insesgado con varianza menor que $Var(T)$. Si existe alguna cota inferior para la varianza de los estimadores, y se encuentra un estimador insesgado $T$ con varianza igual a esta cota, se podrá concluir que no habrá otro estimador insesgado con varianza más pequeña que ésta, y se podrá afirmar que $T$ es el mejor de todos los estimadores insesgados. Esta cota existe efectivamente y se denomina la cota de Cramer Rao, y no solo es la cota inferior para la varianza de los estimadores insesgados, sino también puede ser cota inferior para la varianza de todos los estimadores. Para estudiar la cota de Cramer Rao, introducimos algunos conceptos preliminares.


\begin{Defi}
Dada $X$ una variable aleatoria con función de densidad $f(x,\theta)$, donde $\theta$ es el parámetro de la distribución, y además existe $\dfrac{\partial}{\partial\theta}\ln{f(x,\theta)}$, entonces se define la información de Fisher\index{Información de Fisher!en una variable} contenida en $X$ acerca de $\theta$ como
\begin{equation*}
I_X(\theta)=E\left\{\left[\frac{\partial}{\partial\theta}\ln{f(X,\theta)}\right]^2\right\}.
\end{equation*}
\end{Defi}

Nótese que en la anterior definición, $f(X,\theta)$ no es la función de densidad $f(x,\theta)$, sino la variable $X$ transformada a través de la función $f$, es decir, $f(X,\theta)$ es una variable aleatoria, y por consiguiente, tiene sentido calcular la esperanza. Ahora, en algunas situaciones existe una definición equivalente que mide esta cantidad de información, y puede resultar más fácil el cálculo.
\begin{Res}
En la anterior definición, si además existe $\dfrac{\partial^2}{\partial\theta^2}\ln{f(x,\theta)}$, entonces se tiene que
\begin{equation*}
I_X(\theta)=-E\left\{\dfrac{\partial^2}{\partial\theta^2}\ln{f(X,\theta)}\right\}.
\end{equation*}

\end{Res}
\begin{proof}
Tenemos:
\begin{align*}
-E\left\{\dfrac{\partial^2}{\partial\theta^2}\ln{f(X,\theta)}\right\}&=-E\left\{\frac{\partial}{\partial\theta}\left[\frac{1}{f(X,\theta)}\frac{\partial f(X,\theta)}{\partial\theta}\right]\right\}\\
&=-E\left\{-\frac{1}{f^2(X,\theta)}\left[\frac{\partial f(X,\theta)}{\partial\theta}\right]^2+\frac{1}{f(X,\theta)}\frac{\partial^2f(X,\theta)}{\partial\theta^2}\right\}\\
&=E\left\{\frac{1}{f^2(X,\theta)}\left[\frac{\partial f(X,\theta)}{\partial\theta}\right]^2\right\}-E\left\{\frac{1}{f(X,\theta)}\frac{\partial^2f(X,\theta)}{\partial\theta^2}\right\},
\end{align*}
La última esperanza vale cero, puesto que si $X$ es una variable continua,
\begin{align*}
E\left\{\frac{1}{f(X,\theta)}\frac{\partial^2f(X,\theta)}{\partial\theta^2}\right\}&=\int_{\mathbb{R}}\frac{1}{f(x,\theta)}\frac{\partial^2f(x,\theta)}{\partial\theta^2}f(x,\theta)dx\\
&=\int_{\mathbb{R}}\frac{\partial^2f(x,\theta)}{\partial\theta^2}dx\\
&=\frac{\partial^2}{\partial\theta^2}\int_{\mathbb{R}}f(x,\theta)dx\\
&=\frac{\partial^2}{\partial\theta^2}(1)=0.
\end{align*}
Y si $X$ es discreta, suponga que los valores que toma son $x_1,x_2\cdots$, entonces,
\begin{align*}
E\left\{\frac{1}{f(X,\theta)}\frac{\partial^2f(X,\theta)}{\partial\theta^2}\right\}&=\sum_{i}\frac{1}{f(x_i,\theta)}\frac{\partial^2f(x_i,\theta)}{\partial\theta^2}Pr(X=x_i)\\
&=\sum_{i}\frac{\partial^2f(x_i,\theta)}{\partial\theta^2}\\
&=\frac{\partial^2}{\partial\theta^2}\sum_{i}f(x_i,\theta)\\
&=\frac{\partial^2}{\partial\theta^2}(1)=0.
\end{align*}

En conclusión,
\begin{equation*}
-E\left\{\dfrac{\partial^2}{\partial\theta^2}\ln{f(X,\theta)}\right\}=E\left\{\frac{1}{f^2(X,\theta)}\left[\frac{\partial f(X,\theta)}{\partial\theta}\right]^2\right\}.
\end{equation*}

Ahora,
\begin{align*}
I_X(\theta)&=E\left\{\left[\frac{\partial}{\partial\theta}\ln{f(X,\theta)}\right]^2\right\}\\
&=E\left\{\left[\frac{1}{f(X,\theta)}\frac{\partial f(X,\theta)}{\partial\theta}\right]^2\right\}\\
&=E\left\{\frac{1}{f^2(X,\theta)}\left[\frac{\partial f(X,\theta)}{\partial\theta}\right]^2\right\}\\
&=-E\left\{\dfrac{\partial^2}{\partial\theta^2}\ln{f(X,\theta)}\right\},
\end{align*}
y así el resultado queda demostrado.
\end{proof}

Las anteriores definiciones introducen la información contenida en una variable; sin embargo, cuando tenemos disponible una muestra aleatoria, es necesario definir la información contenida en una muestra aleatoria acerca de algún parámetro.
\begin{Defi}
Dada $X_1$, $\cdots$, $X_n$ variables aleatorias con función de densidad $f(x_i,\theta)$, donde $\theta$ es el parámetro de la distribución, y además existe $\dfrac{\partial}{\partial\theta}\ln{\prod_{i=1}^nf(x_i,\theta)}$, entonces se define la información de Fisher contenida en la muestra aleatoria\index{Información de Fisher!en una muestra} acerca de $\theta$ como
\begin{equation*}
I_{X_1,\cdots,X_n}(\theta)=E\left\{\left[\frac{\partial}{\partial\theta}\ln{\prod_{i=1}^nf(X_i,\theta)}\right]^2\right\}.
\end{equation*}
\end{Defi}

Recordemos que en una muestra aleatoria, las variables tienen la misma distribución de probabilidad, además son independientes, entonces es natural pensar que cada variable debe aportar la misma cantidad de información, es decir, la información contenida en una muestra de tamaño $n$ debe ser igual a $n$ veces la información contenida en cualquier variable de la muestra. El siguiente resultado confirma esta intuición.
\begin{Res}
Dada $X_1$, $\cdots$, $X_n$ una muestra aleatoria, entonces
\begin{equation*}
I_{X_1,\cdots,X_n}(\theta)=nI_X(\theta),
\end{equation*}

donde $I_X(\theta)=I_{X_i}(\theta)$, con $i=1,\cdots,n$. Es decir, en una muestra aleatoria, cada variable aporta la misma cantidad de información, y la cantidad total de información en la muestra es la suma de la información en cada variable.
\end{Res}
\begin{proof}
\begin{align*}
I_{X_1,\cdots,X_n}(\theta)&=E\left\{\left[\frac{\partial}{\partial\theta}\ln{\prod_{i=1}^nf(X_i,\theta)}\right]^2\right\}\\
                          &=E\left\{\left[\sum_{i=1}^n\frac{\partial}{\partial\theta}\ln{f(X_i,\theta)}\right]^2\right\}\\
                          &=E\left\{\sum_{i=1}^n\left[\frac{\partial}{\partial\theta}\ln{f(X_i,\theta)}\right]^2\right\}+\\
                          &\ \ \ \ \ \ \ \ \ \ \ \ \underbrace{E\left\{\sum_{\substack{i,j=1\\i\neq j}}^n\left[\frac{\partial}{\partial\theta}\ln{f(X_i,\theta)}\frac{\partial}{\partial\theta}\ln{f(X_j,\theta)}\right]\right\}}_{=0,\ \text{por la independencia entre}\ X_i\ \text{y}\ X_j}\\
                          &=\sum_{i=1}^nE\left\{\left[\frac{\partial}{\partial\theta}\ln{f(X_i,\theta)}\right]^2\right\}\\
                          &=\sum_{i=1}^nI_X(\theta)=nI_X(\theta).
\end{align*}
\end{proof}

Ilustramos el cálculo de la información contenida en una muestra en el siguiente ejemplo, y posteriormente presentamos cómo este concepto resulta útil en la definición de la cota de Cramer Rao.

\begin{Eje}
\index{Información de Fisher!normal}Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria proveniente de la distribución $N(\mu,\sigma^2)$, la información contenida en la muestra acerca de $\mu$ es $n/\sigma^2$. Para verificar esta afirmación, calculamos la información acerca de $\mu$ en una variable $X$ con distribución $N(\mu,\sigma^2)$. Tenemos:
\begin{align*}
I_X(\mu)&=-E\left\{\dfrac{\partial^2}{\partial\mu^2}\ln{f(X,\theta)}\right\}\\
        &=-E\left\{\dfrac{\partial^2}{\partial\mu^2}\left[-\frac{1}{2}\ln2\pi\sigma^2-\frac{1}{2\sigma^2}(X-\mu)^2\right]\right\}\\
        &=-E\left\{\frac{\partial}{\partial\mu}\left[\frac{X-\mu}{\sigma^2}\right]\right\}\\
        &=-E\left\{-\frac{1}{\sigma^2}\right\}\\
        &=\frac{1}{\sigma^2}.
\end{align*}
Ahora, usando el Resultado 2.4.11, se tiene que $I_{X_1,\cdots,X_n}(\mu)=n/\sigma^2$.

Nótese que esta información, en primer lugar, depende del tamaño $n$ de manera que entre más grande sea la muestra, hay mayor información acerca de $\mu$; en segundo lugar, entre más pequeña sea la varianza $\sigma^2$, la cantidad de información acerca de $\mu$ también incrementa. Esto es natural, puesto que si $\sigma^2$ es pequeña, los datos de la muestra están muy concentrados alrededor de $\mu$, entonces estos datos aportan más información que otros datos con más dispersión.
\end{Eje}

Para muestras provenientes de otras distribuciones como la binomial, exponencial y Poisson, también se puede hallar la información de Fisher de manera análoga. Ilustramos estos casos a continuación.

\begin{Eje}
\index{Información de Fisher!binomial}Si $X$ es una variable aleatoria con distribución $Bin(n,\theta)$, entonces para calcular la información de $X$ acerca de $\theta$, tenemos que
\begin{equation*}
\ln f(X)=\ln \binom{n}{X} + X\ln\theta+(n-X)\ln(1-\theta)
\end{equation*}

y
\begin{equation*}
\frac{\partial^2 \ln f(X)}{\partial\theta^2}=-\frac{X}{\theta^2}-\frac{n-X}{(1-\theta)^2}
\end{equation*}

Por lo tanto al calcular la esperanza, y por consiguiente la información de Fisher, se tiene que
\begin{equation*}
I_X(\theta)=- E\left[\frac{\partial^2 \ln f(X)}{\partial\theta^2}\right]
=\frac{n\theta}{\theta^2}+\frac{n-n\theta}{(1-\theta)^2}= \frac{n}{\theta(1-\theta)}
\end{equation*}

De donde vemos que al  tener un número mayor de ensayos, también se obtiene mayor información acerca de $\theta$.
\end{Eje}

Ahora consideramos una muestra con distribución Poisson.

\begin{Eje}
\index{Información de Fisher!Poisson}Si $X_1$,$\ldots$,$X_n$ es una muestra aleatoria de variables con distribución $Pois(\theta)$, la información de Fisher contenida en la muestra acerca de $\theta$ está dada por $I(\theta)=n/\theta$ puesto que
\begin{equation*}
\ln f(X_1,\ldots,X_n)=-n\theta+\sum_{i=1}^nX_i\ln\theta-\sum_{i=1}^n\ln(X_i!)
\end{equation*}

y
\begin{equation*}
\frac{\partial^2 \ln f(X_1,\ldots,X_n)}{\partial\theta^2}=-\frac{\sum_{i=1}^nX_i}{\theta^2}
\end{equation*}

Por lo tanto al calcular la esperanza, se tiene que
\begin{equation*}
I_{X_1,\cdots,X_n}(\theta)=- E\left[\frac{\partial^2\ln f(X_1,\ldots,X_n)}{\partial\theta^2}\right]
=\frac{\sum_{i=1}^nE(X_i)}{\theta^2}=\frac{n}{\theta}
\end{equation*}

\end{Eje}


Ahora, cuando la distribución teórica tiene más de un parámetro, entonces la información contenida en una variable acerca del vector de parámetros va a ser una matriz. Presentamos la definición correspondiente a continuación.

\begin{Defi}
Dada una variable aleatoria $X$ con función de densidad $f(x,\btheta)$, la matriz de información\index{Matriz de información} contenida en $X$ acerca de $\btheta$ se define como
\begin{equation*}
I_X(\btheta)=E\left\{\frac{\partial\ln f(X,\btheta)}{\partial\btheta}\left(\frac{\partial\ln f(X,\btheta)}{\partial\btheta}\right)'\right\}
\end{equation*}
\end{Defi}

En la anterior definición, $\btheta$ es un vector columna, y por consiguiente $\frac{\partial\ln f(X,\btheta)}{\partial\btheta}$ también lo es, y podemos ver que $I_X(\btheta)$ es una matriz cuadrada de dimensión $r\times r$, donde $r$ denota el número de parámetros en el vector $\btheta$.

En el caso de que se dispone de una muestra aleatoria, el concepto de información se extiende de forma análoga al caso de uniparamétrica.

\begin{Defi}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con función de densidad $f(x_i,\btheta)$, la matriz de información contenida en la muestra acerca de $\btheta$ se define como
\begin{equation*}
I_{X_1,\cdots,X_n}(\btheta)=E\left\{\frac{\partial\ln \prod_{i=1}^nf(X_i,\btheta)}{\partial\btheta}\left(\frac{\partial\ln \prod_{i=1}^nf(X_i,\btheta)}{\partial\btheta}\right)'\right\}
\end{equation*}
\end{Defi}

Y se deja como ejercicio verificar que en una muestra aleatoria $I_{X_1,\cdots,X_n}(\btheta)=nI_{X}(\btheta)$ donde $X$ tiene la misma distribución que las variables $X_1$, $\cdots$, $X_n$ (Ejercicio 2.17).

\begin{Eje}
\index{Matriz de información!normal}Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con distribución común $N(\mu,\sigma^2)$, vamos a hallar la matriz de información contenida en la muestra acerca del vector de parámetros $(\mu,\sigma^2)$. Tenemos que
\begin{align*}
&\ \ \ \ \ \ \ I_{X_1,\cdots,X_n}(\mu,\sigma^2)\\
&=E\left\{
\begin{pmatrix}
\dfrac{\partial\ln \prod_{i=1}^nf(X_i,\mu,\sigma^2)}{\partial\mu}\\
\dfrac{\partial\ln \prod_{i=1}^nf(X_i,\mu,\sigma^2)}{\partial\sigma^2}
\end{pmatrix}
\begin{pmatrix}
\dfrac{\partial\ln \prod_{i=1}^nf(X_i,\mu,\sigma^2)}{\partial\mu}&
\dfrac{\partial\ln \prod_{i=1}^nf(X_i,\mu,\sigma^2)}{\partial\sigma^2}
\end{pmatrix}
\right\}\\
&=E\left\{
\begin{pmatrix}
\dfrac{\sum_{i=1}^nX_i-n\mu}{\sigma^2}\\
\dfrac{\sum_{i=1}^n(X_i-\mu)^2-n\sigma^2}{2\sigma^4}
\end{pmatrix}
\begin{pmatrix}
\dfrac{\sum_{i=1}^nX_i-n\mu}{\sigma^2}&
\dfrac{\sum_{i=1}^n(X_i-\mu)^2-n\sigma^2}{2\sigma^4}
\end{pmatrix}
\right\}\\
&=E\left\{\begin{pmatrix}
\dfrac{(\sum_{i=1}^nX_i-n\mu)^2}{\sigma^4}&\dfrac{(\sum_{i=1}^nX_i-n\mu)(\sum_{i=1}^n(X_i-\mu)^2-n\sigma^2)}{2\sigma^6}\\
\dfrac{(\sum_{i=1}^nX_i-n\mu)(\sum_{i=1}^n(X_i-\mu)^2-n\sigma^2)}{2\sigma^6}&\dfrac{(\sum_{i=1}^n(X_i-\mu)^2-n\sigma^2)^2}{4\sigma^8}
\end{pmatrix}\right\}
\end{align*}

Donde el primer elemento diagonal de la anterior matriz está dado por
\begin{align*}
E\left\{\dfrac{(\sum_{i=1}^nX_i-n\mu)^2}{\sigma^4}\right\}&=\left[Var\left(\sum_{i=1}^nX_i-n\mu\right)+(E\left(\sum_{i=1}^nX_i-n\mu\right))^2\right]/\sigma^4\\
&=n\sigma^2/\sigma^4=n/\sigma^2.
\end{align*}

El segundo elemento diagonal está dado por
\begin{align}\label{feo}
&\ \ \ \ \ E\left\{\dfrac{(\sum_{i=1}^n(X_i-\mu)^2-n\sigma^2)^2}{4\sigma^8}\right\}\\
&=\frac{1}{4\sigma^8}E\left\{\left[\sum_{i=1}^n(X_i-\mu)^2\right]^2+n^2\sigma^4-2n\sigma^2\sum_{i=1}^n(X_i-\mu)^2\right\}\\
&=\frac{1}{4\sigma^8}\left\{Var(\sum_{i=1}^n(X_i-\mu)^2)+\left[E(\sum_{i=1}^n(X_i-\mu)^2)\right]^2+n^2\sigma^4-2n\sigma^2E\left[\sum_{i=1}^n(X_i-\mu)^2\right]\right\}
\end{align}

Usando el hecho de que
\begin{equation*}
\frac{\sum_{i=1}^n(X_i-\mu)^2}{\sigma^2}\sim\chi^2_n
\end{equation*}

y la esperanza y varianza de la distribución $\chi^2_n$, tenemos que la expresión (\ref{feo}) está dada por
\begin{equation*}
\frac{1}{4\sigma^8}\left\{2n\sigma^4+\left[n\sigma^2\right]^2+n^2\sigma^4-2n\sigma^2n\sigma^2\right\}=\frac{n}{2\sigma^4}.
\end{equation*}

Finalmente, el elemento fuera de la diagonal de la matriz $I_{X_1,\cdots,X_n}(\mu,\sigma^2)$ está dado por
\begin{align*}
&\ \ \ \ \ \ E\left\{\left(\sum_{i=1}^nX_i-n\mu\right)\left(\sum_{i=1}^n(X_i-\mu)^2-n\sigma^2\right)\right\}\\
&=E\left\{\sum_{i=1}^nX_i\left(\sum_{i=1}^n(X_i-\mu)^2-n\sigma^2\right)-n\mu\left(\sum_{i=1}^n(X_i-\mu)^2-n\sigma^2\right)\right\}\\
&=E\left\{\sum_{i=1}^nX_i\sum_{i=1}^n(X_i-\mu)^2\right\}-n\sigma^2E\left(\sum_{i=1}^nX_i\right)-n\mu E\left(\sum_{i=1}^n(X_i-\mu)^2\right)+n^2\mu\sigma^2\\
&=E\left(\sum_{i=1}^nX_i\sum_{i=1}^nX_i^2\right)-2\mu E\left[(\sum_{i=1}^nX_i)^2\right]+n^2\mu^3-n^2\mu\sigma^2-n^2\mu\sigma^2+n^2\mu\sigma^2\\
&=E\left(\sum_{i=1}^nX_i^3+\sum_{i\neq j}X_iX_j^2\right)-2\mu(n\sigma^2+n^2\mu^2)+n^2\mu^3-n^2\mu\sigma^2\\
&=\sum_{i=1}^n\left[3\mu E(X_i^2)-2\mu^3\right]+\sum_{i\neq j}E(X_i)E(X_j^2)-2n\mu\sigma^2-2n^2\mu^3+n^2\mu^3-n^2\mu\sigma^2\\
&=3n\mu(\sigma^2+\mu^2)-2n\mu^3+\mu(\sigma^2+\mu^2)(n^2-n)-2n\mu\sigma^2-2n^2\mu^3+n^2\mu^3-n^2\mu\sigma^2\\
&=0
\end{align*}

De donde obtenemos finalmente la matriz de información $I_{X_1,\cdots,X_n}(\mu,\sigma^2)$ dada por
\begin{equation*}
I_{X_1,\cdots,X_n}(\mu,\sigma^2)=\begin{pmatrix}
\dfrac{n}{\sigma^2}&0\\
0&\dfrac{n}{2\sigma^4}
\end{pmatrix}
\end{equation*}

El hecho de que esta matriz de información sea diagonal nos ilustra que la información total en la muestra con respecto a $\mu$ no tiene relación con la información contenida acerca de $\sigma^2$. Y podemos confirmar este hecho con el Resultado 2.4.3 donde muestra que los estimadores de $\mu$ y $\sigma^2$ son efectivamente independientes.
\end{Eje}

Cuando se introdujo el concepto de una estadística suficiente, su interpretación es que contiene toda la información de la muestra acerca de algún parámetro, esta información se puede entender como la información de Fisher, y el siguiente resultado provee la respectiva sustentación.
\begin{Res}
\index{Información de Fisher!estimador suficiente}Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con función de densidad $f(x_i,\theta)$, y sea $T$ un estimador suficiente para $\theta$, entonces la información contenida en $T$ acerca de $\theta$ es la misma información contenida en la muestra aleatoria acerca de $\theta$
\end{Res}

\begin{proof}
Lo que probaremos es $I_{T}(\theta)=I_{X_1,\cdots,X_n}(\theta)$.
Tenemos, en primer lugar,
\begin{align*}
f(X_1,\cdots,X_n|T)&=\dfrac{f(X_1,\cdots,X_n,T)}{f(T)}\\
&=\dfrac{g(T,\theta)h(X_1,\cdots,X_n)}{f(T)}\ \ \ \text{usando el criterio de factorización},
\end{align*}
de donde tenemos que
\begin{equation*}
f(T)=\dfrac{g(T,\theta)h(X_1,\cdots,X_n)}{f(X_1,\cdots,X_n|T)}.
\end{equation*}

Ahora usando la función de densidad de $T$ calculamos la información contenida en $T$ como
\begin{align*}
I_T(\theta)&=E\left\{\left[\frac{\partial}{\partial\theta}\ln{f(T)}\right]^2\right\}\\
&=E\left\{\left[\frac{\partial}{\partial\theta}\ln{\dfrac{g(T,\theta)h(X_1,\cdots,X_n)}{f(X_1,\cdots,X_n|T)}}\right]^2\right\}\\
&=E\left\{\left[\frac{\partial}{\partial\theta}\left(\ln{g(T,\theta)}+\ln{h(X_1,\cdots,X_n)}-\ln{f(X_1,\cdots,X_n|T)}\right)\right]^2\right\}\\
&=E\left\{\left[\frac{\partial}{\partial\theta}\ln{g(T,\theta)}\right]^2\right\},\\
\end{align*}
pues $h(X_1,\cdots,X_n)$ no depende de $\theta$, y tampoco $f(X_1,\cdots,X_n|T)$ por la definición de suficiencia de $T$.

Ahora, calculamos la información contenida en la muestra acerca de $\theta$, tenemos
\begin{align*}
I_{X_1,\cdots,X_n}(\theta)&=E\left\{\left[\frac{\partial}{\partial\theta}\ln{f(X_1,\cdots,X_n)}\right]^2\right\}\\
&=E\left\{\left[\frac{\partial}{\partial\theta}\ln{g(T,\theta)h(X_1,\cdots,X_n)}\right]^2\right\}\\
&=E\left\{\left[\frac{\partial}{\partial\theta}(\ln{g(T,\theta)}-\ln{h(X_1,\cdots,X_n)})\right]^2\right\}\\
&=E\left\{\left[\frac{\partial}{\partial\theta}\ln{g(T,\theta)}\right]^2\right\}.
\end{align*}
Y podemos concluir que $I_T(\theta)=I_{X_1,\cdots,X_n}(\theta)$ de donde se concluye que la información contenida en $T$ con respecto a $\theta$ es la misma información contenida en la muestra $X_1,\cdots,X_n$.
\end{proof}


Ahora, como se mencionaba anteriormente, el concepto de información de Fisher permite encontrar una cota inferior para la varianza de los estimadores. Este se enuncia en la famosa desigualdad de información\index{Desigualdad de información}, y se presenta a continuación:
\begin{Res}
Dada $X_1$, $\cdots$, $X_n$ variables aleatorias con distribución de probabilidad $f(x_i,\theta)$, y $T$ es un estimador para $g(\theta)$, si
\begin{equation}\label{Regular}
\frac{\partial}{\partial\theta}E(T)=\int\frac{\partial}{\partial\theta}t(x_1,\cdots,x_n)f(x_1,\cdots,x_n)dx_1\cdots dx_n
\end{equation}
y $Var(T)<\infty$, entonces
\begin{equation*}
Var(T)\geq\dfrac{(\frac{\partial}{\partial\theta}E(T))^2}{I_{X_1,\cdots,X_n}(\theta)}.
\end{equation*}

Y $\dfrac{(\frac{\partial}{\partial\theta}E(T))^2}{I_{X_1,\cdots,X_n}(\theta)}$ es llamado la cota de Cramer Rao\index{Cota de Cramer-Rao}.
\end{Res}
\begin{proof}
La demostración del resultado se basa en el hecho de que el coeficiente de correlación entre dos variables es siempre menor o igual a 1. Entonces para las variables $T$ y $\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(X_i,\theta)$, se tiene que
\begin{equation*}
1\geq Corr(T,\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(X_i,\theta))=\frac{Cov(T,\frac{\partial}{\partial\theta}\ln\prod_{i=1}^n f(X_i,\theta))}{\sqrt{Var(T)Var(\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(X_i,\theta))}},
\end{equation*}

el cual es equivalente a
\begin{equation*}
1\geq\frac{Cov(T,\frac{\partial}{\partial\theta}\ln\prod_{i=1}^n f(X_i,\theta))^2}{Var(T)Var(\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(X_i,\theta))}
\end{equation*}

de donde se tiene
\begin{equation}\label{coef}
Var(T)\geq\frac{Cov(T,\frac{\partial}{\partial\theta}\ln\prod_{i=1}^n f(X_i,\theta))^2}{Var(\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(X_i,\theta))}.
\end{equation}
Ahora
\begin{align*}
Var(\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(X_i,\theta))&=E\left\{\left[\frac{\partial}{\partial\theta}\ln\prod_{i=1}^n f(X_i,\theta)\right]^2\right\}-\underbrace{\left(E\left[\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(X_i,\theta)\right]\right)^2}_{=0}\\
&=E\left\{\left[\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(X_i,\theta)\right]^2\right\}\\
&=I_{X_1,\cdots,X_n}(\theta),
\end{align*}
puesto que
\begin{align*}
E\left[\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(X_i,\theta)\right]&=\sum_{i=1}^n\int_{\mathbb{R}}\frac{\partial}{\partial\theta}\ln f(x_i,\theta)f(x_i,\theta)dx_i\\
&=\sum_{i=1}^n\int_{\mathbb{R}}\frac{1}{f(x_i,\theta)}\frac{\partial f(x_i,\theta)}{\partial\theta}f(x_i,\theta)dx_i\\
&=\sum_{i=1}^n\frac{\partial}{\partial\theta}\int_{\mathbb{R}}f(x_i,\theta)dx_i\\
&=\sum_{i=1}^n\frac{\partial}{\partial\theta}(1)=0,
\end{align*}
si las variables $X_1$, $\cdots$, $X_n$ son continuas. Cuando son discretas, se tiene análogamente.

Por otro lado,
\begin{align*}
&\ \ \ Cov\left(T,\frac{\partial}{\partial\theta}\ln\prod_{i=1}^n f(X_i,\theta)\right)\\
&=E\left[T\frac{\partial}{\partial\theta}\ln\prod_{i=1}^n f(X_i,\theta)\right]\hspace{2.7cm}\text{pues $E\left(\frac{\partial}{\partial\theta}\ln\prod_{i=1}^n f(X_i,\theta)\right)=0$}\\
&=\int_{\mathbb{R}^n}t(x_1,\cdots,x_n)\frac{\partial\ln f_{X_1,\cdots,X_n}(x_1,\cdots,x_n)}{\partial\theta}f_{X_1,\cdots,X_n}(x_1,\cdots,x_n)dx_1\cdots dx_n\\
&=\int_{\mathbb{R}^n}t(x_1,\cdots,x_n)\frac{\partial}{\partial\theta}f_{X_1,\cdots,X_n}(x_1,\cdots,x_n)dx_1\cdots dx_n\\
&=\frac{\partial}{\partial\theta}\int_{\mathbb{R}^n}t(x_1,\cdots,x_n)f_{X_1,\cdots,X_n}(x_1,\cdots,x_n)dx_1\cdots dx_n\\
&=\frac{\partial}{\partial\theta}E(T).
\end{align*}
Reemplazando las anteriores expresiones en (\ref{coef}), se tiene el resultado.
\end{proof}

La condición en (\ref{Regular}) es parte de las condiciones denominadas condiciones de regularidad, y se debe garantizar que esta condición se cumple para tener la validez de la desigualdad de Cramer Rao. Para las distribuciones pertenecientes a la familia exponencial, esta condición siempre se tiene; sin embargo, para distribuciones donde el rango de la variable depende del parámetro como la distribución uniforme, la desigualdad de Cramer Rao puede no ser cierta. El lector puede consultar \citeasnoun[Ejemplo 7.3.13, p. 339]{Casella} para ver un ejemplo donde ocurre esta situación.

Ahora, la cota de Cramer Rao dada en el anterior resultado no asume muchas condiciones como la independencia acerca de las variables, y tampoco características especiales del estimador $T$. Sin embargo, la mayor utilidad de la cota de Cramer Rao se da cuando las variables constituyen una muestra aleatoria y el estimador $T$ sea insesgado para $g(\theta)$. En este caso $\frac{\partial}{\partial\theta}E(T)=\frac{\partial}{\partial g(\theta)}\theta=g'(\theta)$ y la desigualdad se convierte en
\begin{equation*}
Var(T)\geq\dfrac{g'(\theta)}{nI_{X}(\theta)}.
\end{equation*}

Y en el caso cuando lo que se desea estimar es simplemente el parámetro de la distribución $\theta$, $g(\theta)=\theta$, y la desigualdad se convierte en\index{Desigualdad de Cramer-Rao!estimador insesgado}
\begin{equation}\label{Cramer}
Var(T)\geq\dfrac{1}{nI_{X}(\theta)}.
\end{equation}

Ahora, si un estimador insesgado $T$ tiene varianza igual a la cota de Cramer Rao, entonces cualquier otro estimador insesgado necesariamente tendrá varianza más grande que $T$, es decir, $T$ será el mejor entre todos los estimadores insesgados, y existe un nombre especial para estos estimadores que se presenta en la siguiente definición.
\begin{Defi}
Dada $X_1$, $\cdots$, $X_n$ una muestra aleatoria con función de densidad $f(x_i,\theta)$, y $T$ un estimador insesgado para $\theta$, si $Var(T)\leq Var(T^*)$ para todo $\theta$ para cualquier otro estimador $T^*$ insesgado de $\theta$, entonces se dice que $T$ es un estimador insesgado de varianza uniformemente mínima, UMVUE\index{Estimador!UMVUE} \footnote{Por su sigla en inglés \textit{Uniformly Minimum Variance Unbiased Estimator}.}.
\end{Defi}
Con respecto a estimadores UMVUE, tenemos varios comentarios.
\begin{itemize}
\item Como lo mencionado anteriormente, cuando un estimador insesgado tiene varianza igual a la cota de Cramer Rao, entonces este es un UMVUE, pero no necesariamente sucede lo contrario. Es decir, un UMVUE no necesariamente tiene varianza igual a la cota de Cramer Rao. Más adelante en el Ejemplo 2.4.12, se mostrará un caso de estos.
\item Una forma para verificar que un estimador sea UMVUE es ver que la varianza es igual a la cota de Cramer Rao, además hay que ver que es un estimador insesgado. En otras palabras, un estimador sesgado que tenga varianza igual a la cota de Cramer Rao no es UMVUE.
\item Un estimador UMVUE tiene la varianza más pequeña entre todos los estimadores insesgados, pero puede existir un estimador sesgado $T^*$ con varianza aún más pequeña. En este caso, se debe escoger entre el UMVUE y $T^*$, pues puede suceder que la ganancia en la varianza sea considerable y que $T^*$ sea asintóticamente insesgado y de esta forma, corregir el sesgo aumentando el tamaño de muestra $n$.
\end{itemize}

Consideremos un ejemplo para ilustrar los estimadores UMVUE. En la sección anterior se mencionaba que en una muestra proveniente de la distribución $Pois(\theta)$, existen dos estimadores de momentos $\bar{X}$ y $S^2_n$. Mediante estudios de simulación, se vio que $\bar{X}$ es mejor que $S^2_n$. La razón teórica es que $\bar{X}$ es UMVUE para $\theta$, lo cual mostramos en el siguiente ejemplo.

\begin{Eje}
\index{Estimador!UMVUE!Poisson}Dada $X_1$, $\cdots$, $X_n$ una muestra aleatoria con distribución $Pois(\theta)$, el estimador $\bar{X}$ como estimador de $\theta$ es UMVUE. En primer lugar, se vio en el Resultado 2.4.1. $\bar{X}$ siempre es insesgado para la media teórica, el cual en la distribución Poisson es igual al parámetro $\theta$. Entonces tenemos que $\bar{X}$ es insesgado para $\theta$. Resta verificar que la varianza de $\bar{X}$ es mínima entre todos los estimadores insesgados. Para eso, podemos calcular la cota de Cramer Rao, y ver que ésta es igual a $Var(\bar{X})$.

Por el Resultado 2.4.1, $Var(\bar{X})=\sigma^2/n$ donde $\sigma^2$ es la varianza de la distribución de probabilidad, que en este ejemplo es la distribución $Pois(\theta)$ cuya varianza es $\theta$. En conclusión
\begin{equation*}
Var(\bar{X})=\frac{\theta}{n}.
\end{equation*}

Ahora calculamos la cota de Cramer Rao, tenemos
\begin{align*}
I_{X_1,\cdots,X_n}(\theta)&=nI_{X}(\theta)\\
&=-nE\left\{\frac{\partial^2}{\partial\theta^2}\ln f(X,\theta)\right\}\\
&=-nE\left\{\frac{\partial^2}{\partial\theta^2}\left[-\theta+X\ln\theta-\ln X!\right]\right\}\\
&=-nE\left\{\frac{\partial}{\partial\theta}\left[-1+\frac{X}{\theta}\right]\right\}\\
&=-nE\left\{-\frac{X}{\theta^2}\right\}\\
&=\frac{n}{\theta},
\end{align*}
de donde se concluye que la cota de Cramer Rao es $\theta/n$, que es igual a la varianza de $\bar{X}$. En conclusión, $\bar{X}$ es UMVUE para $\theta$.

Por otro lado, de nuevo usando el Resultado 2.4.1, el otro estimador de momentos $S^2_n$ es insesgado para la varianza teórica que en una distribución Poisson también corresponde al parámetro $\theta$. Este insesgamiento se puede corroborar con la Figura 2.6 donde muestran las estimaciones obtenidas usando $\bar{X}$ y $S^2_n$ en muestras de distribución Poisson, donde es claro que las estimaciones de $S^2_n$ estuvieron siempre alrededor del valor de $\theta$; sin embargo, como $\bar{X}$ es UMVUE, siempre será mejor que $S^2_n$ como estimador de $\theta$.
\end{Eje}

Como se vio en el anterior ejemplo, para ver que la varianza de un estimador sea igual a la cota de Cramer Rao, se necesita calcular la información de Fisher contenida en la muestra, y esto puede ser tedioso. Sin embargo, existe un resultado que requiere tal vez menos operaciones algebraicas, que nos permite saber cuándo un estimador tiene la varianza igual a la cota de Cramer Rao.
\begin{Res}
Dada $X_1$, $\cdots$, $X_n$ una muestra aleatoria con distribución de probabilidad $f(x_i,\theta)$, y $T=T(X_1,\cdots,X_n)$ un estimador de $g(\theta)$. Si se tiene la siguiente factorización
\begin{equation*}
\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(x_i,\theta)=K(\theta)(t(x_1,\cdots,x_n)-g(\theta)),
\end{equation*}

entonces la varianza de $T$ es igual a la cota de Cramer Rao.
\end{Res}
\begin{proof}
La condición
\begin{equation*}
\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(x_i,\theta)=K(\theta)(t(x_1,\cdots,x_n)-g(\theta))
\end{equation*}

es equivalente a
\begin{equation*}
\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(x_i,\theta)=\alpha t(x_1,\cdots,x_n)+\beta
\end{equation*}

para algunos constantes $\alpha$ y $\beta$ que no depende de la muestra aleatoria, sino únicamente del parámetro $\theta$. Y en consecuencia, tenemos que
\begin{equation*}
\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(X_i,\theta)=\alpha T+\beta.
\end{equation*}

Ahora recordando propiedades del coeficiente de correlación, tenemos que
\begin{equation*}
Corr\left(T,\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(X_i,\theta)\right)=1,
\end{equation*}

y retomando la demostración del Teorema 2.3.12, tenemos que la varianza de $T$ es igual a la cota de Cramer Rao.

\end{proof}
Ahora, volvemos al Ejemplo 2.4.8 para ilustrar la utilidad del anterior resultado. Tenemos:
\begin{align*}
\frac{\partial}{\partial\theta}\ln\prod_{i=1}^nf(x_i,\theta)&=\frac{\partial}{\partial\theta}\ln\left\{\frac{e^{-n\theta}\theta^{\sum x_i}}{\prod x_i!}\prod I_{\{0,1,\cdots\}}(x_i)\right\}\\
&=\frac{\partial}{\partial\theta}\left\{-n\theta+\sum_{i=1}^nx_i\ln\theta\right\}\\
&=-n+\frac{\sum_{i=1}^nx_i}{\theta}\\
&=\frac{-n\theta+n\bar{x}}{\theta}\\
&=\frac{n}{\theta}(\bar{x}-\theta),
\end{align*}
la anterior expresión es de la forma $K(\theta)(t(x_1,\cdots,x_n)-\theta)$ donde $K(\theta)=n/\theta$ y $t(x_1,\cdots,x_n)=\bar{x}$. Así se concluye que el estimador $\bar{X}$ tiene varianza igual a la cota de Cramer Rao, teniendo en cuenta que es también insesgado para $\theta$, se puede concluir que $\bar{X}$ es UMVUE para $\theta$.

Finalmente, resaltamos el procedimiento para comprobar que un estimador sea UMVUE. Primero se debe demostrar que el estimador es insesgado y luego comprobar que la varianza del estimador es igual a la cota de Cramer Rao. Para este último hay dos formas de hacerlo: (1) usar directamente la definición, en este caso se debe calcular la varianza del estimador y también calcular la cota de Cramer Rao; (2) usar el Resultado 2.4.14, en este caso no es necesario el cálculo de la varianza del estimador, ni la cota de Cramer Rao.

\subsection{Completez}

Otro concepto útil en la construcción de un buen estimador es el concepto de la completez. Este concepto, a diferencia de conceptos como sesgo, varianza o suficiencia, carece de una interpretación clara y de fácil entendimiento, pero resulta ser muy útil. Como se verá más adelante, este concepto nos permite encontrar los estimadores UMVUE.

\begin{Defi}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con función de densidad $f(x_i,\theta)$, y $T$ una estadística, se dice que $T$ es completo\index{Estimador!completo} para $\theta$, si para cualquier función $g(\cdot)$, el hecho de $E(g(T))=0$ para todo $\theta$, implica que $g(T)=0$.
\end{Defi}

En algunos casos, no es muy complicado demostrar que un estimador es completo, como lo ilustra el siguiente ejemplo.

\begin{Eje}
\index{Estimador!completo!Bernoulli}Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ una muestra aleatoria con distribución $Ber(p)$, entonces el estimador $\sum_{i=1}^nX_i$ es un estimador completo para $p$. Para ver eso, tomamos una función $g(\cdot)$ cualquiera, y supongamos que $E(g(\sum_{i=1}^nX_i))=0$, y veamos que $g(\sum_{i=1}^nX_i)=0$. Recordando que la distribución de $\sum_{i=1}^nX_i$ tiene distribución $Bin(n,p)$, tenemos:
\begin{align*}
0&=E(g(\sum_{i=1}^nX_i))\\
&=\sum_{i=0}^ng(i)\binom{n}{i}p^i(1-p)^{n-i}\\
&=(1-p)^n\sum_{i=0}^n\binom{n}{i}g(i)(\frac{p}{1-p})^i,
\end{align*}

de donde se tiene que $\sum_{i=0}^n\binom{n}{i}g(i)(\frac{p}{1-p})^i=0$, nótese que el lado izquierdo de la igualdad es un polinomio en $\frac{p}{1-p}$ de grado $n$. Recordando que un polinomio es igual a 0 si cada coeficiente del polinomio es 0, entonces se puede concluir que $\binom{n}{i}g(i)=0$ para todo $i=0,\cdots,n$, de donde se tiene que $g(i)=0$ para $i=0,\cdots,n$. Ahora la estadística $\sum_{i=1}^nX_i$ toma valores 0, $\cdots$, $n$, de donde se concluye finalmente que $g(\sum_{i=1}^nX_i)=0$.
\end{Eje}

En el anterior ejemplo, la muestra aleatoria proviene de una distribución discreta, cuando se trata de una distribución continua, la forma de proceder es diferente, como se ilustra en el siguiente ejemplo.

\begin{Eje}
\index{Estimador!completo!uniforme}Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria con distribución uniforme sobre $[0,\theta]$, el estimador por el método de máxima verosimilitud es $X_{(n)}$ el máximo de la muestra, veamos que este estimador es completo. Para cualquier función $g$, tenemos
\begin{align*}
0&=E(g(X_{(n)}))\\
&=\int_{0}^{\theta}g(x)\frac{nx^{n-1}}{\theta^n}dx,
\end{align*}

utilizando el teorema fundamental de cálculo, se tiene que
\begin{equation*}
0=\frac{ng(\theta)}{\theta}
\end{equation*}

para todo $\theta>0$, es decir, $g(\theta)=0$ para todo $\theta>0$, y como la estadística $X_{(n)}$ toma valores positivos, entonces $g(X_{(n)})=0$.
\end{Eje}

Para distribuciones pertenecientes a la familia exponencial, es muy fácil encontrar una estadística completa. Para distribuciones en la familia exponencial uniparamétrica\index{Estimador!completo!familia exponencial} tenemos el siguiente resultado.

\begin{Res}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución $f(x,\theta)$ perteneciente a la familia exponencial, es decir,
\begin{equation*}
f(x,\theta)=h(x)c(\theta)\exp\{d(\theta)T(x)\},
\end{equation*}

entonces la estadística $\sum_{i=1}^nT(X_i)$ es una estadística completa para $\theta$.
\end{Res}

La versión equivalente para distribuciones en la familia exponencial biparamétrica se da a continuación.

\begin{Res}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución $f(x_i,\theta_1,\theta_2)$ perteneciente a la familia exponencial\index{Estimador!completo!familia exponencial} biparamétrica de la forma
\begin{equation*}
f(x_i,\theta_1,\theta_2)=c(\btheta)h(x)\exp\{d(\btheta)'T(x)\},
\end{equation*}

donde $\btheta=(\theta_1,\theta_2)$, $d(\btheta)=(d_1(\btheta),d_2(\btheta))'$ y $T(x)=(T_1(x),T_2(x))'$, entonces las estadística $\sum_{i=1}^nT_1(X_i)$ y $\sum_{i=1}^nT_2(X_i)$ son estadísticas completas para $\theta_1$ y $\theta_2$.
\end{Res}

Los dos anteriores resultados, en conjunto con los resultados 2.4.6 y 2.4.7, nos permiten encontrar fácilmente estadísticas que sean a la vez suficientes y completas para distribuciones pertenecientes a la familia exponencial.

Una utilidad de las estadísticas completas es que, anteriormente se había visto que para un parámetro puede haber más de un estimador insesgado, pero cuando el estimador insesgado es función se combina con completez, sí se tiene la unicidad, como lo ilustra el siguiente resultado.

\begin{Res}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ una muestra aleatoria con parámetro $\theta$, si $T$ es una estadística completa, $g_1(T)$ y $g_2(T)$ son estimadores insesgados de $\theta$, entonces $g_1(T)=g_2(T)$.
\end{Res}

\begin{proof}
Por hipótesis, se tiene que $E(g_1(T))=E(g_2(T))=0$, de donde, $E((g_1-g_2)(T))=0$, por definición de estimador completo, se sigue que $(g_1-g_2)(T)=0$, es decir, $g_1(T)=g_2(T)$.
\end{proof}

En la demostración del Resultado 2.4.3, se mencionó el teorema de Basu. Este teorema necesita los conceptos de suficiencia y completez de una estadística y también el concepto de estadística auxiliar\index{Estadística!auxiliar}, esto es, estadística cuya distribución no depende del parámetro de la distribución. Dado este concepto, presentamos a continuación el teorema de Basu\index{Teorema!de Basu}.

\begin{Res}
Si $T$ es una estadística suficiente y completa, entonces $T(X)$ es independiente de toda estadística auxiliar.
\end{Res}

\begin{proof}
La demostración para el caso discreto se encuentra en \citeasnoun[p. 287]{Casella}.
\end{proof}

El Resultado 2.4.3 puede ser fácilmente demostrado usando el teorema de Basú, puesto que en primer lugar, en una muestra proveniente de la distribución $N(\mu,\sigma^2)$, $\bar{X}$ es una estadística suficiente y completa para $\mu$. Por otro lado, $\dfrac{\sum(X_i-\bar{X})^2}{\sigma^2}\sim\chi^2_{n-1}$, de donde podemos tener que la distribución de $S^2_n$ no depende de $\mu$, al igual que la distribución de $S^2_{n-1}$; de esta forma, podemos concluir que $\bar{X}$ y $S^2_{n}$ (o $S^2_{n-1}$) son independientes.

La mayor importancia de las propiedades de suficiencia y completez es que aparte de ser propiedades deseables para los estimadores, nos permiten construir estimadores UMVUE, como lo ilustra el siguiente resultado.

\begin{Res}
Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con parámetro $\theta$, si $T_1$ es un estimador insesgado para una función del parámetro $g(\theta)$ y $T_2$ es suficiente y completo para $\theta$, entonces se tiene que $E(T_1|T_2)$ es el único estimador UMVUE para $g(\theta)$\index{Estimador!UMVUE}.
\end{Res}

El anterior resultado nos brinda una herramienta poderosa para encontrar estimadores UMVUE, que consiste en los siguientes pasos:
\begin{enumerate}[(1)]
\item Encontrar un estimador insesgado para $g(\theta)$, la función del parámetro $\theta$ que se desea estimar. Nótese que cuando $g(\theta)$ es el media teórica, entonces un estimador insesgado es el promedio muestral.
\item Encontrar un estimador suficiente y completo para $\theta$, que para distribuciones pertenecientes a la familia exponencial resulta bastante útil.
\end{enumerate}

Ahora aplicamos las anteriores herramientas a un problema de estimación.

\begin{Eje}
\index{Estimador!UMVUE!Poisson}Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con distribución $Pois(\theta)$, se quiere encontrar el estimador UMVUE para $\theta$. Para eso, primero se debe encontrar un estimador insesgado para $\theta$, que es $\bar{X}$ por ser $\theta$ el media teórica. En segundo lugar, la distribución Poisson pertenece a la familia exponencial con $T(x)=x$, entonces los Resultados 2.4.5 y 2.4.15 establecen que $\sum_{i=1}^nX_i$ es una estadística suficiente y completa para $\theta$. De esta manera, el estimador UMVUE es $E(\bar{X}|\sum_{i=1}^nX_i)=\bar{X}$.
\end{Eje}

Para distribuciones con dos parámetros, existe la siguiente generalización:

\begin{Res}
\index{Estimador!UMVUE}Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ una muestra aleatoria con parámetro $\btheta=(\theta_1,\theta_2)$, si $T_1$, $T_2$ son estimadores insesgados para $\theta_1$, $\theta_2$, y $S_1$, $S_2$ son suficientes y completos para $\theta_1$, $\theta_2$, entonces se tiene que $E(T_1,T_2|S_1,S_2)$ son UMVUE para $\theta_1$ y $\theta_2$, además se tiene la unicidad\index{Estimador!UMVUE}.
\end{Res}

\begin{Eje}
\index{Estimador!UMVUE!normal}Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria con distribución $N(\mu,\sigma^2)$, $\sum_{i=1}^nX_i$ y $\sum_{i=1}^nX_i^2$ son estimadores suficientes y completos para $\mu$ y $\sigma^2$, y también se vio anteriormente que $\bar{X}$ es insesgado para $\mu$ y $S^2_{n-1}$ insesgado para $\sigma^2$, entonces el resultado anterior indica que las estadísticas $E(\bar{X},S^2_{n-1}|\sum_{i=1}^nX_i,\sum_{i=1}^nX_i^2)$ son UMVUE único para $\mu$ y $\sigma^2$. Para calcular la esperanza condicional, se tiene en cuenta que en primer lugar $\bar{X}$ es función de $\sum_{i=1}^nX_i$ y $S^2_{n-1}=(\sum_{i=1}^nX_i^2-n\bar{X}^2)/(n-1)$ es función de $\sum_{i=1}^nX_i$ y $\sum_{i=1}^nX_i^2$, entonces se tiene que los UMVUE para $\mu$ y $\sigma^2$ son $\bar{X}$ y $S^2_{n-1}$.

Nótese en primer lugar que por la forma como están definidos $S^2_n$ y $S^2_{n-1}$, se tiene que $Var(S^2_n)<Var(S^2_{n-1})$, es decir, el estimador $S^2_n$ tiene una varianza más pequeña que la del estimador UMVUE; sin embargo, nótese que como estimador de $\sigma^2$, $S^2_n$ es sesgado. Lo anterior muestra el hecho de que puede un estimador UMVUE tener varianza más pequeña sólo entre los estimadores insesgados; sin embargo, puede existir un estimador sesgado con varianza aún más pequeña que la de UMVUE.

También observemos que en el Ejemplo 2.4.9, se encontró que la matriz de información en la muestra acerca de los parámetros $\mu$ y $\sigma^2$ es
\begin{equation*}
I_{X_1,\cdots,X_n}(\mu,\sigma^2)=\begin{pmatrix}
\dfrac{n}{\sigma^2}&0\\
0&\dfrac{n}{2\sigma^4}
\end{pmatrix}
\end{equation*}

cuya inversa está dada por
\begin{equation*}
I^{-1}_{X_1,\cdots,X_n}(\mu,\sigma^2)=\begin{pmatrix}
\dfrac{\sigma^2}{n}&0\\
0&\dfrac{2\sigma^4}{n}
\end{pmatrix}
\end{equation*}

la cual corresponde a la cota de Cramer-Rao para el vector de estimadores $(\bar{X},S^2_{n-1})$.

Ahora, a pesar de que ya se comprobó que  $(\bar{X},S^2_{n-1})$ es UMVUE para $(\mu,\sigma^2)$, la matriz de varianzas de $(\bar{X},S^2_{n-1})$ no es la cota de Cramer-Rao, puesto que
\begin{equation*}
Var(S^2_{n-1})=\dfrac{\sigma^4}{(n-1)^2}Var\left(\frac{(n-1)S^2_{n-1}}{\sigma^2}\right)=\dfrac{\sigma^4}{(n-1)^2}2(n-1)=\dfrac{2\sigma^4}{n-1}.
\end{equation*}

la cual es más grande que el elemento $2\sigma^4/n$ en la cota de Cramer-Rao.

\end{Eje}

Hasta este punto del libro, se han expuesto varios aspectos que se deben tener en cuenta al momento de escoger un estimador; más aún, al momento de escoger uno de varios posibles estimadores. Y al momento de realizar comparaciones, es necesario calcular la esperanza y la varianza de los estimadores, y estos cálculos pueden ser muy difíciles cuando los estimadores toman formas complicadas; por ejemplo, cuando se trata de una muestra proveniente de $Unif[\theta_1,\theta_2]$ se vio anteriormente que los estimadores de momentos de los parámetros son $\bar{X}-\sqrt{3}S_n$ y $\bar{X}+\sqrt{3}S_n$, respectivamente, mientras que los estimadores de momentos son las estadísticas de orden $X_{(1)}$ y $X_{(n)}$. Para tener una idea en este caso de cuál método arrojó mejores estimadores, deberíamos calcular el sesgo y la varianzas de estos estimadores, pero es claro que no es nada trivial calcular estas cantidades para los estimadores de momentos. En casos como el anterior, no tenemos otra alternativa que utilizar las simulaciones. Las simulaciones se pueden llevar a cabo simulando muestras de $Unif(\theta_1,\theta_2)$, y en cada muestra simulada se calculan los estimadores de momentos y de máxima verosimilitud.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{uniforme2.eps}
\caption[\textsl{El estimador de MV y el de momentos en muestras uniforme}]{\textsl{Comparación empírico entre los estimadores de máxima verosimilitud y de momentos en una muestra proveniente de $Unif(\theta_1,\theta_2)$}}
\end{figure}

En la Figura 2.16 se muestran los resultados de estos estimadores para muestras provenientes de $Unif(3,5)$. Podemos observar que los estimadores de máxima verosimilitud parecen sobreestimar a $\theta_1$ y subestimar $\theta_2$ en muestras pequeñas; en muestras grandes, el efecto de sobreestimación o subestimación tiende a desaparecer. Por otra parte, con respecto a los estimadores de momentos, en general no se detectan problemas de sobreestimación o subestimación, aunque la varianza es considerablemente mayor que los estimadores de máxima verosimilitud, aún para muestras grandes. Por consiguiente, se recomienda usar los estimadores de máxima verosimilitud en muestras provenientes de la distribución $Unif(\theta_1,\theta_2)$.

\index{Estimador!función del parámetro}El lector puede notar que la teoría expuesta anteriormente es válida para cuando se quiere estimar una función del parámetro $g(\theta)$. Sin embargo, poco se ha discutido acerca de la evaluación de estos estimadores. La razón es que, en primer lugar, en general no es fácil determinar el sesgo de estos estimadores. Sobre este aspecto, se considera el Ejemplo 2.3.3, donde en un conjunto de ensayos del tipo Bernoulli, el parámetro de interés es la probabilidad de éxito $p$ que se estima mediante $\bar{X}$, pero si la cantidad que se desea estimar se cambia a $3p(1-p)^4$, aunque es fácil encontrar el estimador de máxima verosimilitud $3\bar{X}(1-\bar{X})^4$, no es fácil determinar si este es insesgado para $3p(1-p)^4$, y por consiguiente tampoco se puede determinar si este es el estimador UMVUE para $3p(1-p)^4$. Lo anterior indica que en general no se puede establecer que para cualquier función $g$, si $T$ es un estimador insesgado para $\theta$, entonces $g(T)$ es insesgado para $g(\theta)$, esto es, no se tiene en general que $E(g(T))=g(\theta)$ suponiendo que $E(T)=\theta$.

Es claro que cuando la función $g$ es una función lineal, esto es $g(\theta)=a\theta+b$, para constantes $a$ y $b$, entonces $E(g(T))=E(aT+b)=aE(T)+b$ y si $T$ es insesgado para $\theta$, $g(T)$ también lo es para $g(\theta)$. Cuando $g$ toma otras formas, podemos calcular $E(g(T))$ e inclusive $Var(g(T))$ de manera aproximada usando el método de Delta\index{Método de Delta} presentado a continuación.

\begin{Res}
Dado $T$ un estimador de $\theta$, y $g$ una función, entonces se tiene que
\begin{equation*}
E(g(T))\approx g(\theta)+g'(\theta)E(T-\theta)
\end{equation*}
y
\begin{equation*}
Var(g(T))\approx (g'(\theta))^2Var(T)
\end{equation*}
De esta forma, se tiene que cuando $T$ es insesgado para $\theta$, $E(g(T))\approx g(\theta)$.
\end{Res}

\begin{proof}
Para cada valor $t$ que toma el estimador $T$, se hará uso de la expansión de Taylor de primer orden para $g(t)$ alrededor del punto $t=\theta$, tenemos que $g(t)\approx g(\theta)+g'\theta(t-\theta)$. De esta forma, se tiene que $g(T)\approx g(\theta)+g'(\theta)(T-\theta)$. Tomando la esperanza, se tiene que $E(g(T))\approx g(\theta)+g'(\theta) E(T-\theta)$. Es claro que cuando $T$ es insesgado para $\theta$, $E(g(T))\approx g(\theta)$.

Por otro lado, tomando varianza en $g(T)\approx g(\theta)+g'(\theta)(T-\theta)$, se tiene que
\begin{align*}
Var(g(T))&\approx (g'(\theta))^2Var(T-\theta)\\
&=(g'(\theta))^2Var(T)
\end{align*}
\end{proof}

Del anterior resultado, se puede concluir que $g(T)$ es un estimador \emph{aproximadamente} insesgado para $g(\theta)$. Pero no podemos saber qué tan buena (o qué tan mala) resulta esta aproximación, y tampoco saber, en general, si con el posible aumento del tamaño muestral, se puede hacer que $E(g(T))$ se acerque más a $g(\theta)$. Sin embargo, mediante el uso de simulaciones, podemos hacernos una idea de la bondad de esta aproximación con diferentes funciones $g$ para diferentes tamaños muestrales $n$.

Suponga que en una muestra proveniente de la distribución exponencial con parámetro $\theta=1$, se desea estimar $p_1=Pr(X<1)$ y $p_2=Pr(1<X<2)$. Estas dos probabilidades se pueden expresar como funciones del parámetro como $p_1=e^{-1/\theta}$ y $p_2=e^{-1/\theta}-e^{-2/\theta}$, respectivamente. Y los estimadores de máxima verosimilitud de $p_1$ y $p_2$ son $T_1=e^{-1/\bar{X}}$ y $T_2=e^{-1/\bar{X}}-e^{-2/\bar{X}}$ respectivamente, para estudiar el sesgo de $T_1$ y $T_2$ como estimadores de $p_1$ y $p_2$. Se simulan 1000 muestras de tamaño 5, 10, 30, 50, 100, 500 de una distribución $Exp(1)$ y $Exp(5)$, y en cada muestra simulada se calculan los valores que toman $T_1$ y $T_2$. Y para cada $n$ fijo se calcula el promedio de los 1000 valores de $T_1$ y $T_2$, éstos se pueden tomar como estimaciones de $E(T_1)$ y $E(T_2)$. Y por consiguiente, restando $p_1$ y $p_2$ a estas estimaciones, se pueden obtener estimaciones del sesgo de $T_1$ y $T_2$. En la Figura 2.17, se muestra el comportamiento de estos dos estimadores en término del sesgo. Podemos observar que en primer lugar, a medida que el tamaño muestral crece, el sesgo de ambos estimadores se acerca al valor 0, es decir, $T_1$ y $T_2$ parecen ser asintóticamente insesgados. Otro aspecto interesante es que la sobreestimación o la subestimación de los estimadores depende del valor del parámetro $\theta$, pues cuando $\theta=1$, $T_2$ siempre tuvo un sesgo estimado negativo, pero al cambiar el valor de $\theta$ a 5, se encuentra que ahora su sesgo estimado es siempre positivo.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{Delta_exponencial.eps}
\caption[\textsl{Sesgo estimado de diferentes estimadores}]{\textsl{Sesgo estimado de los estimadores $T_1$ y $T_2$ para diferentes tamaños de muestra.}}
\end{figure}

Repitamos la simulación para muestras provenientes de la distribución Bernoulli con parámetro $p$. Suponga que se desea estimar la varianza teórica dada por $p(1-p)$, la probabilidad de obtener un éxito en cuatro ensayos dada por $4p(1-p)^3$ y la probabilidad de obtener más de un éxito en cuatro ensayos dada por $1-(1-p)^4$. Como el estimador de máxima verosimilitud de $p$ es $\bar{X}$, tenemos que los estimadores de máxima verosimilitud de estas tres cantidades que se desean estimar son $T_1=\bar{X}(1-\bar{X})$, $T_2=4\bar{X}(1-\bar{X})^3$ y $T_3=1-(1-\bar{X})^4$, respectivamente. Para estudiar el sesgo de estos tres estimadores, se simulan 1000 muestras de tamaño 5, 10, 30, 50, 100, 500 de una distribución $Ber(0.3)$ y $Ber(0.7)$, y en cada muestra simulada se calculan los valores que toman $T_1$, $T_2$ y $T_3$. Y se obtienen las estimaciones de los sesgos de los tres estimadores análogamente al caso de la distribución exponencial. En la Figura 2.18, se muestra el comportamiento de estos dos estimadores en término del sesgo, y podemos observar comportamientos análogos al caso de la distribución exponencial, esto es, al incrementar $n$ los estimadores son asintóticamente insesgados, y la sobreestimación o la subestimación puede depender del valor del parámetro $p$.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{Delta_binomial.eps}
\caption[\textsl{Sesgo estimado de diferentes estimadores}]{\textsl{Sesgo estimado de los estimadores $T_1$, $T_2$ y $T_3$ para diferentes tamaños de muestra.}}
\end{figure}

\subsection{Consistencia}

El concepto de consistencia, al igual que el concepto del insesgamiento asintótico, es una propiedad asintótica, es decir, se considera el caso cuando el tamaño muestral $n\rightarrow\infty$. El concepto de insesgamiento asintótico establece que cuando el tamaño muestral es suficientemente grande, los valores que toma el estimador están alrededor de la cantidad que se desea estimar, $g(\theta)$; mientras que el concepto de consistencia va un paso más allá, y estudia características del estimador visto como una variable aleatoria. Más específicamente, estudia la probabilidad de que el estimador esté cercano de $g(\theta)$. La definición de consistencia se da a continuación.

\begin{Defi}
Dada una muestra aleatoria con parámetro desconocido $\theta$, y $T$ un estimador de $g(\theta)$ para alguna función $g$, se dice que $T$ es un estimador consistente\index{Estimador!consistente} si $T$ converge en probabilidad a $g(\theta)$ ($T_n\overset{P}{\rightarrow}g(\theta)$), esto es, para todo $\varepsilon>0$, se tiene que
\begin{equation*}
\lim_{n\rightarrow\infty}Pr(|T-g(\theta)|>\varepsilon)=0.
\end{equation*}
\end{Defi}

Como se había mencionado anteriormente, es natural estimar la media teórica $\mu$ utilizando el promedio muestral $\bar{X}$, y en distribuciones como la Bernoulli, Poisson, exponencial y normal $\bar{X}$ es UMVUE para $\mu$, es decir, entre todos los estimadores insesgados, $\bar{X}$ tiene menor varianza. Adicionalmente, la siguiente ley débil de los grandes números nos ilustra que además es un estimador consistente.

\begin{Res}
\index{Estimador!consistente!media muestral}Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con media común $\mu$ y $E(|X_i|)<\infty$ para todo $i$, y se define $\bar{X}_n=\sum_{i=1}^nX_i/n$, entonces $\bar{X}_n$ convergen en probabilidad a $\mu$.
\end{Res}
\begin{proof}
Esta versión de la ley débil de los grandes números se conoce con el nombre de Khintchin, y no se requiere de la existencia de la varianza teórica. Cuando ésta existe, el resultado se puede mostrar fácilmente aplicando la desigualdad de Chebychev. En el caso general sin ningún supuesto acerca de la varianza teórica, la prueba es más complicada, el lector interesado puede consultar \citeasnoun[p. 205]{Resnick}.
\end{proof}

Dado que el concepto de la convergencia está directamente relacionado con la convergencia en probabilidades, las propiedades deseables de esta convergencia nos permiten obtener conclusiones interesantes acerca de la consistencia. Específicamente, se conoce en la teoría estadística que si $X_n\overset{P}{\rightarrow}a$ y $g$ es una función continua en $a$, entonces $g(X_n)\overset{P}{\rightarrow}g(a)$. De esta forma, no sólo podemos afirmar que en una muestra del tipo Bernoulli, $\bar{X}$ es consistente para estimar la probabilidad de éxito $p$, sino también $\bar{X}(1-\bar{X})$ es consistente para la varianza teórica $p(1-p)$\index{Estimador!consistente!invarianza}, y los estimadores mencionados en el Ejemplo 2.3.3 también lo son (aunque no podemos afirmar lo mismo acerca de las propiedades de insesgamiento y varianza mínima).

El concepto de consistencia, por su definición, describe la propiedad de que a medida que el tamaño muestral crece, el estimador se hace cada vez más cercano a la cantidad que se desea estimar $g(\theta)$. Este concepto está ligado, naturalmente, con la varianza estimador, pues entre más pequeña sea ésta, más concentrados están los valores que toma el estimador, y si adicionalmente el estimador es insesgado, los valores que toman el estimador deben estar muy cercanos a $g(\theta)$. El siguiente resultado confirma esta relación entre los conceptos del insesgamiento, varianza pequeña y consistencia.

\begin{Res}
\index{Estimador!consistente}Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con parámetro desconocido $\theta$, si $T$ es un estimador asintóticamente insesgado y su $Var(T)\rightarrow0$ cuando $n\rightarrow\infty$, entonces $T$ es consistente.
\end{Res}

\begin{proof}
Consideramos el error cuadrático medio de $T$, este se puede escribir en término del sesgo y la varianza de $T$ de la forma $ECM(T)=B_T^2+Var(T)$. Dada la hipótesis del resultado, tenemos que
\begin{equation*}
\lim_{n\rightarrow\infty}ECM(T)=0.
\end{equation*}
Ahora,
\begin{align*}
\lim_{n\rightarrow\infty}Pr(|T-g(\theta)|>\varepsilon)
&\leq\lim_{n\rightarrow\infty}\dfrac{1}{\varepsilon^2}E[(T-g(\theta))^2]\hspace{0.8cm}\text{Desigualdad de Chebychev}\\
&=\lim_{n\rightarrow\infty}\dfrac{1}{\varepsilon^2}ECM(T)\\
&=0
\end{align*}
y concluimos que $T$ es consistente.
\end{proof}

Nótese que usando el anterior resultado junto con el Resultado 2.4.1, es otra forma de probar que en muestras provenientes de cualquier distribución, se tiene que $\bar{X}$ es un estimador consistente para $\mu$, aunque en este caso, se exige que la varianza teórica sea finita para la prueba.

Dada la anterior relación entre la consistencia y la varianza de un estimador, podemos establecer alguna relación entre los estimadores consistentes y los UMVUE. Cuando un estimador $T$ es insesgado para $g(\theta)$, tenemos que la cota de Cramer-Rao dada en (\ref{Cramer}), donde $I_X(\theta)$ es la información contenida en una variable, y por consiguiente no depende de $n$. De esta forma, si un estimador insesgado tiene varianza igual a la cota de Cramer-Rao, su varianza converge a 0 y por el Resultado 2.4.23, podemos afirmar que también es consistente. Pero si podemos afirmar la igualdad entre la cota de Cramer-Rao y la varianza del estimador, tampoco podemos concluir la consistencia.

Para finalizar el concepto de consistencia, el lector puede visualizar este concepto en la Figura 2.14, donde se observa claramente que cuando $n$ crece, los valores de $\bar{x}$ se acercan cada vez más a $\mu$.


\section{Comparación empírica de algunas propiedades}

Hasta este punto, hemos introducido varias propiedades de un estimador para tener en cuenta a la hora de escoger un buen estimador. Sin embargo, muchos estadísticos y/o profesores han manifestado la dificultad de <<traducir>> estos conceptos abstractos en la práctica. Una manera de solucionar este problema es ilustrar gráficamente estos conceptos como lo hemos venido haciendo en este texto. Aquí ilustramos otros aspectos y conceptos adicionales que pueden ayudar a los lectores a entender mejor estos conceptos abstractos.

En primer lugar, suponga que tenemos dos estimadores para un mismo parámetro tales que uno es insesgado y el otro es levemente sesgado. ¿Cuál estimador se debe escoger? Más específicamente, suponga que se desean comparar dos estimadores de la varianza de una muestra aleatoria de variables con distribución Normal de media 5 y varianza 81; por ejemplo, el estimador de máxima verosimilitud
$S^2_n=\frac{1}{n}\sum_{i=1}^n(x_i-\bar{x})$ y el clásico estimador insesgado $S^2_{n-1}=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{x})$.

La propiedad del insesgamiento está relacionada directamente con la esperanza matemática del estimador, en términos de su distribución de muestreo, y por consiguiente, para un estimador sesgado es frecuente que ese sesgo dependa del parámetro teórico, por lo que no se puede cuantificar en la práctica. Sin embargo, podemos usar simulaciones para tener una estimación de su esperanza. La Figura 2.19 nos muestra una estimación de la esperanza de los estimadores $S^2_n$ y $S^2_{n-1}$. Esta figura fue realizada de la siguiente manera: para un tamaño de muestra fijo $ n=10$ generada de la distribución $N(5,81)$, se estima el parámetro de interés con los dos estimadores. Ahora, este proceso se realiza 1, 10, 100, 1000, 10000, 100000 y 1000000 veces. En cada repetición se calcula el promedio de las estimaciones y se grafica. Nótese que en un momento dado cada una de las dos líneas parece converger a un valor. Por supuesto, el estimador insesgado $S^2_n$ converge a 81, el verdadero valor, mientras que el sesgado converge a un valor inferior, lo cual coincide con el hecho de que $S^2_{n-1}$ subestima a la varianza teórica.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{est2.eps}
\caption{\textsl{Esperanza simulada de los estimadores $S^2_n$ y $S^2_{n-1}$ con $n=10$.}}
\end{figure}

\begin{verbatim}
> # PROPIEDAD DE INSESGAMIENTO
>
>  N<-c(1,10,100,1000,10000,100000,1000000)
>  n<-10
>
>  meanvar1<-rep(NA,length(N))
>  meanvar2<-rep(NA,length(N))
>
>  for(k in 1:length(N))
+  {
+  var1<-rep(NA,N[k])
+  var2<-rep(NA,N[k])
+  for(l in 1:(N[k])){
+  data<-rnorm(n,5,9)
+  var1[l]<-var(data)
+  var2[l]<-(n-1)*var(data)/n
+  }
+  meanvar1[k]<-mean(var1)
+  meanvar2[k]<-mean(var2)
+  }
>
>  meanvar1
[1] 78.75174 67.39362 88.84951 81.07334 81.07010 80.91384 81.01865
>  meanvar2
[1] 70.87657 60.65425 79.96456 72.96601 72.96309 72.82246 72.91679
>
>  plot(meanvar1,type="b", col=4,ylim=c(60,170),xlab="Número de
+  repeticiones", ylab="Estimación de la varianza", xaxt="n")
>  lines(meanvar2,type="b", col=2, pch=2)
>  abline(h=81)
>  axis(1, 1:length(N), N)
>  legend(3,120,c("Insesgado","Sesgado"), col=c(4,2), lty=c(1,1),
+  pch=c(1,2))
>
\end{verbatim}


Ahora, podemos preguntarnos si los resultados de la Figura 2.19 son una prueba fehaciente de que el estimador insesgado resulta mejor que su competidor. Sin embargo, el hecho de que un estimador sea insesgado indica que <<en promedio>> sus valores son iguales al parámetro, pero no garantiza que las estimaciones individuales sean buenas. Es posible que un estimador insesgado arroje estimaciones individuales ridículas pero en promedio sea igual al parámetro. Así que la propiedad de insesgamiento no basta para escoger un estimador.

Existe otro criterio que podemos tener en cuenta para escoger entre dos estimadores de un mismo parámetro$\theta$. Al denotar estos dos estimadores por $ T_1$ y $ T_2$, se dice que $ T_1$ domina a $ T_2$ si el ECM de $T_1$ siempre es menor o igual al ECM de $T_2$ para todo $\theta$, esto es,  $ E[(T_1-\theta)^2]\leq E[(T_1-\theta)^2]$. Y como consecuencia se define la eficiencia relativa como la cociente entre los ECM, dada por \index{Eficiencia relativa}
\begin{equation*}
e(T_1,T_2)=\frac{ E[(T_1-\theta)^2]}{ E[(T_2-\theta)^2]}.
\end{equation*}

De esta forma, un valor de $e(T_1,T_2)$ inferior a 1 para todo $\theta$ muestra la superioridad de $T_1$ comparado con $T_2$. Ahora, análogo al sesgo o la esperanza de un estimador, el ECM y la eficiencia tampoco se pueden conocer en la práctica pues también dependen del parámetro teórico. Podemos adoptar un procedimiento similar al presentado anteriormente para tener una estimación de ECM de los dos estimadores. El siguiente código nos permite calcular ECM de los dos estimadores $S^2_n$ y $S^2_{n-1}$.

\begin{verbatim}
> # PROPIEDAD DE EFICIENCIA
>
>  N<-c(2,10,100,1000,10000,100000,1000000)
>  n<-10
>
>  msevar1<-rep(NA,length(N))
>  msevar2<-rep(NA,length(N))
>
>  for(k in 1:length(N))
+  {
+  var1<-rep(NA,N[k])
+  var2<-rep(NA,N[k])
+  for(l in 1:(N[k])){
+  data<-rnorm(n,5,9)
+  var1[l]<-var(data)
+  var2[l]<-(n-1)*var(data)/n
+  }
+  msevar1[k]<-var(var1)
+  msevar2[k]<-var(var2)+(mean(var1)-81)^2
+  }
>
> msevar1
[1]  965.0842 1529.4889 1460.5530 1651.2588 1420.0702 1461.4549 1458.1439
>  msevar2
[1] 1194.755 1244.103 1183.322 1337.701 1150.396 1183.819 1181.098
>
>  plot(msevar1,type="b", col=4,ylim=c(1000,3100),xlab="Número de
+  repeticiones", ylab="Eficiencia de las estimaciones", xaxt="n")
>  lines(msevar2,type="b", col=2, pch=2)
>  axis(1, 1:length(N), N)
>  legend(3,2500,c("Insesgado","Sesgado"), col=c(4,2), lty=c(1,1),
+  pch=c(1,2))
\end{verbatim}

El anterior código arroja la Figura 2.20 donde se aprecia que el ECM del estimador insesgado está alrededor de 1500, siendo más alto que el ECM del estimador sesgado, que se encuentra alrededor de 1200. Las anteriores cantidades se pueden calcular teóricamente: para el estimador insesgado, resulta ser igual a 1458 y para el sesgado resulta ser 1246. Y podemos concluir que no siempre es mejor un estimador insesgado frente a un sesgado, puesto que el ECM del estimador sesgado puede ser menor.

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{est3.eps}
\caption{\textsl{ECM simulado de los estimadores $S^2_n$ y $S^2_{n-1}$ con $n=10$.}}
\end{figure}


\section{Ejercicios}

\begin{enumerate}[2.1]

\item En una muestra aleatoria $X_1$, $\cdots$, $X_n$ con función de densidad y de distribución $f_X$ y $F_X$, respectivamente, demuestre que la función de densidad y de distribución del máximo $X_{(n)}$ están dadas por (\ref{F_max}) y (\ref{f_max}).

\item En la literatura estadística también es común definir la distribución exponencial con la siguiente función de densidad
        \begin{equation}\label{densidad_exp1}
        f_{X}(x)=\lambda e^{-\lambda x}I_{(0,\infty)}(x)
        \end{equation}
        Se puede ver que la anterior función de densidad utiliza la reparametrización $\lambda=1/\theta$ en la función (\ref{densidad_exp}). Demuestre que en una muestra aleatoria con la función de densidad (\ref{densidad_exp1}), $\hat{\lambda}_{MV}=1/\bar{X}$.

\item En efecto de la estimación, observar una muestra aleatoria $X_1$, $\cdots$, $X_n$ de tamaño $n$ proveniente de una distribución $Ber(p)$ equivale a observar el valor de la variable aleatoria $S=\sum X_i$ la cual tiene distribución $Bin(n,p)$ (esto es, $S$ es una estadística suficiente.).
        \begin{enumerate}[(a)]
             \item Demuestre que en la muestra $X_1$, $\cdots$, $X_n$, $\hat{p}_{MV}=\hat{p}_{mom}=\bar{X}$,
             \item Demuestre que cuando se usa $S$ para estimar $p$, $\hat{p}_{MV}=S/n=\bar{X}$,
             \item Demuestre que $S$ es suficiente.
        \end{enumerate}

\item Un almacén de ropas femeninas, después de la navidad, lanza la promoción del descuento de hasta 60\% en todo el almacén. El gerente desea conocer qué tan efectiva es la promoción; para ello, él tuvo en cuenta que en un determinado día entraron al almacén 40 clientes, y 25 de ellos hicieron alguna compra. Cómo puede estimar la probabilidad de que
        \begin{enumerate}[(a)]
            \item ¿Un cliente realice alguna compra?
            \item ¿Ninguna venta sea exitosa en cinco clientes consecutivos?
        \end{enumerate}

\item Considere una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución $N(\mu,\sigma^2)$. Demuestre que
        \begin{enumerate}[(a)]
             \item El estimador de máxima verosimilitud de $\mu$ siempre es $\bar{X}$ sin importar si $\sigma^2$ es conocida o no, esto es, demuestre que el estimador de máxima verosimilitud de $\mu$ cuando $\sigma^2=\sigma^2_0$ es $\bar{X}$
             \item El estimador de máxima verosimilitud de $\sigma^2$ puede variar dependiendo si $\mu$ es conocida o no, esto es, demuestre que el estimador de máxima verosimilitud de $\sigma^2$ cuando $\mu=\mu_0$ es $\sum_{i=1}^n(X_i-\mu_0)^2/n$.
        \end{enumerate}

\item Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria proveniente de distribución exponencial con media $\theta$,
        \begin{enumerate}[(a)]
            \item Encuentre el estimador de máxima verosimilitud de $\theta$.
            \item Encuentre el estimador de máxima verosimilitud de $Pr(X>1)$, donde $X$ tiene la misma distribución exponencial.
            \item Encuentre dos estimadores de momentos de $\theta$.
            \item Encuentre un estimador de momentos de $Pr(X<2)$, donde $X$ tiene la misma distribución exponencial.
        \end{enumerate}

\item Se desea conocer el funcionamiento de un cierto dispositivo electrónico fabricado por una empresa, se seleccionan 20 dispositivos terminados al final de la línea de producción, y se ponen en funcionamiento. La vida útil de los dispositivos (en horas) fue: 6, 23, 1, 38, 43, 149, 2, 32, 41, 23, 10, 47, 46, 111, 43, 30, 21, 3, 96 y 53.
\begin{enumerate}[(a)]
    \item Elabore una gráfica QQ plot para verificar si los datos provienen de una distribución exponencial.
    \item ¿Cómo se puede obtener un estimativo de la vida útil del dispositivo electrónico basando en la muestra seleccionada?
    \item ¿Cuál es la probabilidad estimada de que un dispositivo funcione por más de 55 horas?
    \item Suponga que un cliente compra dos de esos dispositivos, y ambos se dañaron antes de las 30 horas, ¿cuál es la probabilidad de que esto ocurra?
\end{enumerate}

\item Dentro del contexto del Ejemplo 2.3.8,
    \begin{enumerate}[(a)]
        \item Grafique la función $L(N)$ para $R=8$, $n=5$ y $x=4$, y verifica en la gráfica que $L(N)$ tiene un máximo en $[Rn/x]$.
        \item Considerando a (\ref{L_hipergeo}) como una función de $R$, grafica la función $L(R)$ para $N=12$, $n=8$, $x=4$. Verifique en la gráfica que $L(R)$ tiene un máximo en $[x(N+1)/n]$
        \item Grafique la función $L(R)$ para $N=11$, $n=8$, $x=4$. Verifique en la gráfica que $L(R)$ tiene un máximo en $x(N+1)/n-1$ y $x(N+1)/n$.
    \end{enumerate}

\item En una época de brote de una enfermedad respiratoria, se quiere conocer el número de colegios en una determinada ciudad que cuenta con condiciones sanitarias adecuadas. Suponga que la ciudad tiene 1325 colegios, y 86 de los 1400 colegios seleccionados al azar tienen buenas condiciones sanitarias. Basado en lo anterior, ¿cuántos colegios de esta ciudad se estima que tienen buenas condiciones sanitarias, y cuántos no lo hacen? Y ¿cuál es el porcentaje de colegios en la ciudad que tienen buenas condiciones sanitarias?

\item El estimador de máxima verosimilitud puede no ser único, como lo ilustra esta situación: suponga que $X_1$, $\cdots$, $X_n$ constituyen una muestra aleatoria proveniente de la distribución uniforme continua sobre el intervalo $[\theta-0.5,\theta+0.5]$, demuestre que cualquier estadística $T$ con $X_{(n)}-0.5\leq T\leq X_{(1)}+0.5$ es un estimador de máxima verosimilitud de $\theta$.

\item  Dada una muestra aleatoria proveniente de una distribución $Unif(-\theta,\theta)$, demuestre que el estimador de máxima verosimilitud de $\theta$ está dado por $\hat{\theta}_{MV}=\max\{-X_{(1)},X_{(n)}\}$.

\item Suponga que un sistema masivo de transporte urbano comienza a funcionar desde las 6 am, y la hora de llegada de la ruta A1 a la primera parada de buses puede ser cualquier hora a partir de las 6 am. Para tener un control acerca de la calidad de servicio de este sistema, se observa el tiempo de llegada de esta ruta en 8 días. Estos tiempos son 6:01 am, 6:07 am, 6:03 am, 6:07 am, 6:10 am, 6:11 am, 6:05 am y 6:03 am. Basado en estas observaciones
    \begin{enumerate}[(a)]
        \item Encuentre una estimación para la hora máxima de llegada de esta ruta.
        \item Encuentre una estimación para la probabilidad de que en un día determinado, la ruta llegue antes de las 6:05 am.
    \end{enumerate}

\item  Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con distribución $U[\theta_1,\theta_2]$,
    \begin{enumerate}[(a)]
        \item Demuestre que los estimadores de máxima verosimilitud de $\theta_1$ y $\theta_2$ son $X_{(1)}$ y $X_{(n)}$, respectivamente.
        \item Calcule el ECM de los estimadores de máxima verosimilitud y de momentos, y concluya cuáles son mejores en términos del menor ECM.
    \end{enumerate}

\item Suponga que se quiere comparar dos marcas de carros con respecto al rendimiento en términos de la distancia recorrida por galón de dos marcas de automóviles en referencias con especificaciones similares bajo circunstancias similares con respecto a la carretera, clima y demás condiciones controlables por los técnicos y expertos automovilísticos. Los datos se muestran en la Tabla 2.3
        \begin{table}
        \centering
        \begin{tabular}{|c|c|}\hline
        &Distancia recorrida (en Km) por galón\\\hline
        Marca A&39.4, 41.1, 39.5, 40.0, 43.7, 46.0, 43.5, 42.1\\
        Marca B&52.6, 49.4, 49.4, 46.4, 51.2, 49.2, 55.0, 53.6, 55.7, 57.4\\\hline
        \end{tabular}
        \caption{\textsl{Datos del Ejercicio 2.14.}}
        \end{table}
        \begin{enumerate}[(a)]
            \item Elabore la gráfica QQ para cada uno de los dos conjuntos datos y verifique que la distribución normal puede ser apropiada para describirlos.
            \item Estime el número de kilómetros recorridos por galón de gasolina y la respectiva desviación estándar en cada una de las dos marcas.
            \item Estime el coeficiente de variación para cada número de kilómetros recorridos por galón de gasolina en cada una de las dos marcas
            \item Estime el porcentaje de automóviles que recorren más de 50 kilómetros por galón para cada una de las dos marcas.
            \item Dado lo anterior, compare las dos marcas en términos de rendimiento y de estabilidad.
            \item Si suponemos que los automóviles de las dos marcas son igualmente estables en términos del rendimiento de gasolina, estime una medida que describa la diferencia promedio entre los automóviles de las dos marcas en términos del rendimiento de gasolina.
        \end{enumerate}

\item Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria con distribución $Exp(\theta)$
        \begin{enumerate}[(a)]
            \item Demuestre que las estadísticas $\sum_{i=1}^nX_i$ y $\bar{X}$ son suficientes para $\theta$.
            \item Encuentre la información de Fisher contenida en la muestra acerca de $\theta$.
        \end{enumerate}

\item Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria con distribución $Pois(\theta)$, un estimador suficiente para $\theta$ es $T=\sum_{i=1}^nX_i$. Compruebe que $I_T(\theta)=I_{X_1,\cdots,X_n}(\theta)$ usando la definición.

\item En una muestra aleatoria $X_1$, $\cdots$, $X_n$ donde $\btheta$ denota el vector de parámetros desconocidos, demostrar que $I_{X_1,\cdots,X_n}(\btheta)=nI_{X}(\btheta)$ donde $X$ tiene la misma distribución que las variables $X_1$, $\cdots$, $X_n$.

\item Se ha visto que en una muestra aleatoria proveniente de una distribución $Pois(\theta)$, el estimador de máxima verosimilitud y el estimador de momentos es el mismo: $\theta_{MV}=\theta_{mom}=\bar{X}$, ¿Este estimador es $UMVUE$ para $\theta$?

\item Dada $X\sim Bin(n,p)$, con $n$ conocido y $p$ desconocido
        \begin{enumerate}[(a)]
            \item Encuentre el estimador de máxima verosimilitud de $p$.
            \item Encuentre el estimador de momentos de $p$.
            \item ¿Son iguales los dos estimadores hallados anteriormente? En caso afirmativo, comente sobre el desempeño del estimador. En caso negativo, ¿cuál estimador es mejor?
        \end{enumerate}

\item Considere una muestra aleatoria $X_1$, $\cdots$, $X_n$ proveniente de una distribución Gamma con parámetro de forma $k$ fijo y parámetro de escala $\theta$ desconocido.
        \begin{enumerate}[(a)]
            \item Calcular la información de Fisher contenida en la muestra aleatoria acerca del parámetro $\theta$.
            \item Encuentra el estimador de máxima verosimilitud para $\theta$.
            \item Encuentra el estimador de momentos para $\theta$.
            \item ¿Son iguales los dos estimadores hallados anteriormente? En caso afirmativo, comente sobre el desempeño del estimador. En caso negativo, ¿cuál estimador es mejor?
        \end{enumerate}

\item Sea $X_1$, $\cdots$, $X_n$ una muestra aleatoria proveniente de $N(\mu_0,\sigma^2)$ con $\mu_0$ conocido, se ha visto en el Ejercicio 2.5 que ahora el estimador de máxima verosimilitud de $\sigma^2$ es $\dfrac{1}{n}\sum_{i=1}^n(X_i-\mu_0)^2$, mientras que cuando $\mu$ es desconocido el estimador de máxima verosimilitud de $\sigma^2$ es $\dfrac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2$. Compare los dos estimadores en término de sesgo y varianza y decida cuál es mejor (\textit{Ayuda: usar $\sum_{i=1}^n\dfrac{(X_i-\bar{X})^2}{\sigma^2}\sim\chi^2_{n}$}).

\item La función de densidad de la distribución Pareto con parámetro $\theta$ está dada por:
        \begin{equation*}
        f(x,\theta)=\theta c^{\theta}x^{-(\theta+1)},
        \end{equation*}

        para $x\geq c$, $c>0$ y $\theta>0$. Encuentre los estimadores de máxima verosimilitud y de momentos para el parámetro $\theta$.

\item  La función de densidad de la distribución Rayleigh está dada por:
        \begin{equation*}
        f(x,\theta)=\dfrac{x}{\theta^2}\exp\left\{-\dfrac{x^2}{2\theta^2}\right\},
        \end{equation*}

        para $x>0$ y $\theta>0$. Encuentre el estimador de máxima verosimilitud de $\theta$.

\item  La función de densidad de la distribución Weibull está dada por:
        \begin{equation*}
        f(x,\theta)=\theta cx^{c-1}\exp\{-\theta x^c\},
        \end{equation*}

        para $x\geq0$, $c>0$ y $\theta>0$. Encuentre el estimador de máxima verosimilitud de $\theta$.

\item Sea $X_1$, $\cdots$, $X_{n_X}$ y $Y_1$, $\cdots$, $Y_{n_Y}$ dos muestras aleatorias independientes provenientes de una distribución $N(\mu_X,\sigma^2)$ y $N(\mu_Y, \sigma^2)$ respectivamente. Demuestre que el estimador de máxima verosimilitud de $\mu_X$, $\mu_Y$ y $\sigma^2$ es $\bar{X}$, $\bar{Y}$ y $[\sum_{i=1}^{n_X}(X_i-\bar{X}_2)^2+\sum_{j=1}^{n_Y}(Y_j-\bar{Y})^2]/(n_X+n_Y)$.

\item  Dada una muestra aleatoria proveniente de una distribución con parámetro desconocido $\theta$, si $T$ es un estimador insesgado de $\theta$, y $g(\cdot)$ es una función, entonces en general no se tiene que $g(T)$ es un estimador insesgado para $g(\theta)$. Lo anterior se ilustra con la siguiente situación: $X$ es una variable aleatoria con distribución $Bin(n,p)$, demuestra que
\begin{enumerate}[(a)]
\item $X/n$ es un estimador insesgado para $p$.
\item $(X/n)(1-X/n)$ no es un estimador insesgado para $p(1-p)$.
\end{enumerate}

\item  En general, el estimador de máxima verosimilitud y el estimador de momentos para los parámetros en una distribución uniforme no son iguales. Considere $X_1$, $\cdots$, $X_n$ una muestra aleatoria con distribución $Unif[\theta_1,\theta_2]$. Encuentre los estimadores de MV y de momentos de $\theta_1$ y $\theta_2$.

\item  Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con distribución Gamma con parámetro de forma $k$ y parámetro de escala $\theta$, encuentre una estadística suficiente y completa para el vector de parámetros $\theta=(k,\theta)'$.

\item  Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con distribución $Bin(n,p)$, encuentre el estimador UMVUE para $p$.

\item  Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con distribución $N(\mu,\sigma^2_0)$, donde $\sigma^2_0$ es conocido,
        \begin{enumerate}[(a)]
            \item Demuestre que las estadísticas $\sum_{i=1}^nX_i$ y $\bar{X}$ son suficientes y completas para $\mu$.
            \item Encuentre el estimador UMVUE para $\mu$.
        \end{enumerate}

\item  Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con distribución $N(\mu_0,\sigma^2)$, donde $\mu_0$ es conocido,
        \begin{enumerate}[(a)]
            \item Encuentre una estadística suficiente y completa para $\sigma^2$.
            \item Encuentre el estimador UMVUE para $\sigma^2$.
        \end{enumerate}

\item  Dada una muestra aleatoria $X_1$, $\cdots$, $X_n$ con distribución Gamma con parámetro de forma $k$ conocido y parámetro de escala $\theta$ desconocido, encuentre el estimador UMVUE para $\theta$.

\item  En una muestra aleatoria con distribución $N(\mu,\sigma^2)$,
        \begin{enumerate}[(a)]
            \item Calcule la información de Fisher contenida en la muestra acerca del parámetro $\sigma^2$.
            \item Calcule la cota de Cramer Rao para $S^2_{n-1}$ como estimador de $\sigma^2$
            \item Verifique que la desigualdad de información se cumple para $S^2_{n-1}$
            \item Repita (b) y (c) para $S^2_n$ como estimador de $\sigma^2$.
            \item Compare $S^2_n$ y $S^2_{n-1}$ en términos del ECM.
        \end{enumerate}

\item Realice un ejercicio de simulación donde muestra que $Var(\bar{X})$ disminuye al hacer crecer a $n$ en muestras provenientes de
    \begin{enumerate}[(a)]
        \item Bernoulli.
        \item Poisson.
    \end{enumerate}

\item Utilizar el teorema 2.4.23 para ver que en una muestra aleatoria con distribución $N(\mu,\sigma^2)$, tanto $S^2_{n}$ como $S^2_{n-1}$ son estimadores consistentes de $\sigma^2$.

\end{enumerate}

\clearpage
\newpage
\phantom{xxx}
\thispagestyle{empty} 